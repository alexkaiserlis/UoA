import torch
import os
import json
import time
import numpy as np
import collections
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    AutoModel,
    AutoConfig
)
from transformers import DataCollatorForTokenClassification
from datasets import load_dataset
import evaluate
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
from datetime import datetime
from torchcrf import CRF

# =============================================================================
# JSON SERIALIZATION HELPER
# =============================================================================

def clean_for_json_serialization(obj):
    """
    Recursively clean data structure to make it JSON serializable.
    Converts NumPy types to native Python types.
    
    Args:
        obj: Object to clean
        
    Returns:
        JSON-serializable version of the object
    """
    if isinstance(obj, dict):
        return {key: clean_for_json_serialization(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [clean_for_json_serialization(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'item'):  # numpy scalars
        return obj.item()
    else:
        return obj

def save_run_results(results_file, new_results, config_name):
    # Î¦ÏŒÏÏ„Ï‰ÏƒÎµ Ï„Î¿ Ï…Ï€Î¬ÏÏ‡Î¿Î½ json (Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹)
    if os.path.exists(results_file):
        with open(results_file, 'r', encoding='utf-8') as f:
            try:
                all_results = json.load(f)
            except Exception:
                all_results = {}
    else:
        all_results = {}

    # Î”Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ Î¼Î¿Î½Î±Î´Î¹ÎºÏŒ key Î³Î¹Î± Ï„Î¿ run (Ï€.Ï‡. Î¼Îµ timestamp)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    run_key = f"{config_name}_{timestamp}"

    all_results[run_key] = new_results

    with open(results_file, 'w', encoding='utf-8') as f:
        clean_all_results = clean_for_json_serialization(all_results)
        json.dump(clean_all_results, f, indent=4, sort_keys=True)
    print(f"Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± Ï„Î¿ run '{run_key}' Ï€ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î¿ '{results_file}'.")

# --- 0. Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ Î Î•Î™Î¡Î‘ÎœÎ‘Î¤ÎŸÎ£ ---
# Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Ï„Î± experiments
FORCE_RERUN = True  # Set to True Î³Î¹Î± Î½Î± Î¾Î±Î½Î±Ï„ÏÎ­Î¾ÎµÎ¹ ÏŒÎ»Î± Ï„Î± tests

# --- 1. CRF MODEL IMPLEMENTATION ---

class BertCRFForTokenClassification(nn.Module):
    """
    ROBERTa + CRF Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î¹Î± Token Classification
    
    ğŸ” CRF VALIDATION & EXPLANATION:
    ================================
    Î¤Î¿ CRF (Conditional Random Field) layer Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹ structural constraints:
    
    1. FEATURES Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ CRF:
       - ROBERTa logits Î³Î¹Î± ÎºÎ¬Î¸Îµ token (Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ·: num_labels)
       - Transition probabilities Î¼ÎµÏ„Î±Î¾Ï consecutive labels
       - Start/End transitions Î³Î¹Î± sentence boundaries
    
    2. CONSTRAINTS Ï€Î¿Ï… ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹:
       âœ… B-PERSON Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î·Î¸ÎµÎ¯ Î±Ï€ÏŒ I-PERSON Î® O
       âŒ B-PERSON Î”Î•Î Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î·Î¸ÎµÎ¯ Î±Ï€ÏŒ I-ORG
       âŒ I-PERSON Î”Î•Î Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¼Ï†Î±Î½Î¹ÏƒÏ„ÎµÎ¯ Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÏÏÏ„Î± B-PERSON
       âœ… O Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î·Î¸ÎµÎ¯ Î±Ï€ÏŒ Î¿Ï€Î¿Î¹Î¿Î´Î®Ï€Î¿Ï„Îµ B- tag Î® O
    
    3. TRAINING PROCESS:
       - Forward pass: ROBERTa â†’ logits â†’ CRF loss
       - CRF loss: -log P(true_sequence | logits, transitions)
       - Backward pass: Gradient flows back through CRF to ROBERTa
    
    4. INFERENCE PROCESS:
       - Forward pass: ROBERTa â†’ logits
       - CRF decode: Viterbi algorithm Î³Î¹Î± best sequence
       - Output: Structurally valid B-I-O sequence
    """
    def __init__(self, config, num_labels, bert_model_name="AI-team-UoA/GreekLegalRoBERTa_v2"):
        super(BertCRFForTokenClassification, self).__init__()
        self.num_labels = num_labels
        self.config = config
        
        # Load ROBERTa model
        self.bert = AutoModel.from_pretrained(bert_model_name, config=config)
        
        # Dropout
        self.dropout = nn.Dropout(config.hidden_dropout_prob if hasattr(config, 'hidden_dropout_prob') else 0.1)
        
        # Classifier layer
        self.classifier = nn.Linear(config.hidden_size, num_labels)
        
        # CRF layer
        self.crf = CRF(num_labels, batch_first=True)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize the classifier weights"""
        self.classifier.weight.data.normal_(mean=0.0, std=0.02)
        if self.classifier.bias is not None:
            self.classifier.bias.data.zero_()
    
    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        """
        Forward pass Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        """
        # ROBERTa encoding
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        
        sequence_output = outputs[0]  # (batch_size, seq_len, hidden_size)
        sequence_output = self.dropout(sequence_output)
        
        # Classification
        logits = self.classifier(sequence_output)  # (batch_size, seq_len, num_labels)
        
        outputs = {"logits": logits}
        
        if labels is not None:
            # Create attention mask for CRF (exclude padding tokens)
            if attention_mask is not None:
                crf_mask = attention_mask.bool()
            else:
                crf_mask = torch.ones_like(input_ids).bool()
            
            # Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Ï‰Î½ -100 labels Î¼Îµ 0 (O tag) Î³Î¹Î± Ï„Î¿ CRF
            # Î¤Î¿ CRF Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï‡ÎµÎ¹ÏÎ¹ÏƒÏ„ÎµÎ¯ -100 values
            crf_labels = labels.clone()
            crf_labels[crf_labels == -100] = 0  # Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î¼Îµ O tag (index 0)
            
            # Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Ï„Î¿Ï… mask Î³Î¹Î± Î½Î± Î±Î³Î½Î¿Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î± padding tokens
            crf_mask = crf_mask & (labels != -100)
            
            # CRF loss computation
            loss = -self.crf(logits, crf_labels, mask=crf_mask, reduction='mean')
            outputs["loss"] = loss
        
        return outputs
    
    def decode(self, input_ids, attention_mask=None, token_type_ids=None):
        """
        Decoding Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿Î½ CRF Î³Î¹Î± Ï„Î¹Ï‚ ÎºÎ±Î»ÏÏ„ÎµÏÎµÏ‚ predictions
        """
        outputs = self.forward(input_ids, attention_mask, token_type_ids)
        logits = outputs["logits"]
        
        if attention_mask is not None:
            crf_mask = attention_mask.bool()
        else:
            crf_mask = torch.ones_like(input_ids).bool()
        
        # CRF decoding
        predictions = self.crf.decode(logits, mask=crf_mask)
        return predictions

# --- 2. Î’Î•Î›Î¤Î™Î©ÎœÎ•ÎÎ— FOCAL LOSS IMPLEMENTATION (UNCHANGED) ---

class AdaptiveFocalLoss(nn.Module):
    """
    Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· ÎµÎºÎ´Î¿Ï‡Î® Focal Loss ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± NER tasks
    Î¼Îµ Ï€Î¹Î¿ conservative Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ ÎºÎ±Î¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ error handling
    """
    def __init__(self, gamma=1.0, alpha=None, reduction='mean', ignore_index=-100):
        super(AdaptiveFocalLoss, self).__init__()
        self.gamma = gamma  # Î Î¹Î¿ Ï‡Î±Î¼Î·Î»ÏŒ gamma Î³Î¹Î± Ï€Î¹Î¿ Î®Ï€Î¹Î± ÎµÏƒÏ„Î¯Î±ÏƒÎ·
        self.alpha = alpha
        self.reduction = reduction
        self.ignore_index = ignore_index

    def forward(self, inputs, targets):
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± mask Î³Î¹Î± non-padded tokens
        mask = targets != self.ignore_index
        
        # Handle empty batches
        if mask.sum() == 0:
            return torch.tensor(0.0, device=inputs.device, requires_grad=True)
        
        # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± padding tokens
        active_inputs = inputs[mask]
        active_targets = targets[mask]
        
        # Cross entropy loss
        ce_loss = F.cross_entropy(active_inputs, active_targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # Alpha weighting - Ï€Î¹Î¿ balanced approach
        if self.alpha is not None:
            if isinstance(self.alpha, (float, int)):
                alpha_t = self.alpha
            else:
                # Handle tensor alpha
                alpha_tensor = self.alpha.to(device=active_targets.device, dtype=torch.float)
                alpha_t = alpha_tensor[active_targets]
        else:
            alpha_t = 1.0
        
        # Focal loss Î¼Îµ Ï€Î¹Î¿ Î¼Î±Î»Î±ÎºÎ® ÎµÏƒÏ„Î¯Î±ÏƒÎ·
        focal_loss = alpha_t * (1 - pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# --- 2. Î’Î•Î›Î¤Î™Î©ÎœÎ•ÎÎ— FOCAL LOSS IMPLEMENTATION (UNCHANGED) ---

class AdaptiveFocalLoss(nn.Module):
    """
    Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· ÎµÎºÎ´Î¿Ï‡Î® Focal Loss ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± NER tasks
    Î¼Îµ Ï€Î¹Î¿ conservative Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ ÎºÎ±Î¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ error handling
    """
    def __init__(self, gamma=1.0, alpha=None, reduction='mean', ignore_index=-100):
        super(AdaptiveFocalLoss, self).__init__()
        self.gamma = gamma  # Î Î¹Î¿ Ï‡Î±Î¼Î·Î»ÏŒ gamma Î³Î¹Î± Ï€Î¹Î¿ Î®Ï€Î¹Î± ÎµÏƒÏ„Î¯Î±ÏƒÎ·
        self.alpha = alpha
        self.reduction = reduction
        self.ignore_index = ignore_index

    def forward(self, inputs, targets):
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± mask Î³Î¹Î± non-padded tokens
        mask = targets != self.ignore_index
        
        # Handle empty batches
        if mask.sum() == 0:
            return torch.tensor(0.0, device=inputs.device, requires_grad=True)
        
        # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± padding tokens
        active_inputs = inputs[mask]
        active_targets = targets[mask]
        
        # Cross entropy loss
        ce_loss = F.cross_entropy(active_inputs, active_targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # Alpha weighting - Ï€Î¹Î¿ balanced approach
        if self.alpha is not None:
            if isinstance(self.alpha, (float, int)):
                alpha_t = self.alpha
            else:
                # Handle tensor alpha
                alpha_tensor = self.alpha.to(device=active_targets.device, dtype=torch.float)
                alpha_t = alpha_tensor[active_targets]
        else:
            alpha_t = 1.0
        
        # Focal loss Î¼Îµ Ï€Î¹Î¿ Î¼Î±Î»Î±ÎºÎ® ÎµÏƒÏ„Î¯Î±ÏƒÎ·
        focal_loss = alpha_t * (1 - pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class FlexibleTrainer(Trainer):
    """
    Trainer Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎºÎ¬Î½ÎµÎ¹ switch Î¼ÎµÏ„Î±Î¾Ï different loss functions
    ÎºÎ±Î¹ Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ Ï„ÏŒÏƒÎ¿ ROBERTa ÏŒÏƒÎ¿ ÎºÎ±Î¹ ROBERTa+CRF Î¼Î¿Î½Ï„Î­Î»Î±
    """
    def __init__(self, *args, loss_type="adaptive_focal", focal_gamma=1.0, focal_alpha=None, 
                 class_weights=None, debug_mode=False, use_crf=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_type = loss_type
        self.focal_gamma = focal_gamma
        self.focal_alpha = focal_alpha
        self.class_weights = class_weights
        self.debug_mode = debug_mode
        self.use_crf = use_crf
        self.step_count = 0
        
        # Override the compute_metrics Î¼Îµ custom implementation Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
        self._original_compute_metrics = self.compute_metrics
        
    def compute_metrics_with_model_info(self, eval_pred):
        """
        Custom compute_metrics Ï€Î¿Ï… Ï€ÎµÏÎ½Î¬ÎµÎ¹ model info ÏƒÏ„Î¹Ï‚ CRF metrics
        """
        predictions, labels = eval_pred
        
        # ğŸ” DEBUG INFO: Print shapes ÎºÎ±Î¹ types Î³Î¹Î± debugging
        print(f"\nğŸ” COMPUTE_METRICS DEBUG INFO:")
        print(f"   Predictions type: {type(predictions)}")
        print(f"   Predictions shape: {predictions.shape}")
        print(f"   Labels shape: {labels.shape}")
        print(f"   Use CRF: {self.use_crf}")
        
        # ğŸ” CRF COMPATIBILITY FIX: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Ï„Î¿ format Ï„Ï‰Î½ predictions
        if self.use_crf:
            # Î“Î¹Î± CRF models, Ï„Î± predictions Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ Î®Î´Î· decoded sequences
            if len(predictions.shape) == 3:
                # Standard logits format (batch_size, seq_len, num_labels)
                print(f"   CRF: Converting 3D logits to predictions via argmax")
                predictions = np.argmax(predictions, axis=2)
            elif len(predictions.shape) == 2:
                # Î‰Î´Î· decoded sequences (batch_size, seq_len)
                print(f"   CRF: Using 2D predictions as-is")
                predictions = predictions.astype(int)
            else:
                print(f"âš ï¸  Unexpected predictions shape for CRF: {predictions.shape}")
                return {"error": "Invalid predictions shape"}
        else:
            # Î“Î¹Î± standard models, ÎºÎ¬Î½Îµ argmax ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¬
            if len(predictions.shape) == 3:
                print(f"   Standard: Converting 3D logits to predictions via argmax")
                if predictions.shape[-1] == 0:
                    print(f"âŒ ERROR: Predictions have 0 classes! Shape: {predictions.shape}")
                    return {"error": "Empty predictions - no classes"}
                predictions = np.argmax(predictions, axis=2)
            else:
                print(f"âš ï¸  Unexpected predictions shape: {predictions.shape}")
                return {"error": "Invalid predictions shape"}
        
        # ğŸ” ADDITIONAL VALIDATION: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± ÎºÎµÎ½Î¬ predictions
        if predictions.size == 0:
            print(f"âŒ ERROR: Predictions array is empty!")
            return {"error": "Empty predictions array"}
        
        print(f"   Final predictions shape: {predictions.shape}")
        print(f"   Sample predictions: {predictions.flat[:10] if predictions.size > 0 else 'EMPTY'}")
        
        # Convert to label strings
        true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] 
                           for prediction, label in zip(predictions, labels)]
        true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] 
                      for prediction, label in zip(predictions, labels)]
        
        # ğŸ” TAG-LEVEL ACCURACY ANALYSIS - Î“Î¹Î± Î½Î± Î´Î¿ÏÎ¼Îµ Ï„Î¹ Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Îµ Ï„Î± I- tags
        print(f"\nğŸ·ï¸  TAG-LEVEL ACCURACY ANALYSIS")
        print("="*60)
        
        # Flatten Î³Î¹Î± tag-level analysis
        flat_predictions = [tag for seq in true_predictions for tag in seq]
        flat_true_labels = [tag for seq in true_labels for tag in seq]
        
        # Tag-level accuracy
        from sklearn.metrics import accuracy_score
        tag_accuracy = accuracy_score(flat_true_labels, flat_predictions)
        print(f"Overall tag-level accuracy: {tag_accuracy:.4f}")
        
        # Count predictions by tag type
        from collections import Counter
        pred_counter = Counter(flat_predictions)
        true_counter = Counter(flat_true_labels)
        
        print(f"\nğŸ“Š Tag distribution comparison:")
        print(f"{'Tag':<15} {'True':<8} {'Predicted':<8} {'Difference':<8}")
        print("-" * 50)
        
        # Focus on B-/I- tags
        for tag_type in ['B-', 'I-', 'O']:
            relevant_tags = [tag for tag in sorted(set(flat_true_labels + flat_predictions)) 
                           if tag.startswith(tag_type) or tag == tag_type]
            
            if relevant_tags:
                for tag in relevant_tags:
                    true_count = true_counter[tag]
                    pred_count = pred_counter[tag]
                    diff = pred_count - true_count
                    print(f"{tag:<15} {true_count:<8} {pred_count:<8} {diff:+8}")
                print()
        
        # Specific I- tag analysis
        i_tags_true = [tag for tag in flat_true_labels if tag.startswith('I-')]
        i_tags_pred = [tag for tag in flat_predictions if tag.startswith('I-')]
        
        print(f"ğŸ¯ I- tags specific analysis:")
        print(f"  True I- tags: {len(i_tags_true)}")
        print(f"  Predicted I- tags: {len(i_tags_pred)}")
        if len(i_tags_true) > 0:
            print(f"  I- tag recall: {len(i_tags_pred) / len(i_tags_true):.4f}")
        
        # B- tag analysis
        b_tags_true = [tag for tag in flat_true_labels if tag.startswith('B-')]
        b_tags_pred = [tag for tag in flat_predictions if tag.startswith('B-')]
        
        print(f"\nğŸ¯ B- tags specific analysis:")
        print(f"  True B- tags: {len(b_tags_true)}")
        print(f"  Predicted B- tags: {len(b_tags_pred)}")
        if len(b_tags_true) > 0:
            print(f"  B- tag recall: {len(b_tags_pred) / len(b_tags_true):.4f}")
        
        # ğŸ“Š DETAILED ENTITY-TYPE METRICS: B- ÎºÎ±Î¹ I- Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„Î¬ Î±Î½Î¬ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
        # (Calculate for JSON storage, don't display full table in terminal)
        
        # Î’ÏÎ¯ÏƒÎºÏ‰ ÏŒÎ»Î± Ï„Î± entity types
        entity_types = set()
        for tag in label_list:
            if tag.startswith('B-') or tag.startswith('I-'):
                entity_types.add(tag[2:])
        
        entity_metrics = {}
        
        for entity_type in sorted(entity_types):
            # Count Î³Î¹Î± B- tags
            b_tag = f"B-{entity_type}"
            i_tag = f"I-{entity_type}"
            
            # True B- tags
            b_true_count = flat_true_labels.count(b_tag)
            b_pred_count = flat_predictions.count(b_tag)
            b_correct_count = sum(1 for true, pred in zip(flat_true_labels, flat_predictions) 
                                if true == b_tag and pred == b_tag)
            
            # True I- tags  
            i_true_count = flat_true_labels.count(i_tag)
            i_pred_count = flat_predictions.count(i_tag)
            i_correct_count = sum(1 for true, pred in zip(flat_true_labels, flat_predictions) 
                                if true == i_tag and pred == i_tag)
            
            # Accuracy Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Î¯
            b_accuracy = b_correct_count / max(b_true_count, 1)
            i_accuracy = i_correct_count / max(i_true_count, 1)
            
            # Combined F1 Î³Î¹Î± Î±Ï…Ï„ÏŒÎ½ Ï„Î¿Î½ entity type (B- + I- combined)
            total_true = b_true_count + i_true_count
            total_pred = b_pred_count + i_pred_count  
            total_correct = b_correct_count + i_correct_count
            
            if total_true > 0 and total_pred > 0:
                precision = total_correct / total_pred
                recall = total_correct / total_true
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            else:
                f1 = 0
            
            # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î³Î¹Î± summary
            entity_metrics[entity_type] = {
                'b_true': b_true_count,
                'b_pred': b_pred_count, 
                'b_correct': b_correct_count,
                'b_accuracy': b_accuracy,
                'i_true': i_true_count,
                'i_pred': i_pred_count,
                'i_correct': i_correct_count, 
                'i_accuracy': i_accuracy,
                'combined_f1': f1,
                'total_true': total_true,
                'total_pred': total_pred,
                'total_correct': total_correct
            }
            
            print(f"{entity_type:<15} {b_true_count:<8} {b_pred_count:<8} {b_correct_count:<10} {b_accuracy:<8.3f} {i_true_count:<8} {i_pred_count:<8} {i_correct_count:<10} {i_accuracy:<8.3f} {f1:<12.4f}")
        
        # Summary statistics
        print("-" * 120)
        
        # Macro-averaged F1 (unweighted average)
        macro_f1 = sum(metrics['combined_f1'] for metrics in entity_metrics.values()) / len(entity_metrics) if entity_metrics else 0
        
        # Micro-averaged F1 (weighted by frequency)
        total_all_true = sum(metrics['total_true'] for metrics in entity_metrics.values())
        total_all_pred = sum(metrics['total_pred'] for metrics in entity_metrics.values())
        total_all_correct = sum(metrics['total_correct'] for metrics in entity_metrics.values())
        
        if total_all_true > 0 and total_all_pred > 0:
            micro_precision = total_all_correct / total_all_pred
            micro_recall = total_all_correct / total_all_true
            micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        else:
            micro_f1 = 0
            
        print(f"{'MACRO AVG':<15} {'':<8} {'':<8} {'':<10} {'':<8} {'':<8} {'':<8} {'':<10} {'':<8} {macro_f1:<12.4f}")
        print(f"{'MICRO AVG':<15} {'':<8} {'':<8} {'':<10} {'':<8} {'':<8} {'':<8} {'':<10} {'':<8} {micro_f1:<12.4f}")
        
        print("\nï¿½ TAG-LEVEL METRICS SUMMARY:")
        print("="*60)
        print(f"ğŸ“‹ OVERVIEW:")
        print(f"   Overall Tag Accuracy: {tag_accuracy:.4f}")
        print(f"   Micro F1 (tag-weighted): {micro_f1:.4f}")
        print(f"   Macro F1 (entity-avg): {macro_f1:.4f}")
        print()
        print(f"ğŸ“Š PER-ENTITY PERFORMANCE (Combined B-/I- F1):")
        print("-" * 50)
        
        # Show only the Combined F1 for each entity in terminal
        for entity_type in ['FACILITY', 'GPE', 'LEGISLATION_REFERENCE', 'NATIONAL_LOCATION', 'UNKNOWN_LOCATION', 'ORGANIZATION', 'PERSON', 'PUBLIC_DOCUMENT']:
            if entity_type in entity_metrics:
                combined_f1 = entity_metrics[entity_type]['combined_f1']
                status = "ğŸ”´" if combined_f1 < 0.3 else "ğŸŸ¡" if combined_f1 < 0.7 else "ğŸŸ¢"
                print(f"   {status} {entity_type:<15}: {combined_f1:.4f}")
        
        print("-" * 50)
        print(f"ğŸ“ NOTE: Full detailed B-/I- breakdown saved to JSON file")
        print("="*60)
        
        # ğŸ” VALIDATION: Check entity extraction is working correctly
        validate_entity_extraction(true_predictions, true_labels, label_list)
        # ğŸ” ENTITY CONSISTENCY CHECK

        print(f"\nğŸ” ENTITY CONSISTENCY VALIDATION:")
        print("="*50)
        
        def check_entity_consistency(predictions):
            """
            Î•Î»Î­Î³Ï‡ÎµÎ¹ Ï„Î·Î½ consistency Ï„Ï‰Î½ B-I-O sequences
            Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: (invalid_sequences, orphaned_i_tags, total_sequences)
            """
            invalid_seqs = 0
            orphaned_tags = 0
            total_seqs = len(predictions)
            
            for seq_idx, pred_seq in enumerate(predictions):
                has_invalid = False
                
                for i, tag in enumerate(pred_seq):
                    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± orphaned I- tags (I- Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿ B-)
                    if tag.startswith('I-'):
                        entity_type = tag[2:]
                        
                        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ B- tag Ï€ÏÎ¹Î½ Î±Ï€ÏŒ Î±Ï…Ï„ÏŒ Ï„Î¿ I- tag
                        found_b_tag = False
                        for j in range(i-1, -1, -1):  # Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Ï€ÏÎ¿Ï‚ Ï„Î± Ï€Î¯ÏƒÏ‰
                            prev_tag = pred_seq[j]
                            if prev_tag == 'O':
                                break  # Î”Î¹Î±ÎºÎ¿Ï€Î® Î±Î½ Î²ÏÎ¿ÏÎ¼Îµ O tag
                            elif prev_tag == f'B-{entity_type}':
                                found_b_tag = True
                                break
                            elif prev_tag.startswith('I-') and prev_tag[2:] == entity_type:
                                continue  # Î£Ï…Î½Î­Ï‡ÎµÎ¹Î± Ï„Î¿Ï… entity
                            else:
                                break  # Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ entity type
                        
                        if not found_b_tag:
                            orphaned_tags += 1
                            has_invalid = True
                            
                            # Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎ· sample orphaned tags Î³Î¹Î± debugging
                            if orphaned_tags <= 3:  # ÎœÏŒÎ½Î¿ Ï„Î± Ï€ÏÏÏ„Î± 3
                                context_start = max(0, i-2)
                                context_end = min(len(pred_seq), i+3)
                                context = pred_seq[context_start:context_end]
                                print(f"   Orphaned I-tag found: {tag} at position {i}")
                                print(f"   Context: {context}")
                
                if has_invalid:
                    invalid_seqs += 1
            
            return invalid_seqs, orphaned_tags, total_seqs
        
        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ consistency
        invalid_seqs, orphaned_tags, total_seqs = check_entity_consistency(true_predictions)
        consistency_rate = (total_seqs - invalid_seqs) / total_seqs * 100
        
        print(f"   Total sequences: {total_seqs}")
        print(f"   Invalid sequences: {invalid_seqs}")
        print(f"   Orphaned I- tags: {orphaned_tags}")
        print(f"   Consistency rate: {consistency_rate:.2f}%")
        
        if invalid_seqs > 0:
            print(f"   âš ï¸  {invalid_seqs} sequences have BIO inconsistencies!")
            print(f"   This suggests sliding window fragmentation issues.")
        else:
            print(f"   âœ… All sequences are BIO consistent!")
        
        print("="*50)

        # Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ (Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï€ÏÎ¹Î½ Ï„Î·Î½ Ï€ÏÎ¿ÏƒÎ¸Î®ÎºÎ· consistency metrics)
        final_metrics = {
            "precision": 0,  # placeholder, will be set below
            "recall": 0,
            "f1": 0,
            "accuracy": 0
        }

        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· consistency metrics
        final_metrics["consistency_rate"] = consistency_rate
        final_metrics["orphaned_i_tags"] = orphaned_tags
        final_metrics["invalid_sequences"] = invalid_seqs

        # ğŸ” CRF-SPECIFIC METRICS: Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚ CRF Î¼Îµ model info
        crf_metrics = compute_crf_specific_metrics(
            predictions=true_predictions,
            true_labels=true_labels,
            model=self.model,
            use_crf=self.use_crf
        )

        # Compute seqeval metrics (entity-level, not tag-level)
        results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)

        print(f"\nğŸ“Š SEQEVAL METRICS VALIDATION:")
        print("="*50)
        print(f"SeqEval Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ entities, ÏŒÏ‡Î¹ tags:")
        print(f"  - ÎšÎ¬Î¸Îµ entity (B- + I-*) Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ Ï‰Ï‚ 1")
        print(f"  - F1 Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÏ„Î±Î¹ ÏƒÎµ entity-level")
        print(f"  - Exact match: Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Ï„Î¿ entity Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ ÏƒÏ‰ÏƒÏ„ÏŒ")
        print()
        
        # DEBUG: Î”ÎµÎ¯Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î· seqeval
        print(f"ğŸ” SEQEVAL PER-ENTITY RESULTS:")
        for entity_type in sorted(['FACILITY', 'GPE', 'LEGISLATION_REFERENCE', 'NATIONAL_LOCATION', 'UNKNOWN_LOCATION', 'ORGANIZATION', 'PERSON', 'PUBLIC_DOCUMENT']):
            if entity_type in results:
                metrics = results[entity_type]
                f1 = metrics.get('f1', 0)
                precision = metrics.get('precision', 0)
                recall = metrics.get('recall', 0)
                number = metrics.get('number', 0)
                status = "ğŸ”´" if f1 < 0.3 else "ğŸŸ¡" if f1 < 0.7 else "ğŸŸ¢"
                print(f"   {status} {entity_type:<15}: F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}, N={number}")
        print("="*50)

        # Entity counting validation
        total_entity_count = 0
        total_tag_count = 0

        for seq in true_labels[:100]:            
            # ...existing code...
            pass
        print(f"Validation sample (first 100 sequences):")
        print(f"  Total B- tags (entities): {total_entity_count}")
        print(f"  Total B-/I- tags: {total_tag_count}")
        print(f"  Average I- tags per entity: {(total_tag_count - total_entity_count) / max(total_entity_count, 1):.2f}")
        print("="*50)

        # Update Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ Î¼Îµ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±
        final_metrics["precision"] = results["overall_precision"]
        final_metrics["recall"] = results["overall_recall"]
        final_metrics["f1"] = results["overall_f1"]
        final_metrics["accuracy"] = results["overall_accuracy"]
        
        # ğŸ“Š Î Î¡ÎŸÎ£Î˜Î—ÎšÎ— ÎŸÎ›Î©Î Î¤Î©Î Î‘ÎÎ‘Î›Î¥Î¤Î™ÎšÎ©Î METRICS - ÏŒÏ€Ï‰Ï‚ ÏƒÏ„Î¿ crf2.json
        # Tag-level metrics
        final_metrics["tag_level_accuracy"] = tag_accuracy
        final_metrics["micro_f1"] = micro_f1
        final_metrics["macro_f1"] = macro_f1
        
        # Entity type metrics (detailed breakdown)
        final_metrics["entity_type_metrics"] = entity_metrics
        
        # Per-entity F1 scores (Î”Î™ÎŸÎ¡Î˜Î©Î£Î—: Î§ÏÎ®ÏƒÎ· Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ©Î entity-level metrics Î±Ï€ÏŒ seqeval)
        for entity_type in entity_metrics:
            # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ‘ entity-level metrics Î±Ï€ÏŒ seqeval Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
            if entity_type in results:
                # SeqEval Î´Î¯Î½ÎµÎ¹ Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ‘ entity-level metrics (ÏŒÏ‡Î¹ tag-level)
                final_metrics[f"{entity_type}_f1"] = results[entity_type]["f1"]
                final_metrics[f"{entity_type}_precision"] = results[entity_type]["precision"] 
                final_metrics[f"{entity_type}_recall"] = results[entity_type]["recall"]
                final_metrics[f"{entity_type}_number"] = results[entity_type]["number"]
            else:
                # Fallback ÏƒÏ„Î± tag-level combined metrics Î±Î½ Ï„Î¿ entity Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î± seqeval results
                final_metrics[f"{entity_type}_f1"] = entity_metrics[entity_type]["combined_f1"]
                final_metrics[f"{entity_type}_precision"] = entity_metrics[entity_type]["total_correct"] / max(entity_metrics[entity_type]["total_pred"], 1)
                final_metrics[f"{entity_type}_recall"] = entity_metrics[entity_type]["total_correct"] / max(entity_metrics[entity_type]["total_true"], 1)
                final_metrics[f"{entity_type}_number"] = len([seq for seq in true_labels for tag in seq if tag == f"B-{entity_type}"])
        
        # CRF-specific metrics
        if crf_metrics:
            for key, value in crf_metrics.items():
                final_metrics[f"crf_{key}"] = value
        
        # B-/I- tag statistics
        final_metrics["total_b_tags_true"] = len(b_tags_true)
        final_metrics["total_b_tags_predicted"] = len(b_tags_pred)
        final_metrics["total_i_tags_true"] = len(i_tags_true)
        final_metrics["total_i_tags_predicted"] = len(i_tags_pred)
        final_metrics["b_tag_recall"] = len(b_tags_pred) / max(len(b_tags_true), 1)
        final_metrics["i_tag_recall"] = len(i_tags_pred) / max(len(i_tags_true), 1)
        
        return final_metrics
        
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        
        # Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ CRF Î¼Î¿Î½Ï„Î­Î»Î¿
        if self.use_crf:
            outputs = model(**inputs)
            loss = outputs.get("loss")
            if loss is None:
                # Fallback Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ loss ÏƒÏ„Î¿ output
                logits = outputs.get("logits")
                # Î•Î´Ï Î¸Î± Î¼Ï€Î¿ÏÎ¿ÏÏƒÎ±Î¼Îµ Î½Î± Ï€ÏÎ¿ÏƒÎ¸Î­ÏƒÎ¿Ï…Î¼Îµ custom loss computation Î³Î¹Î± CRF
                # Î±Î»Î»Î¬ Ï„Î¿ CRF Î¼Î¿Î½Ï„Î­Î»Î¿ Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒ internally
                loss = torch.tensor(0.0, device=logits.device, requires_grad=True)
        else:
            # Standard ROBERTa processing
            outputs = model(**inputs)
            logits = outputs.get("logits")
            
            # Reshape Î³Î¹Î± loss computation
            shift_logits = logits.view(-1, self.model.config.num_labels)
            shift_labels = labels.view(-1)
            
            if self.loss_type == "adaptive_focal" or self.loss_type == "soft_focal":
                # Adaptive/Soft Focal Loss
                gamma = self.focal_gamma if self.loss_type == "soft_focal" else 1.0
                alpha = self.focal_alpha if self.focal_alpha is not None else self.class_weights
                
                loss_fct = AdaptiveFocalLoss(
                    gamma=gamma, 
                    alpha=alpha, 
                    ignore_index=-100
                )
                loss = loss_fct(shift_logits, shift_labels)
                
            elif self.loss_type == "weighted_ce":
                # Weighted Cross Entropy
                weights = self.class_weights if self.class_weights is not None else None
                loss_fct = nn.CrossEntropyLoss(weight=weights, ignore_index=-100)
                loss = loss_fct(shift_logits, shift_labels)
                
            else:  # "standard"
                # Standard Cross Entropy
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                loss = loss_fct(shift_logits, shift_labels)
        
        # Debug logging
        if self.debug_mode and self.step_count % 100 == 0:
            model_type = "CRF" if self.use_crf else "ROBERTa"
            
            # ğŸ” TRAINING VALIDATION: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î²Î»Î­Ï€ÎµÎ¹ Ï„Î± ÏƒÏ‰ÏƒÏ„Î¬ labels
            if not self.use_crf and labels is not None:
                # Î“Î¹Î± standard ROBERTa, ÎµÎ»Î­Î³Ï‡Ï‰ Ï„Î¹ labels Î²Î»Î­Ï€ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
                active_labels = labels[labels != -100]
                if len(active_labels) > 0:
                    unique_labels = torch.unique(active_labels)
                    max_label = unique_labels.max().item()
                    min_label = unique_labels.min().item()
                    expected_max = self.model.config.num_labels - 1
                    
                    if max_label > expected_max:
                        print(f"âš ï¸  WARNING: Model sees label {max_label} but max expected is {expected_max}")
                    if min_label < 0:
                        print(f"âš ï¸  WARNING: Model sees negative label {min_label}")
                    
                    # Î”ÎµÎ¯Î³Î¼Î± Î±Ï€ÏŒ Ï„Î± labels Ï€Î¿Ï… Î²Î»Î­Ï€ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
                    sample_labels = unique_labels[:5].tolist()
                    print(f"Step {self.step_count}: Loss={loss.item():.4f}, Model={model_type}, "
                          f"Loss_type={self.loss_type}, Active_labels={len(active_labels)}, "
                          f"Sample_labels={sample_labels}")
                else:
                    print(f"Step {self.step_count}: Loss={loss.item():.4f}, Model={model_type}, "
                          f"Loss_type={self.loss_type}, No_active_labels")
            else:
                print(f"Step {self.step_count}: Loss={loss.item():.4f}, Model={model_type}, "
                      f"Loss_type={self.loss_type}")
        
        self.step_count += 1
        return (loss, outputs) if return_outputs else loss

    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        """
        Custom prediction step Ï€Î¿Ï… Ï‡ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ Ï„ÏŒÏƒÎ¿ ROBERTa ÏŒÏƒÎ¿ ÎºÎ±Î¹ CRF Î¼Î¿Î½Ï„Î­Î»Î±
        """
        inputs = self._prepare_inputs(inputs)
        
        with torch.no_grad():
            if self.use_crf and hasattr(model, 'decode'):
                # Î“Î¹Î± CRF Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ decode method
                loss = None
                if not prediction_loss_only:
                    outputs = model(**inputs)
                    loss = outputs.get("loss")
                
                # Get predictions using CRF decoding
                try:
                    predictions = model.decode(
                        input_ids=inputs["input_ids"],
                        attention_mask=inputs.get("attention_mask"),
                        token_type_ids=inputs.get("token_type_ids")
                    )
                    
                    print(f"ğŸ” CRF decode predictions type: {type(predictions)}")
                    print(f"ğŸ” CRF decode predictions length: {len(predictions) if predictions else 'None'}")
                    if predictions and len(predictions) > 0:
                        print(f"ğŸ” First prediction shape: {len(predictions[0]) if hasattr(predictions[0], '__len__') else 'scalar'}")
                    
                    # Convert predictions to tensor format Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ ÏƒÏ…Î¼Î²Î±Ï„ÏŒ Î¼Îµ Ï„Î¿ evaluation
                    batch_size, seq_len = inputs["input_ids"].shape
                    num_labels = self.model.num_labels
                    
                    print(f"ğŸ” Creating logits tensor: batch_size={batch_size}, seq_len={seq_len}, num_labels={num_labels}")
                    
                    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± logits tensor Î±Ï€ÏŒ Ï„Î¹Ï‚ CRF predictions
                    logits = torch.zeros((batch_size, seq_len, num_labels), device=inputs["input_ids"].device)
                    
                    if predictions is not None and len(predictions) > 0:
                        for i, seq_pred in enumerate(predictions):
                            if seq_pred is not None and len(seq_pred) > 0:
                                pred_len = min(len(seq_pred), seq_len)
                                for j in range(pred_len):
                                    if 0 <= seq_pred[j] < num_labels:  # Enhanced safety check
                                        logits[i, j, seq_pred[j]] = 1.0  # One-hot encoding
                    else:
                        print("âš ï¸ Warning: CRF decode returned empty predictions")
                        
                except Exception as e:
                    print(f"âŒ Error in CRF decode: {e}")
                    # Fallback: Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± empty logits
                    batch_size, seq_len = inputs["input_ids"].shape
                    num_labels = self.model.num_labels
                    logits = torch.zeros((batch_size, seq_len, num_labels), device=inputs["input_ids"].device)
                
            else:
                # Standard ROBERTa prediction
                outputs = model(**inputs)
                loss = outputs.get("loss")
                logits = outputs.get("logits")
        
        labels = inputs.get("labels")
        
        if prediction_loss_only:
            return (loss, None, None)
        
        return (loss, logits, labels)

# --- 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (UPDATED TO USE LEXTREME DATASET) ---
print("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· tokenizer 'AI-team-UoA/GreekLegalRoBERTa_v2'...")
tokenizer = AutoTokenizer.from_pretrained("AI-team-UoA/GreekLegalRoBERTa_v2", add_prefix_space=True)

# Î¦ÎŸÎ¡Î¤Î©Î£Î— GREEK LEGAL NER IOB DATASET
print("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Greek Legal NER IOB dataset...")
try:
    import json
    from datasets import Dataset, DatasetDict
    import os
    
    # Paths Î³Î¹Î± Ï„Î± IOB Î±ÏÏ‡ÎµÎ¯Î± (Î±Ï€ÏŒÎ»Ï…Ï„Î± paths)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.join(script_dir, "..", "..", "01_DATASETS", "GREEK_LEGAL_NER")
    train_path = os.path.join(base_dir, "train_iob.json")
    test_path = os.path.join(base_dir, "test_iob.json")
    validation_path = os.path.join(base_dir, "validation_iob.json")
    
    def load_iob_dataset(file_path):
        """Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Î­Î½Î± IOB JSON Î±ÏÏ‡ÎµÎ¯Î¿ ÎºÎ±Î¹ Ï„Î¿ Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÎµ dataset format"""
        print(f"   ğŸ“‚ Î¦ÏŒÏÏ„Ï‰ÏƒÎ·: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Î­Ï‡ÎµÎ¹ Î´Î¿Î¼Î®: {"input": [...], "label": [...], "language": [...]}
        inputs = data['input']  # List of token lists
        labels = data['label']  # List of label lists (IOB tags)
        languages = data['language']  # List of language codes
        
        # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ format Ï€Î¿Ï… Î¸Î­Î»ÎµÎ¹ Ï„Î¿ HuggingFace
        dataset_data = {
            'tokens': inputs,
            'ner': labels  # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ 'ner' Î³Î¹Î± consistency Î¼Îµ Ï„Î¿ Ï…Ï€Î¬ÏÏ‡Î¿Î½ script
        }
        
        print(f"   âœ… Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(inputs)} Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚")
        return Dataset.from_dict(dataset_data)
    
    # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ splits
    train_ds = load_iob_dataset(train_path)
    test_ds = load_iob_dataset(test_path)
    validation_ds = load_iob_dataset(validation_path)
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± DatasetDict
    ds = DatasetDict({
        'train': train_ds,
        'test': test_ds,
        'validation': validation_ds
    })
    
    print(f"âœ… Greek Legal NER IOB dataset Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
    
except Exception as e:
    print(f"âŒ Î£Ï†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ IOB dataset: {str(e)}")
    print(f"ï¿½ Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½:")
    print(f"   - {train_path}")
    print(f"   - {test_path}")
    print(f"   - {validation_path}")
    raise e
print(f"ï¿½ Î¤ÎµÎ»Î¹ÎºÏŒ dataset - Splits: {list(ds.keys())}")
print(f"ğŸ“Š Training examples: {len(ds['train'])}")
print(f"ğŸ“Š Test examples: {len(ds['test'])}")
print(f"ğŸ“Š Validation examples: {len(ds['validation'])}")

# --- 3. Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î•Ï„Î¹ÎºÎµÏ„ÏÎ½ (ENHANCED WITH VALIDATION) ---
all_ner_tags_set = set(tag for example in ds['train'] for tag in example['ner'])
if 'O' in all_ner_tags_set:
    label_list = ['O'] + sorted(list(all_ner_tags_set - {'O'}))
else:
    label_list = sorted(list(all_ner_tags_set))
id2label = {i: label for i, label in enumerate(label_list)}
label2id = {label: i for i, label in id2label.items()}

print(f"ğŸ“Š DATASET LABEL ANALYSIS:")
print(f"Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î¼Î¿Î½Î±Î´Î¹ÎºÏÎ½ ÎµÏ„Î¹ÎºÎµÏ„ÏÎ½: {len(label_list)}")
print(f"ÎŒÎ»Î± Ï„Î± labels ÏƒÏ„Î¿ dataset:")
for i, label in enumerate(label_list):
    print(f"  {i:2d}: {label}")

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Î­Ï‡Î¿Ï…Î¼Îµ Ï„Î± Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± Greek Legal NER tags
expected_entity_types = ['FACILITY', 'GPE', 'LEGISLATION_REFERENCE', 'NATIONAL_LOCATION', 'ORGANIZATION', 'PERSON', 'PUBLIC_DOCUMENT', 'UNKNOWN_LOCATION']
found_entities = set()
for label in label_list:
    if label.startswith('B-') or label.startswith('I-'):
        entity_type = label[2:]
        found_entities.add(entity_type)

print(f"\nğŸ” ENTITY TYPE VALIDATION:")
print(f"Î‘Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± entity types: {len(expected_entity_types)}")
print(f"Î’ÏÎµÎ¸Î­Î½Ï„Î± entity types: {len(found_entities)}")
for entity_type in expected_entity_types:
    status = "âœ…" if entity_type in found_entities else "âŒ"
    print(f"  {status} {entity_type}")

missing_entities = set(expected_entity_types) - found_entities
extra_entities = found_entities - set(expected_entity_types)
if missing_entities:
    print(f"âš ï¸  Î›ÎµÎ¯Ï€Î¿Ï…Î½ entities: {missing_entities}")
if extra_entities:
    print(f"â„¹ï¸  Î•Ï€Î¹Ï€Î»Î­Î¿Î½ entities: {extra_entities}")

print(f"\nğŸ“‹ COMPLETE LABEL MAPPING:")
print(f"{'ID':<3} {'LABEL':<25} {'TYPE':<10} {'ENTITY'}")
print("-" * 60)
for i, label in enumerate(label_list):
    if label == 'O':
        label_type = "NON-ENTITY"
        entity_name = "N/A"
    elif label.startswith('B-'):
        label_type = "BEGIN"
        entity_name = label[2:]
    elif label.startswith('I-'):
        label_type = "INSIDE"
        entity_name = label[2:]
    else:
        label_type = "OTHER"
        entity_name = label
    print(f"{i:<3} {label:<25} {label_type:<10} {entity_name}")
print("="*60)

# --- 4. Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± (Unchanged) ---
# --- 4. Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± (Î¤Î¡ÎŸÎ ÎŸÎ ÎŸÎ™Î—ÎœÎ•ÎÎ— Î“Î™Î‘ SLIDING WINDOW) ---
MAX_LENGTH = 512
OVERLAP = 50

# Î£Ï„Î¿ compute_metrics, Î´ÏÏƒÎµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ Î²Î¬ÏÎ¿Ï‚ ÏƒÏ„Î¹Ï‚ ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚:
def compute_weighted_f1(results):
    """Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ weighted F1 Ï€Î¿Ï… Î´Î¯Î½ÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ· Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î¹Ï‚ ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚"""
    rare_classes = ["NATIONAL_LOCATION", "FACILITY", "UNKNOWN_LOCATION"]
    rare_weight = 2.0
    normal_weight = 1.0
    
    weighted_f1 = 0
    total_weight = 0
    
    for class_name, metrics in results.items():
        if isinstance(metrics, dict) and 'f1' in metrics:
            # Î”Î™ÎŸÎ¡Î˜Î©Î£Î—: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î¼Îµ endswith Î³Î¹Î± B-NATIONAL_LOCATION, I-NATIONAL_LOCATION ÎºÏ„Î»
            is_rare = any(class_name.endswith(rare_class) for rare_class in rare_classes)
            weight = rare_weight if is_rare else normal_weight
            weighted_f1 += metrics['f1'] * weight
            total_weight += weight
    
    return weighted_f1 / total_weight if total_weight > 0 else 0

# Î“Î¹Î± Ï„Î¹Ï‚ Ï€Î¿Î»Ï ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚, ÎºÎ¬Î½Îµ manual oversampling:
def oversample_rare_classes(tokenized_ds, rare_threshold=30, oversample_factor=3):
    """ÎšÎ¬Î½ÎµÎ¹ oversample Ï„Î± rare class instances"""
    train_data = tokenized_ds["train"]
    
    # Î’ÏÎµÏ‚ Ï€Î¿Î¹Î± examples Î­Ï‡Î¿Ï…Î½ rare classes
    rare_examples = []
    for example in train_data:
        labels = example["labels"]
        # Î”Î™ÎŸÎ¡Î˜Î©Î£Î—: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î¼Îµ endswith Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ B-NATIONAL_LOCATION, I-NATIONAL_LOCATION ÎºÏ„Î»
        has_rare = False
        for l in labels:
            if l != -100:
                label_name = label_list[l]
                if any(label_name.endswith(rare_class) for rare_class in ["NATIONAL_LOCATION", "FACILITY"]):
                    has_rare = True
                    break
        if has_rare:
            rare_examples.append(example)
    
    # Î ÏÏŒÏƒÎ¸ÎµÏƒÎµ Ï„Î± rare examples Ï€Î¿Î»Î»Î­Ï‚ Ï†Î¿ÏÎ­Ï‚
    for _ in range(oversample_factor):
        train_data = train_data.add_item(rare_examples)
    
    return train_data

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        is_split_into_words=True,
        max_length=MAX_LENGTH,
        stride=OVERLAP,
        return_overflowing_tokens=True,
        padding="max_length"
    )

    all_labels = []
    for i, overflow_map_idx in enumerate(tokenized_inputs["overflow_to_sample_mapping"]):
        ner_tags = examples["ner"][overflow_map_idx]
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx == previous_word_idx:
                label_ids.append(-100)
            else:
                label_ids.append(label2id[ner_tags[word_idx]])
            previous_word_idx = word_idx
        all_labels.append(label_ids)
    tokenized_inputs["labels"] = all_labels
    return tokenized_inputs

print("Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ Î¼Îµ Sliding Window ÏƒÏ„Î¿ dataset...")
tokenized_ds = ds.map(
    tokenize_and_align_labels, 
    batched=True, 
    remove_columns=ds["train"].column_names
)

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ training set
print(f"Î‘ÏÏ‡Î¹ÎºÏŒ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ training set: {len(ds['train'])} Î­Î³Î³ÏÎ±Ï†Î±")
print(f"ÎÎ­Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ training set: {len(tokenized_ds['train'])} chunks (Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚)")

# ğŸ” ÎšÎ¡Î™Î£Î™ÎœÎŸÎ£ Î•Î›Î•Î“Î§ÎŸÎ£: Validation ÏŒÏ„Î¹ Ï„Î¿ tokenization Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î± labels ÏƒÏ‰ÏƒÏ„Î¬
print(f"\nğŸ” TOKENIZATION VALIDATION:")
print("="*60)

# Î£Ï…Î»Î»Î­Î³Ï‰ ÏŒÎ»Î± Ï„Î± labels Î±Ï€ÏŒ Ï„Î¿ tokenized dataset
all_tokenized_labels = []
for example in tokenized_ds["train"]:
    labels = example["labels"]
    all_tokenized_labels.extend([l for l in labels if l != -100])

tokenized_label_counts = Counter(all_tokenized_labels)
print(f"Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ valid labels Î¼ÎµÏ„Î¬ tokenization: {len(all_tokenized_labels):,}")

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± labels Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
missing_labels = []
present_labels = []
for i, label in enumerate(label_list):
    if i in tokenized_label_counts:
        present_labels.append((i, label, tokenized_label_counts[i]))
    else:
        missing_labels.append((i, label))

print(f"\nğŸ“Š LABEL PRESENCE CHECK:")
print(f"âœ… Î Î±ÏÏŒÎ½Ï„Î± labels: {len(present_labels)}/{len(label_list)}")
print(f"âŒ Î‘Ï€ÏŒÎ½Ï„Î± labels: {len(missing_labels)}")

if missing_labels:
    print(f"\nâš ï¸  Î Î¡ÎŸÎ£ÎŸÎ§Î—: Î¤Î± Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ labels Î»ÎµÎ¯Ï€Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿ tokenized dataset:")
    for label_id, label_name in missing_labels:
        print(f"  {label_id}: {label_name}")
    print(f"Î‘Ï…Ï„ÏŒ ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î”Î•Î Î¸Î± ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„ÎµÎ¯ Î³Î¹Î± Î±Ï…Ï„Î¬ Ï„Î± labels!")

# Î”ÎµÎ¯Ï‡Î½Ï‰ Ï„Î± 5 ÏƒÏ€Î±Î½Î¹ÏŒÏ„ÎµÏÎ± labels Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
print(f"\nğŸ“ˆ RAREST LABELS IN TOKENIZED DATASET:")
rare_labels = sorted(present_labels, key=lambda x: x[2])[:5]
for label_id, label_name, count in rare_labels:
    percentage = (count / len(all_tokenized_labels)) * 100
    print(f"  {label_name}: {count:,} occurrences ({percentage:.3f}%)")

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î± entity types Ï€Î¿Ï… Î±Î½Î±Î¼Î­Î½Î¿Ï…Î¼Îµ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
entity_types_in_tokenized = set()
for label_id, label_name, count in present_labels:
    if label_name.startswith('B-') or label_name.startswith('I-'):
        entity_type = label_name[2:]
        entity_types_in_tokenized.add(entity_type)

expected_entities = {'FACILITY', 'GPE', 'LEGISLATION_REFERENCE', 'NATIONAL_LOCATION', 'UNKNOWN_LOCATION', 'ORGANIZATION', 'PERSON', 'PUBLIC_DOCUMENT'}
missing_entity_types = expected_entities - entity_types_in_tokenized

if missing_entity_types:
    print(f"\nâŒ MISSING ENTITY TYPES:")
    for entity_type in missing_entity_types:
        print(f"  {entity_type} - Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î”Î•Î Î¸Î± Î¼Î¬Î¸ÎµÎ¹ Î½Î± ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ Î±Ï…Ï„ÏŒ Ï„Î¿ entity type!")
else:
    print(f"\nâœ… ÎŒÎ»Î± Ï„Î± Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î± entity types Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ„Î¿ tokenized dataset")

print("="*60)

# --- 5. Î’Î•Î›Î¤Î™Î©ÎœÎ•ÎÎŸÎ£ Î¥Î ÎŸÎ›ÎŸÎ“Î™Î£ÎœÎŸÎ£ CLASS WEIGHTS ---
def compute_class_weights(tokenized_ds, label_list, method="capped_sqrt_inv_freq", original_dataset=None):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ class weights Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚.
    Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î­ÎºÎ´Î¿ÏƒÎ·: manual boost Î³Î¹Î± Ï€Î¿Î»Ï rare classes Î²Î¬ÏƒÎµÎ¹ Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ©Î Î±ÏÎ¹Î¸Î¼ÏÎ½:
    Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ entity counts Î±Ï€ÏŒ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset, ÏŒÏ‡Î¹ Ï„Î± tokenized counts
    """
    
    # Î¥Î ÎŸÎ›ÎŸÎ“Î™Î£ÎœÎŸÎ£ Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ©Î ENTITY COUNTS Î±Ï€ÏŒ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset
    if original_dataset is not None:
        print("Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏÎ½ entity counts Î±Ï€ÏŒ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset...")
        
        # Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± Î¼Î­Ï„ÏÎ·ÏƒÎ· entities ÏƒÎµ Î­Î½Î± split
        def count_entities_in_split(dataset_split):
            entity_counts = {}
            i_tag_counts = {}  # ÎÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„Î® Î¼Î­Ï„ÏÎ·ÏƒÎ· Î³Î¹Î± I- tags
            
            for example in dataset_split:
                ner_tags = example['ner']
                
                for tag in ner_tags:
                    if tag.startswith('B-'):
                        # ÎÎ­Î¿ entity Î¾ÎµÎºÎ¹Î½Î¬ÎµÎ¹
                        entity_type = tag[2:]  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Î¿Ï… 'B-'
                        entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1
                    elif tag.startswith('I-'):
                        # I- tag count Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„Î¬
                        entity_type = tag[2:]  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Î¿Ï… 'I-'
                        i_tag_counts[entity_type] = i_tag_counts.get(entity_type, 0) + 1
                    # Î¤Î± I- tags Î´ÎµÎ½ Î¼ÎµÏ„ÏÎ¬Î½Îµ Ï‰Ï‚ Î½Î­Î± entities
            
            return entity_counts, i_tag_counts
        
        # ÎœÎ­Ï„ÏÎ·ÏƒÎ· entities ÏƒÏ„Î¿ training set
        train_entity_counts, train_i_tag_counts = count_entities_in_split(original_dataset['train'])
        
        print("Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ entity counts Î±Ï€ÏŒ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset:")
        for entity_type, count in sorted(train_entity_counts.items()):
            print(f"  {entity_type}: {count} entities")
        print()
    else:
        # Fallback ÏƒÏ„Î·Î½ Ï€Î±Î»Î¹Î¬ Î¼Î­Î¸Î¿Î´Î¿ Î±Î½ Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î¼Îµ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset
        train_entity_counts = {}
        train_i_tag_counts = {}
    
    # Î¡Î¥Î˜ÎœÎ™Î–Î•Î™Î£ Î²Î¬ÏƒÎµÎ¹ Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ©Î Î¼ÎµÏ„ÏÎ·Î¼Î­Î½Ï‰Î½ B- tag counts Î±Ï€ÏŒ Ï„Î¿ dataset
    # Î’Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ ÎÎ•Î©Î Î£Î©Î£Î¤Î©Î Î¼ÎµÏ„ÏÎ·Î¼Î­Î½Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½:
    # NATIONAL_LOCATION=25, UNKNOWN_LOCATION=261, PUBLIC_DOCUMENT=874, PERSON=1212, FACILITY=1041, etc.
    very_rare_threshold = 100    # NATIONAL_LOCATION: 25 entities (ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÏ€Î¬Î½Î¹Î¿!)
    rare_threshold = 1000        # UNKNOWN_LOCATION: 261, PUBLIC_DOCUMENT: 874 
    medium_threshold = 2500      # PERSON: 1212, FACILITY: 1041, LEGISLATION_REFERENCE: 2158
    common_threshold = 5000      # ORG: 3706, GPE: 4315
    
    very_rare_boost = 200.0      # Î•Î¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Ï…ÏˆÎ·Î»ÏŒ Î³Î¹Î± NATIONAL_LOCATION (25 entities)
    rare_boost_value = 75.0      # Î¥ÏˆÎ·Î»ÏŒ Î³Î¹Î± UNKNOWN_LOCATION, PUBLIC_DOCUMENT
    medium_boost_value = 25.0    # ÎœÎ­Ï„ÏÎ¹Î¿ Î³Î¹Î± PERSON, FACILITY, LEG-REFS
    common_boost_value = 15.0    # Î§Î±Î¼Î·Î»ÏŒ Î³Î¹Î± ORG, GPE
    max_cap = 150.0              # Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ cap Î³Î¹Î± extreme cases

    # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ token-level counts Î³Î¹Î± normalization
    all_labels = []
    for example in tokenized_ds["train"]:
        labels = example["labels"]
        all_labels.extend([l for l in labels if l != -100])
    
    label_counts = Counter(all_labels)
    total_samples = len(all_labels)
    
    weights = torch.ones(len(label_list))
    
    if method == "capped_sqrt_inv_freq":
        for label_id, token_count in label_counts.items():
            label_name = label_list[label_id]
            
            # Î”Î™ÎŸÎ¡Î˜Î©Î£Î—: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ entity counts
            if label_name.startswith('B-'):
                entity_type = label_name[2:]  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Î¿Ï… 'B-'
                actual_entity_count = train_entity_counts.get(entity_type, 0)
            elif label_name.startswith('I-'):
                entity_type = label_name[2:]  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· Ï„Î¿Ï… 'I-'
                # Î“Î™Î‘ I- TAGS: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ I- tag counts
                # ÎŸÎ§Î™ Ï„Î± B- tag counts!
                actual_entity_count = train_i_tag_counts.get(entity_type, token_count)
            else:
                # Î“Î¹Î± O tag, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ token count
                actual_entity_count = token_count
            
            if actual_entity_count < very_rare_threshold:
                weights[label_id] = very_rare_boost
                print(f"  Very Rare: {label_name} -> weight {very_rare_boost} (entities: {actual_entity_count})")
            elif actual_entity_count < rare_threshold:
                weights[label_id] = rare_boost_value
                print(f"  Rare: {label_name} -> weight {rare_boost_value} (entities: {actual_entity_count})")
            elif actual_entity_count < medium_threshold:
                weights[label_id] = medium_boost_value
                print(f"  Medium: {label_name} -> weight {medium_boost_value} (entities: {actual_entity_count})")
            else:
                # Î“Î¹Î± common classes, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ token count Î³Î¹Î± Ï„Î¿ weight calculation
                raw_weight = np.sqrt(total_samples / (len(label_list) * token_count))
                weights[label_id] = min(raw_weight, max_cap)
                print(f"  Common: {label_name} -> weight {min(raw_weight, max_cap):.3f} (entities: {actual_entity_count})")
        
    elif method == "inverse_freq":
        for label_id, count in label_counts.items():
            weights[label_id] = total_samples / (len(label_list) * count)
    
    elif method == "sqrt_inv_freq":
        for label_id, count in label_counts.items():
            weights[label_id] = np.sqrt(total_samples / (len(label_list) * count))
    
    elif method == "log_balanced":
        for label_id, count in label_counts.items():
            weights[label_id] = np.log(total_samples / count + 1)
    
    elif method == "effective_num":
        beta = 0.9999
        for label_id, count in label_counts.items():
            effective_num = (1 - beta**count) / (1 - beta)
            weights[label_id] = 1.0 / effective_num
    
    # Normalize weights
    weights = weights / weights.mean()
    
    # Special handling Î³Î¹Î± Ï„Î¿ "O" tag
    o_index = label_list.index('O') if 'O' in label_list else None
    if o_index is not None and method != "inverse_freq":
        weights[o_index] = weights[o_index] * 0.05  # Î‘ÎºÏŒÎ¼Î· Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ¿ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÏƒÏ„Î¯Î±ÏƒÎ·
    
    # Print weight distribution
    print(f"\nClass Weight Distribution (Method: {method}):")
    print("-" * 70)
    print(f"{'Label':<15} {'Weight':<8} {'Count':<8} {'Percentage':<10} {'Category'}")
    print("-" * 70)
    
    for i, (label, weight) in enumerate(zip(label_list, weights)):
        count = label_counts.get(i, 0)
        percentage = (count / total_samples) * 100
        if count > 50000:
            category = "Dominant"
        elif count > 5000:
            category = "Common"
        elif count > 1000:
            category = "Medium"
        elif count > 100:
            category = "Rare"
        else:
            category = "Very Rare"
        print(f"  {label:<15} {weight:<8.3f} {count:<8} {percentage:<10.2f}% {category}")
    print("-" * 70)
    print(f"Total samples: {total_samples:,}")
    print(f"Weight range: {weights.min():.3f} - {weights.max():.3f}")
    print(f"Weight std: {weights.std():.3f}")
    
    return weights.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

def apply_rare_class_augmentation(tokenized_ds, enable_augmentation=True, multiplier=3):
    """
    Î•Ï†Î±ÏÎ¼ÏŒÎ¶ÎµÎ¹ data augmentation Î³Î¹Î± rare classes Î¼Îµ on/off switch
    
    Args:
        tokenized_ds: Î¤Î¿ tokenized dataset
        enable_augmentation: True/False Î³Î¹Î± ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¯Î·ÏƒÎ·
        multiplier: Î ÏŒÏƒÎµÏ‚ Ï†Î¿ÏÎ­Ï‚ Î½Î± Ï€Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î¬ÏƒÎµÎ¹ Ï„Î± rare examples
    
    Returns:
        Î¤Î¿ (augmented Î® ÏŒÏ‡Î¹) dataset
    """
    if not enable_augmentation:
        print("\nğŸ”„ RARE CLASS AUGMENTATION: DISABLED")
        print("="*50)
        print("   Augmentation is turned OFF - using original dataset")
        print("="*50)
        return tokenized_ds
    
    print(f"\nğŸ”„ RARE CLASS AUGMENTATION: ENABLED")
    print("="*50)
    print(f"   Multiplier: {multiplier}x")
    print(f"   Target rare classes: NATIONAL_LOCATION, UNKNOWN_LOCATION, FACILITY")
    
    # ÎšÏÎ´Î¹ÎºÎ±Ï‚ augmentation Î¼ÏŒÎ½Î¿ Î³Î¹Î± Ï„Î¿ training set
    train_dataset = tokenized_ds["train"]
    
    rare_examples = []
    original_size = len(train_dataset)
    
    # ÎœÎµÏ„ÏÎ·Ï„Î®Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ rare class
    rare_counts = {
        'NATIONAL_LOCATION': 0,
        'UNKNOWN_LOCATION': 0, 
        'FACILITY': 0
    }
    
    print(f"   Scanning {original_size:,} training examples...")
    
    for example in train_dataset:
        labels = example["labels"]
        # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® label IDs ÏƒÎµ label names Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿
        label_names = [label_list[l] for l in labels if l != -100]
        
        has_rare = any(
            label.endswith(('NATIONAL_LOCATION', 'UNKNOWN_LOCATION', 'FACILITY'))
            for label in label_names
        )
        
        if has_rare:
            # Î ÏÎ¿ÏƒÎ´Î¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï€Î¿Î¹Î± rare class Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ (Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎŸ)
            rare_classes_in_example = []
            for rare_class in rare_counts.keys():
                if any(label.endswith(rare_class) for label in label_names):
                    rare_classes_in_example.append(rare_class)
                    rare_counts[rare_class] += 1
            
            # Î Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î±ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… example Î¼ÏŒÎ½Î¿ ÎœÎ™Î‘ Ï†Î¿ÏÎ¬ Î±Î½ÎµÎ¾Î¬ÏÏ„Î·Ï„Î± Î±Ï€ÏŒ Ï„Î¿ Ï€ÏŒÏƒÎ± rare classes Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹
            rare_examples.extend([example] * multiplier)
    
    print(f"\n   ğŸ“Š RARE CLASS STATISTICS:")
    print(f"   {'Class':<15} {'Found':<8} {'Will Add':<10}")
    print(f"   {'-'*35}")
    
    total_rare_examples = len(rare_examples) // multiplier
    for rare_class, count in rare_counts.items():
        will_add = count * multiplier
        print(f"   {rare_class:<15} {count:<8} {will_add:<10}")
    
    print(f"   {'-'*35}")
    print(f"   {'TOTAL':<15} {total_rare_examples:<8} {len(rare_examples):<10}")
    
    if rare_examples:
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î¿Ï… augmented training set
        from datasets import Dataset, concatenate_datasets
        augmented_examples = Dataset.from_list(rare_examples)
        augmented_train = concatenate_datasets([train_dataset, augmented_examples])
        # Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· Ï„Î¿Ï… dataset
        tokenized_ds["train"] = augmented_train
        final_size = len(augmented_train)
        increase_ratio = final_size / original_size
        print(f"\n   âœ… AUGMENTATION COMPLETED:")
        print(f"   Original training size: {original_size:,}")
        print(f"   Augmented training size: {final_size:,}")
        print(f"   Size increase: {increase_ratio:.2f}x ({((increase_ratio-1)*100):.1f}% more)")
        
    else:
        print(f"\n   âš ï¸  No rare classes found - no augmentation applied")
    
    print("="*50)
    return tokenized_ds


class_weights = None
# Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Ï…Î¼Îµ class weights Ï€Î¬Î½Ï„Î± Î³Î¹Î± Ï‡ÏÎ®ÏƒÎ· ÏƒÎµ specific configurations
print("Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ class weights...")
class_weights = compute_class_weights(tokenized_ds, label_list, method="capped_sqrt_inv_freq", original_dataset=ds)

# --- 6. ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎœÎµÏ„ÏÎ¹ÎºÏÎ½ (ENHANCED WITH VALIDATION) ---
metric = evaluate.load("seqeval")

def validate_entity_extraction(true_predictions, true_labels, label_list):
    """
    Validates that entities are correctly extracted and counted from B-/I- tag sequences
    """
    print(f"\nğŸ” ENTITY EXTRACTION VALIDATION:")
    print("="*60)
    
    # Count entities in true labels and predictions
    def extract_entities_from_sequence(sequence):
        entities = []
        current_entity = None
        
        for i, tag in enumerate(sequence):
            if tag.startswith('B-'):
                # ÎÎ­Î¿ entity Î¾ÎµÎºÎ¹Î½Î¬ÎµÎ¹
                if current_entity is not None:
                    entities.append(current_entity)
                current_entity = {
                    'type': tag[2:],  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· 'B-'
                    'start': i,
                    'end': i,
                    'tokens': [tag]
                }
            elif tag.startswith('I-') and current_entity is not None:
                # Î£Ï…Î½Î­Ï‡ÎµÎ¹Î± Ï„Î¿Ï… Ï„ÏÎ­Ï‡Î¿Î½Ï„Î¿Ï‚ entity
                entity_type = tag[2:]  # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· 'I-'
                if entity_type == current_entity['type']:
                    current_entity['end'] = i
                    current_entity['tokens'].append(tag)
                else:
                    # Incompatible I- tag, close current and start new
                    entities.append(current_entity)
                    current_entity = {
                        'type': entity_type,
                        'start': i,
                        'end': i,
                        'tokens': [tag]
                    }
            elif tag == 'O':
                # End current entity if exists
                if current_entity is not None:
                    entities.append(current_entity)
                    current_entity = None
            # Ignore other tags
        
        # Close final entity if exists
        if current_entity is not None:
            entities.append(current_entity)
            
        return entities
    
    # Sample validation on first few sequences
    sample_size = min(3, len(true_predictions))
    total_true_entities = 0
    total_pred_entities = 0
    
    for i in range(sample_size):
        true_seq = true_labels[i]
        pred_seq = true_predictions[i]
        
        true_entities = extract_entities_from_sequence(true_seq)
        pred_entities = extract_entities_from_sequence(pred_seq)
        
        total_true_entities += len(true_entities)
        total_pred_entities += len(pred_entities)
        
        print(f"Sample {i+1}:")
        print(f"  True entities: {len(true_entities)}")
        print(f"  Pred entities: {len(pred_entities)}")
        
        # Show first few entities
        for j, entity in enumerate(true_entities[:2]):
            print(f"    True {j+1}: {entity['type']} (tokens: {len(entity['tokens'])})")
        for j, entity in enumerate(pred_entities[:2]):
            print(f"    Pred {j+1}: {entity['type']} (tokens: {len(entity['tokens'])})")
    
    print(f"\nTotal entities in sample:")
    print(f"  True: {total_true_entities}")
    print(f"  Predicted: {total_pred_entities}")
    
    # Check entity type distribution
    entity_types_true = set()
    entity_types_pred = set()
    
    for seq in true_labels[:10]:  # Check first 10 sequences
        for tag in seq:
            if tag.startswith('B-') or tag.startswith('I-'):
                entity_types_true.add(tag[2:])
    
    for seq in true_predictions[:10]:
        for tag in seq:
            if tag.startswith('B-') or tag.startswith('I-'):
                entity_types_pred.add(tag[2:])
    
    print(f"\nEntity types found:")
    print(f"  In true labels: {sorted(entity_types_true)}")
    print(f"  In predictions: {sorted(entity_types_pred)}")
    
    missing_in_pred = entity_types_true - entity_types_pred
    if missing_in_pred:
        print(f"âš ï¸  Entity types missing in predictions: {missing_in_pred}")
    else:
        print(f"âœ… All true entity types found in predictions")
    
    print("="*60)

def compute_crf_specific_metrics(predictions, true_labels, model, use_crf=False):
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± CRF Î³Î¹Î± Î½Î± Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸Î¿ÏÎ¼Îµ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï„Î¿Ï…
    
    ğŸ” CRF MONITORING METRICS:
    ==========================
    1. Transition Consistency: Î ÏŒÏƒÎ¿ ÏƒÏ…Ï‡Î½Î¬ Î¿Î¹ transitions ÎµÎ¯Î½Î±Î¹ valid
    2. Entity Boundary Accuracy: Î ÏŒÏƒÎ¿ ÎºÎ±Î»Î¬ ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ Ï„Î± ÏŒÏÎ¹Î± Ï„Ï‰Î½ entities
    3. Sequence Coherence: Î ÏŒÏƒÎ¿ ÏƒÏ…Î½ÎµÏ€ÎµÎ¯Ï‚ ÎµÎ¯Î½Î±Î¹ Î¿Î¹ predicted sequences
    """
    print(f"\nğŸ” CRF-SPECIFIC METRICS ANALYSIS:")
    print("="*50)
    
    if not use_crf:
        print(f"âš ï¸  ÎœÎ¿Î½Ï„Î­Î»Î¿ Ï‡Ï‰ÏÎ¯Ï‚ CRF - Basic transition analysis:")
        
        # Î“Î¹Î± non-CRF models, Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ invalid transitions
        invalid_transitions = 0
        total_transitions = 0
        invalid_examples = []
        
        for seq_idx, pred_seq in enumerate(predictions):  # Check all sequences, not just 100
            for i in range(len(pred_seq) - 1):
                current_tag = pred_seq[i]
                next_tag = pred_seq[i + 1]
                total_transitions += 1
                
                # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± invalid transitions
                is_invalid = False
                
                # ÎšÎ±Î½ÏŒÎ½ÎµÏ‚ BIO
                if current_tag.startswith('B-') and next_tag.startswith('I-'):
                    # B-X â†’ I-Y: Valid Î¼ÏŒÎ½Î¿ Î±Î½ X == Y
                    current_entity = current_tag[2:]
                    next_entity = next_tag[2:]
                    if current_entity != next_entity:
                        is_invalid = True
                        invalid_examples.append(f"B-{current_entity} â†’ I-{next_entity}")
                
                elif current_tag.startswith('I-') and next_tag.startswith('I-'):
                    # I-X â†’ I-Y: Valid Î¼ÏŒÎ½Î¿ Î±Î½ X == Y
                    current_entity = current_tag[2:]
                    next_entity = next_tag[2:]
                    if current_entity != next_entity:
                        is_invalid = True
                        invalid_examples.append(f"I-{current_entity} â†’ I-{next_entity}")
                
                elif next_tag.startswith('I-') and not current_tag.startswith(('B-', 'I-')):
                    # O â†’ I-X: Invalid (orphaned I- tag)
                    is_invalid = True
                    invalid_examples.append(f"{current_tag} â†’ {next_tag}")
                
                if is_invalid:
                    invalid_transitions += 1
        
        transition_validity = (total_transitions - invalid_transitions) / total_transitions * 100
        print(f"   Invalid transitions: {invalid_transitions}/{total_transitions} ({100-transition_validity:.2f}%)")
        print(f"   Transition validity: {transition_validity:.2f}%")
        
        if invalid_examples:
            print(f"   Sample invalid transitions: {invalid_examples[:5]}")
        
    else:
        print(f"âœ… CRF MODEL - Advanced CRF analysis:")
        
        # CRF specific metrics
        print(f"   CRF guarantees valid transitions: 100%")
        print(f"   Using Viterbi decoding for optimal sequences")
        
        # Transition matrix analysis (Î±Î½ Î­Ï‡Î¿Ï…Î¼Îµ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿)
        if hasattr(model, 'crf'):
            crf_layer = model.crf
            transition_matrix = crf_layer.transitions.detach().cpu()
            
            print(f"\nğŸ“Š CRF TRANSITION MATRIX ANALYSIS:")
            print(f"   Matrix shape: {transition_matrix.shape}")
            print(f"   Min transition score: {transition_matrix.min().item():.3f}")
            print(f"   Max transition score: {transition_matrix.max().item():.3f}")
            print(f"   Mean transition score: {transition_matrix.mean().item():.3f}")
            
            # Î‘Î½Î¬Î»Ï…ÏƒÎ· high/low probability transitions
            num_labels = transition_matrix.shape[0]
            high_prob_transitions = (transition_matrix > transition_matrix.mean()).sum().item()
            low_prob_transitions = (transition_matrix < transition_matrix.mean()).sum().item()
            
            print(f"   High probability transitions: {high_prob_transitions}/{num_labels*num_labels}")
            print(f"   Low probability transitions: {low_prob_transitions}/{num_labels*num_labels}")
            
            # Start/End transition analysis
            start_transitions = crf_layer.start_transitions.detach().cpu()
            end_transitions = crf_layer.end_transitions.detach().cpu()
            
            print(f"\nğŸš€ START/END TRANSITIONS:")
            print(f"   Start transition range: {start_transitions.min().item():.3f} to {start_transitions.max().item():.3f}")
            print(f"   End transition range: {end_transitions.min().item():.3f} to {end_transitions.max().item():.3f}")
            
            # Find most/least likely starting tags
            start_probs = torch.softmax(start_transitions, dim=0)
            most_likely_start = start_probs.argmax().item()
            least_likely_start = start_probs.argmin().item()
            
            print(f"   Most likely start tag: {label_list[most_likely_start]} ({start_probs[most_likely_start].item():.3f})")
            print(f"   Least likely start tag: {label_list[least_likely_start]} ({start_probs[least_likely_start].item():.3f})")
    
    # Entity boundary analysis (ÎºÎ¿Î¹Î½ÏŒ Î³Î¹Î± ÏŒÎ»Î± Ï„Î± models)
    print(f"\nğŸ¯ ENTITY BOUNDARY ANALYSIS:")
    
    def analyze_entity_boundaries(pred_sequences, true_sequences):
        correct_starts = 0
        correct_ends = 0
        total_entities = 0
        boundary_errors = []
        
        for pred_seq, true_seq in zip(pred_sequences, true_sequences):  # Analyze all sequences
            # Find entities in both sequences
            true_entities = []
            pred_entities = []
            
            # Extract true entities
            current_entity = None
            for i, tag in enumerate(true_seq):
                if tag.startswith('B-'):
                    if current_entity:
                        true_entities.append(current_entity)
                    current_entity = {'type': tag[2:], 'start': i, 'end': i}
                elif tag.startswith('I-') and current_entity and tag[2:] == current_entity['type']:
                    current_entity['end'] = i
                else:
                    if current_entity:
                        true_entities.append(current_entity)
                        current_entity = None
            if current_entity:
                true_entities.append(current_entity)
            
            # Extract predicted entities
            current_entity = None
            for i, tag in enumerate(pred_seq):
                if tag.startswith('B-'):
                    if current_entity:
                        pred_entities.append(current_entity)
                    current_entity = {'type': tag[2:], 'start': i, 'end': i}
                elif tag.startswith('I-') and current_entity and tag[2:] == current_entity['type']:
                    current_entity['end'] = i
                else:
                    if current_entity:
                        pred_entities.append(current_entity)
                        current_entity = None
            if current_entity:
                pred_entities.append(current_entity)
            
            # Check boundary accuracy
            for true_entity in true_entities:
                total_entities += 1
                
                # Find matching predicted entity by type and position
                for pred_entity in pred_entities:
                    if (true_entity['type'] == pred_entity['type'] and 
                        abs(true_entity['start'] - pred_entity['start']) <= 1):  # Allow 1-token tolerance
                        
                        if true_entity['start'] == pred_entity['start']:
                            correct_starts += 1
                        if true_entity['end'] == pred_entity['end']:
                            correct_ends += 1
                        break
                else:
                    # No matching predicted entity found
                    boundary_errors.append(f"Missing {true_entity['type']} at {true_entity['start']}-{true_entity['end']}")
        
        return correct_starts, correct_ends, total_entities, boundary_errors
    
    correct_starts, correct_ends, total_entities, boundary_errors = analyze_entity_boundaries(predictions, true_labels)
    
    if total_entities > 0:
        start_accuracy = correct_starts / total_entities * 100
        end_accuracy = correct_ends / total_entities * 100
        print(f"   Correct entity starts: {correct_starts}/{total_entities} ({start_accuracy:.2f}%)")
        print(f"   Correct entity ends: {correct_ends}/{total_entities} ({end_accuracy:.2f}%)")
        
        if boundary_errors:
            print(f"   Sample boundary errors: {boundary_errors[:3]}")
    else:
        print(f"   No entities found in sample for boundary analysis")
    
    # Sequence coherence score
    print(f"\nğŸ”— SEQUENCE COHERENCE ANALYSIS:")
    
    def compute_coherence_score(sequences):
        coherence_scores = []
        
        for seq in sequences:  # Analyze all sequences, not just 50
            score = 0
            total_positions = len(seq) - 1
            
            if total_positions == 0:
                continue
                
            for i in range(total_positions):
                current = seq[i]
                next_tag = seq[i + 1]
                
                # Coherence rules (higher score = more coherent)
                if current == 'O' and next_tag.startswith(('O', 'B-')):
                    score += 1  # Valid: O can be followed by O or B-
                elif current.startswith('B-') and (next_tag == 'O' or next_tag.startswith('B-') or 
                                                   (next_tag.startswith('I-') and next_tag[2:] == current[2:])):
                    score += 1  # Valid: B-X can be followed by O, B-*, or I-X
                elif current.startswith('I-') and (next_tag == 'O' or next_tag.startswith('B-') or 
                                                   (next_tag.startswith('I-') and next_tag[2:] == current[2:])):
                    score += 1  # Valid: I-X can be followed by O, B-*, or I-X
            
            if total_positions > 0:
                coherence_scores.append(score / total_positions)
        
        return coherence_scores
    
    coherence_scores = compute_coherence_score(predictions)
    
    if coherence_scores:
        avg_coherence = sum(coherence_scores) / len(coherence_scores) * 100
        print(f"   Average sequence coherence: {avg_coherence:.2f}%")
        print(f"   Coherence range: {min(coherence_scores)*100:.1f}% - {max(coherence_scores)*100:.1f}%")
        
        if use_crf:
            print(f"   âœ… CRF should achieve 100% coherence (structural constraints)")
        else:
            print(f"   âš ï¸  Standard model may have coherence issues")
    
    print("="*50)
    
    # Return metrics Î³Î¹Î± logging
    crf_metrics = {
        'boundary_start_accuracy': correct_starts / max(total_entities, 1) * 100,
        'boundary_end_accuracy': correct_ends / max(total_entities, 1) * 100,
        'sequence_coherence': sum(coherence_scores) / max(len(coherence_scores), 1) * 100 if coherence_scores else 0,
    }
    
    if not use_crf and 'transition_validity' in locals():
        crf_metrics['transition_validity'] = transition_validity
    
    return crf_metrics

# --- 7. Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ & Data Collator ---
LEARNING_RATE = 5e-5
TRAIN_BATCH_SIZE = 8
EVAL_BATCH_SIZE = 16 
NUM_EPOCHS = 6
WEIGHT_DECAY = 0.01
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

# --- 8. Î‘Î¥Î¤ÎŸÎœÎ‘Î¤Î— Î”ÎŸÎšÎ™ÎœÎ— ÎŸÎ›Î©Î Î¤Î©Î CONFIGURATIONS ---
def test_all_loss_configurations():
    """Î”Î¿ÎºÎ¹Î¼Î¬Î¶ÎµÎ¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ configurations ÎºÎ±Î¹ ÏƒÏ…Î³ÎºÏÎ¯Î½ÎµÎ¹ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±"""
    
    # Access to global variables
    global tokenized_ds, label_list, id2label, label2id, ds, data_collator
    
    # ğŸ“ ÎˆÎ›Î•Î“Î§ÎŸÎ£ EXISTING RESULTS - Skip configurations Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ Î®Î´Î· ÎµÎºÏ„ÎµÎ»ÎµÏƒÏ„ÎµÎ¯
    incremental_save_file = "progressive_results_with_crf.json"
    completed_configs = set()
    
    if not FORCE_RERUN:  # ÎœÏŒÎ½Î¿ Î±Î½ Î´ÎµÎ½ Î¸Î­Î»Î¿Ï…Î¼Îµ force rerun
        try:
            with open(incremental_save_file, 'r', encoding='utf-8') as f:
                existing_results = json.load(f)
                completed_configs = set(existing_results.keys())
                print(f"ğŸ“‚ Î’ÏÎ­Î¸Î·ÎºÎ±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î±: {list(completed_configs)}")
        except (FileNotFoundError, json.JSONDecodeError):
            existing_results = {}
            print(f"ğŸ“‚ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± - Î¾ÎµÎºÎ¹Î½Î¬Ï‰ Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏÏ‡Î®")
    else:
        print(f"ğŸ”„ FORCE_RERUN=True - ÎÎ±Î½Î±Ï„ÏÎ­Ï‡Ï‰ ÏŒÎ»Î± Ï„Î± tests")
        existing_results = {}
    
    # ÎœÎŸÎÎŸ CROSS ENTROPY - Î‘Ï€Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· configuration Î³Î¹Î± Î²Î±ÏƒÎ¹ÎºÎ® ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·
    configurations = [
        # ÎœÎŸÎÎŸ Cross Entropy Ï‡Ï‰ÏÎ¯Ï‚ ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ techniques
        {
            "name": "cross_entropy_only",
            "loss_type": "standard",  # Î’Î±ÏƒÎ¹ÎºÏŒ Cross Entropy
            "focal_gamma": None,
            "focal_alpha": None,
            "use_weights": False,  # Î”Î•Î Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ class weights
            "use_crf": False,      # Î”Î•Î Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ CRF
            "weight_method": None
        }
    ]
    
    # â­ï¸ Î¦Î™Î›Î¤Î¡Î‘Î¡Î™Î£ÎœÎ‘ - Skip configurations Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ Î®Î´Î· Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¸ÎµÎ¯
    remaining_configs = [config for config in configurations if config['name'] not in completed_configs]
    
    if not remaining_configs:
        print(f"âœ… ÎŒÎ»ÎµÏ‚ Î¿Î¹ configurations Î­Ï‡Î¿Ï…Î½ Î®Î´Î· Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¸ÎµÎ¯!")
        print(f"ğŸ” Î¤ÎµÎ»Î¹ÎºÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±: {list(completed_configs)}")
        return existing_results
    
    print(f"â­ï¸  Configurations Ï€ÏÎ¿Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·: {[c['name'] for c in remaining_configs]}")
    print(f"â¹ï¸  Configurations Ï€Î¿Ï… Î¸Î± Ï€Î±ÏÎ±Î»ÎµÎ¹Ï†Î¸Î¿ÏÎ½: {list(completed_configs)}")
        
    results = existing_results  # ÎÎµÎºÎ¹Î½Î¬Ï‰ Î¼Îµ Ï„Î± Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± results
    # ğŸ”„ RARE CLASS AUGMENTATION (Î¼Î¹Î± Ï†Î¿ÏÎ¬ Î³Î¹Î± ÏŒÎ»Î± Ï„Î± experiments)
    tokenized_ds = apply_rare_class_augmentation(
        tokenized_ds, 
        enable_augmentation=False,  # Î‘Î Î•ÎÎ•Î¡Î“ÎŸÎ ÎŸÎ™Î—ÎœÎ•ÎÎŸ - Î”Î•Î ÎºÎ¬Î½Î¿Ï…Î¼Îµ augmentation
        multiplier=3  # 3x multiplier Î³Î¹Î± rare classes (Î´ÎµÎ½ ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÏ„Î±Î¹ ÏŒÏ„Î±Î½ disabled)
    )
    for config in remaining_configs:  # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Ï remaining_configs Î±Î½Ï„Î¯ Î³Î¹Î± configurations
        print(f"\n{'='*80}")
        print(f"ğŸ”„ Î”Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰ configuration: {config['name']}")
        print(f"   Model: {'ROBERTa+CRF' if config.get('use_crf') else 'ROBERTa'}")
        print(f"   Loss: {config['loss_type']}, Gamma: {config.get('focal_gamma', 'N/A')}")
        print(f"   Alpha: {config.get('focal_alpha', 'N/A')}, Weight method: {config.get('weight_method', 'None')}")
        print(f"{'='*80}")
        
        # Compute specific weights for this configuration
        config_weights = None
        if config.get("use_weights") and config.get("weight_method"):
            print(f"ğŸ”§ Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶Ï‰ weights Î¼Îµ Î¼Î­Î¸Î¿Î´Î¿: {config['weight_method']}")
            config_weights = compute_class_weights(tokenized_ds, label_list, method=config["weight_method"], original_dataset=ds)
        
        # Create appropriate model based on configuration
        if config.get("use_crf"):
            # Create ROBERTa+CRF model
            print("ğŸ—ï¸ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ROBERTa+CRF Î¼Î¿Î½Ï„Î­Î»Î¿Ï…...")
            bert_config = AutoConfig.from_pretrained("AI-team-UoA/GreekLegalRoBERTa_v2")
            bert_config.num_labels = len(label_list)
            bert_config.id2label = id2label
            bert_config.label2id = label2id
            
            # ğŸ” VALIDATION: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ config Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î¼Îµ Ï„Î± dataset labels
            print(f"ğŸ” MODEL CONFIG VALIDATION:")
            print(f"   Dataset labels: {len(label_list)}")
            print(f"   Model num_labels: {bert_config.num_labels}")
            print(f"   ID2Label mapping: {len(bert_config.id2label)} entries")
            print(f"   Label2ID mapping: {len(bert_config.label2id)} entries")
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± labels Î±Ï€ÏŒ Ï„Î¿ dataset Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ„Î¿ model config
            for i, label in enumerate(label_list):
                assert i in bert_config.id2label, f"Label ID {i} missing from id2label"
                assert label in bert_config.label2id, f"Label '{label}' missing from label2id"
                assert bert_config.id2label[i] == label, f"ID2Label mismatch: {bert_config.id2label[i]} != {label}"
                assert bert_config.label2id[label] == i, f"Label2ID mismatch: {bert_config.label2id[label]} != {i}"
            print(f"   âœ… ÎŒÎ»Î± Ï„Î± dataset labels Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ Ï„Î¿ model config")
            
            model = BertCRFForTokenClassification(
                config=bert_config,
                num_labels=len(label_list),
                bert_model_name="AI-team-UoA/GreekLegalRoBERTa_v2"
            )
            
            # ğŸ” Î•ÎšÎ¤Î•ÎÎ—Î£ CRF VALIDATION
            print(f"\nğŸ” CRF IMPLEMENTATION VALIDATION:")
            print("="*60)
            
            # 1. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ CRF architecture
            print(f"ğŸ“ CRF ARCHITECTURE:")
            print(f"   CRF num_tags: {model.crf.num_tags}")
            print(f"   Expected num_labels: {len(label_list)}")
            print(f"   Batch_first: {model.crf.batch_first}")
            assert model.crf.num_tags == len(label_list), f"CRF num_tags mismatch!"
            print(f"   âœ… CRF architecture is correct")
            
            # 2. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ transition matrix initialization
            print(f"\nğŸ”— TRANSITION MATRIX VALIDATION:")
            print(f"   Transition matrix shape: {model.crf.transitions.shape}")
            expected_shape = (len(label_list), len(label_list))
            assert model.crf.transitions.shape == expected_shape, f"Transition matrix shape mismatch!"
            print(f"   Expected shape: {expected_shape}")
            print(f"   âœ… Transition matrix shape is correct")
            
            # 3. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ start/end transitions
            print(f"\nğŸš€ START/END TRANSITIONS:")
            print(f"   Start transitions shape: {model.crf.start_transitions.shape}")
            print(f"   End transitions shape: {model.crf.end_transitions.shape}")
            assert model.crf.start_transitions.shape == (len(label_list),), "Start transitions shape wrong!"
            assert model.crf.end_transitions.shape == (len(label_list),), "End transitions shape wrong!"
            print(f"   âœ… Start/End transitions are correctly initialized")
            
            # 4. B-I-O Constraint Validation
            print(f"\nğŸ·ï¸  B-I-O CONSTRAINT VALIDATION:")
            
            # Î‘Î½Î¬Î»Ï…ÏƒÎ· label structure
            o_indices = [i for i, label in enumerate(label_list) if label == 'O']
            b_indices = [(i, label[2:]) for i, label in enumerate(label_list) if label.startswith('B-')]
            i_indices = [(i, label[2:]) for i, label in enumerate(label_list) if label.startswith('I-')]
            
            print(f"   O tag indices: {o_indices}")
            print(f"   B- tag count: {len(b_indices)} tags")
            print(f"   I- tag count: {len(i_indices)} tags")
            
            # Entity type consistency check
            b_entity_types = set(entity_type for _, entity_type in b_indices)
            i_entity_types = set(entity_type for _, entity_type in i_indices)
            
            print(f"   B- entity types: {sorted(b_entity_types)}")
            print(f"   I- entity types: {sorted(i_entity_types)}")
            
            missing_i_tags = b_entity_types - i_entity_types
            extra_i_tags = i_entity_types - b_entity_types
            
            if missing_i_tags:
                print(f"   âš ï¸  Entity types with B- but no I-: {missing_i_tags}")
                print(f"      (This is OK for single-token entities)")
            
            if extra_i_tags:
                print(f"   âŒ Entity types with I- but no B-: {extra_i_tags}")
                print(f"      This could cause CRF training issues!")
            else:
                print(f"   âœ… All I- tags have corresponding B- tags")
            
            # 5. Sample forward pass validation
            print(f"\nğŸ§ª SAMPLE FORWARD PASS VALIDATION:")
            
            # Create a small test sample
            sample_input = tokenized_ds["train"].select([0])
            
            # Test forward pass
            model.eval()
            with torch.no_grad():
                test_input = {
                    'input_ids': torch.tensor([sample_input['input_ids'][0]]),
                    'attention_mask': torch.tensor([sample_input['attention_mask'][0]]),
                    'labels': torch.tensor([sample_input['labels'][0]])
                }
                
                try:
                    outputs = model(**test_input)
                    
                    print(f"   âœ… Forward pass successful")
                    print(f"   Loss shape: {outputs['loss'].shape}")
                    print(f"   Loss value: {outputs['loss'].item():.4f}")
                    print(f"   Logits shape: {outputs['logits'].shape}")
                    
                    # Test decode functionality
                    predictions = model.decode(
                        input_ids=test_input['input_ids'],
                        attention_mask=test_input['attention_mask']
                    )
                    
                    print(f"   âœ… CRF decode successful")
                    print(f"   Decoded sequence length: {len(predictions[0])}")
                    
                    # Validate decoded sequence
                    decoded_labels = [label_list[p] for p in predictions[0][:10]]  # First 10 predictions
                    print(f"   Sample decoded labels: {decoded_labels}")
                    
                    # Check for invalid label IDs
                    invalid_predictions = [p for p in predictions[0] if p >= len(label_list) or p < 0]
                    if invalid_predictions:
                        print(f"   âŒ Invalid predictions found: {invalid_predictions}")
                    else:
                        print(f"   âœ… All predictions are valid label IDs")
                        
                except Exception as e:
                    print(f"   âŒ Forward pass failed: {str(e)}")
                    raise e
            
            print(f"\nğŸ¯ CRF EXPECTED BEHAVIOR:")
            print(f"   During training:")
            print(f"     - CRF will learn transition probabilities between labels")
            print(f"     - Invalid transitions (e.g., B-PERSON â†’ I-ORG) will get low probability")
            print(f"     - Valid transitions (e.g., B-PERSON â†’ I-PERSON, O â†’ B-*) will get high probability")
            print(f"   During inference:")
            print(f"     - Viterbi algorithm will find the most probable valid sequence")
            print(f"     - Output will always respect B-I-O constraints")
            print(f"     - No orphaned I- tags without preceding B- tag")
            
            print("="*60)
        else:
            # Standard ROBERTa model
            print("ğŸ—ï¸ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± standard ROBERTa Î¼Î¿Î½Ï„Î­Î»Î¿Ï…...")
            model = AutoModelForTokenClassification.from_pretrained(
                "AI-team-UoA/GreekLegalRoBERTa_v2", 
                num_labels=len(label_list), 
                id2label=id2label, 
                label2id=label2id
            )
            
            # ğŸ” VALIDATION: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ config Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Î¼Îµ Ï„Î± dataset labels  
            print(f"ğŸ” MODEL CONFIG VALIDATION:")
            print(f"   Dataset labels: {len(label_list)}")
            print(f"   Model num_labels: {model.config.num_labels}")
            print(f"   ID2Label mapping: {len(model.config.id2label)} entries")
            print(f"   Label2ID mapping: {len(model.config.label2id)} entries")
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± labels Î±Ï€ÏŒ Ï„Î¿ dataset Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ„Î¿ model config
            for i, label in enumerate(label_list):
                assert i in model.config.id2label, f"Label ID {i} missing from id2label"
                assert label in model.config.label2id, f"Label '{label}' missing from label2id"
                assert model.config.id2label[i] == label, f"ID2Label mismatch: {model.config.id2label[i]} != {label}"
                assert model.config.label2id[label] == i, f"Label2ID mismatch: {model.config.label2id[label]} != {i}"
            print(f"   âœ… ÎŒÎ»Î± Ï„Î± dataset labels Ï„Î±Î¹ÏÎ¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ Ï„Î¿ model config")
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=f"./ner_results/test_{config['name']}",
            logging_dir=f"./ner_logs/test_{config['name']}",
            learning_rate=LEARNING_RATE,
            per_device_train_batch_size=TRAIN_BATCH_SIZE,
            per_device_eval_batch_size=EVAL_BATCH_SIZE,
            num_train_epochs=NUM_EPOCHS,
            weight_decay=WEIGHT_DECAY,
            eval_strategy="epoch",
            save_strategy="no",
            load_best_model_at_end=False,
            seed=44,
            fp16=torch.cuda.is_available(),
        )
        
        # Trainer Î¼Îµ specific configuration
        trainer_kwargs = {
            "model": model,
            "args": training_args,
            "train_dataset": tokenized_ds["train"],
            "eval_dataset": tokenized_ds["validation"],
            "data_collator": data_collator,
            "loss_type": config["loss_type"],
            "use_crf": config.get("use_crf", False),
            "debug_mode": True
        }
        
        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· focal loss parameters Î¼ÏŒÎ½Î¿ Î±Î½ Î´ÎµÎ½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ CRF
        if not config.get("use_crf"):
            if config.get("focal_gamma"):
                trainer_kwargs["focal_gamma"] = config["focal_gamma"]
            if config.get("focal_alpha"):
                trainer_kwargs["focal_alpha"] = config["focal_alpha"]
        
        if config_weights is not None:
            trainer_kwargs["class_weights"] = config_weights
        
        trainer = FlexibleTrainer(**trainer_kwargs)
        
        # Î‘Î½Î¬Î¸ÎµÏƒÎ· Ï„Î·Ï‚ custom compute_metrics Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
        trainer.compute_metrics = trainer.compute_metrics_with_model_info
        
        # --- Î•Î›Î•Î“Î§ÎŸÎ£ LABELS Î Î¡Î™Î Î¤ÎŸ TRAINING ---
        print("\n" + "="*70)
        print("ğŸ“Š Î‘ÎÎ‘Î›Î¥Î£Î— LABELS Î£Î¤ÎŸ TOKENIZED TRAINING SET")
        print("="*70)
        
        all_train_labels = []
        for example in tokenized_ds["train"]:
            labels = example["labels"]
            all_train_labels.extend([l for l in labels if l != -100])
        
        train_label_counts = Counter(all_train_labels)
        total_train_labels = len(all_train_labels)
        
        print(f"Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ valid labels ÏƒÏ„Î¿ training set: {total_train_labels:,}")
        print(f"ÎœÎ¿Î½Î±Î´Î¹ÎºÎ¬ labels: {len(train_label_counts)}")
        print()
        
        # Î‘Î½Î¬Î»Ï…ÏƒÎ· B- ÎºÎ±Î¹ I- tags
        b_count = sum(count for label_id, count in train_label_counts.items() 
                     if label_list[label_id].startswith('B-'))
        i_count = sum(count for label_id, count in train_label_counts.items() 
                     if label_list[label_id].startswith('I-'))
        o_count = train_label_counts.get(0, 0)  # O tag ÎµÎ¯Î½Î±Î¹ ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ index 0
        
        print(f"ğŸ·ï¸  Î£Î¥ÎÎŸÎ›Î™ÎšÎ— ÎšÎ‘Î¤Î‘ÎÎŸÎœÎ—:")
        print(f"   O tags:  {o_count:,} ({(o_count/total_train_labels)*100:.2f}%)")
        print(f"   B- tags: {b_count:,} ({(b_count/total_train_labels)*100:.2f}%)")
        print(f"   I- tags: {i_count:,} ({(i_count/total_train_labels)*100:.2f}%)")
        print()
        
        # Î›ÎµÏ€Ï„Î¿Î¼ÎµÏÎ®Ï‚ Î±Î½Î¬Î»Ï…ÏƒÎ· Î±Î½Î¬ label
        print(f"ğŸ“‹ Î›Î•Î Î¤ÎŸÎœÎ•Î¡Î—Î£ ÎšÎ‘Î¤Î‘ÎÎŸÎœÎ— Î‘ÎÎ‘ LABEL:")
        print(f"{'LABEL':<20} {'COUNT':<10} {'PERCENTAGE':<12} {'TYPE'}")
        print("-" * 55)
        
        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·: Ï€ÏÏÏ„Î± O, Î¼ÎµÏ„Î¬ B-, Î¼ÎµÏ„Î¬ I-
        sorted_labels = []
        if 0 in train_label_counts and label_list[0] == 'O':
            sorted_labels.append((0, 'O'))
        
        # B- tags
        b_labels = [(label_id, label_list[label_id]) for label_id in train_label_counts.keys() 
                   if label_list[label_id].startswith('B-')]
        sorted_labels.extend(sorted(b_labels, key=lambda x: x[1]))
        
        # I- tags
        i_labels = [(label_id, label_list[label_id]) for label_id in train_label_counts.keys() 
                   if label_list[label_id].startswith('I-')]
        sorted_labels.extend(sorted(i_labels, key=lambda x: x[1]))
        
        for label_id, label_name in sorted_labels:
            count = train_label_counts[label_id]
            percentage = (count / total_train_labels) * 100
            
            if label_name == 'O':
                label_type = "NON-ENTITY"
            elif label_name.startswith('B-'):
                label_type = "BEGIN"
            elif label_name.startswith('I-'):
                label_type = "INSIDE"
            else:
                label_type = "OTHER"
                
            print(f"{label_name:<20} {count:<10,} {percentage:<12.2f}% {label_type}")
        
        print("-" * 55)
        print(f"{'TOTAL':<20} {total_train_labels:<10,} {'100.00%':<12}")
        print("="*70)
        
        # Train ÎºÎ±Î¹ evaluate
        print("ğŸš€ Training Î¾ÎµÎºÎ¹Î½Î¬ÎµÎ¹...")
        trainer.train()
        
        print("ğŸ“Š Evaluation ÏƒÏ„Î¿ test set...")
        
        # ğŸ” PRE-EVALUATION VALIDATION: ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹ ÏŒÎ»Î± Ï„Î± labels
        print(f"\nğŸ” PRE-EVALUATION MODEL VALIDATION:")
        print("="*50)
        
        # Î”Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÎ® Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÎµ Î­Î½Î± Î¼Î¹ÎºÏÏŒ batch Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿
        test_sample = tokenized_ds["test"].select(range(min(10, len(tokenized_ds["test"]))))
        sample_predictions = trainer.predict(test_sample)
        
        if not config.get("use_crf"):
            # Î“Î¹Î± standard ROBERTa
            sample_preds = np.argmax(sample_predictions.predictions, axis=2)
            unique_predicted_labels = set()
            
            for example_preds, example in zip(sample_preds, test_sample):
                labels = example["labels"]
                for pred, label in zip(example_preds, labels):
                    if label != -100:  # ÎœÏŒÎ½Î¿ valid positions
                        unique_predicted_labels.add(pred)
            
            print(f"Model predicted {len(unique_predicted_labels)} unique labels")
            print(f"Expected {len(label_list)} total labels")
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± invalid predictions
            invalid_preds = [p for p in unique_predicted_labels if p >= len(label_list)]
            if invalid_preds:
                print(f"âš ï¸  WARNING: Model produced invalid predictions: {invalid_preds}")
            else:
                print(f"âœ… All predictions are within valid label range")
                
            # Î”ÎµÎ¯Î³Î¼Î± Î±Ï€ÏŒ Ï„Î± predicted labels
            sample_pred_labels = [label_list[p] for p in sorted(unique_predicted_labels)[:10]]
            print(f"Sample predicted labels: {sample_pred_labels}")
        
        print("="*50)
        
        test_results = trainer.evaluate(eval_dataset=tokenized_ds["test"], metric_key_prefix="test")

        # ğŸ” POST-EVALUATION TEST SET VALIDATION
        print(f"\nğŸ” TEST SET EVALUATION VALIDATION:")
        print("="*60)
        
        # Get detailed predictions for analysis
        test_predictions = trainer.predict(tokenized_ds["test"])
        
        if not config.get("use_crf"):
            # Detailed analysis Î³Î¹Î± standard ROBERTa
            test_preds = np.argmax(test_predictions.predictions, axis=2)
            
            # Convert to label sequences Î³Î¹Î± detailed analysis
            test_pred_sequences = []
            test_true_sequences = []
            
            for example_preds, example in zip(test_preds, tokenized_ds["test"]):
                true_labels = example["labels"]
                pred_sequence = [label_list[p] for p, l in zip(example_preds, true_labels) if l != -100]
                true_sequence = [label_list[l] for l in true_labels if l != -100]
                
                if len(pred_sequence) > 0:  # Only non-empty sequences
                    test_pred_sequences.append(pred_sequence)
                    test_true_sequences.append(true_sequence)
            
            print(f"Test sequences analyzed: {len(test_pred_sequences)}")
            
            # Entity-level counting validation
            def count_entities_in_sequences(sequences):
                entity_counts = {}
                total_entities = 0
                total_tags = 0
                
                for seq in sequences:
                    for tag in seq:
                        if tag.startswith('B-'):
                            entity_type = tag[2:]
                            entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1
                            total_entities += 1
                        if tag.startswith('B-') or tag.startswith('I-'):
                            total_tags += 1
                
                return entity_counts, total_entities, total_tags
            
            true_entity_counts, true_entities, true_tags = count_entities_in_sequences(test_true_sequences)
            pred_entity_counts, pred_entities, pred_tags = count_entities_in_sequences(test_pred_sequences)
            
            print(f"\nEntity counting validation:")
            print(f"  True entities (B- tags): {true_entities:,}")
            print(f"  Predicted entities (B- tags): {pred_entities:,}")
            print(f"  True B-/I- tags total: {true_tags:,}")
            print(f"  Predicted B-/I- tags total: {pred_tags:,}")
            
            if true_entities > 0:
                print(f"  Average I- tags per true entity: {(true_tags - true_entities) / true_entities:.2f}")
            if pred_entities > 0:
                print(f"  Average I- tags per pred entity: {(pred_tags - pred_entities) / pred_entities:.2f}")
            
            # Per-entity-type analysis
            print(f"\nPer-entity-type validation:")
            all_entity_types = set(true_entity_counts.keys()) | set(pred_entity_counts.keys())
            
            print(f"{'Entity Type':<15} {'True':<8} {'Pred':<8} {'Ratio':<8}")
            print("-" * 45)
            for entity_type in sorted(all_entity_types):
                true_count = true_entity_counts.get(entity_type, 0)
                pred_count = pred_entity_counts.get(entity_type, 0)
                ratio = pred_count / max(true_count, 1)
                print(f"{entity_type:<15} {true_count:<8} {pred_count:<8} {ratio:<8.2f}")
            
            # Test F1 validation - cross-check with seqeval
            print(f"\nF1 Score validation:")
            test_f1 = test_results.get('test_f1', 0)
            print(f"  Reported F1: {test_f1:.4f}")
            print(f"  This F1 is calculated at ENTITY level:")
            print(f"    - Each complete entity (B- + I-*) counts as 1")
            print(f"    - Partial matches count as 0")
            print(f"    - Entity boundaries must be exact")
            
            # Manual entity F1 calculation Î³Î¹Î± cross-validation
            from collections import defaultdict
            true_entities_set = set()
            pred_entities_set = set()
            
            for seq_idx, (true_seq, pred_seq) in enumerate(zip(test_true_sequences[:100], test_pred_sequences[:100])):
                # Extract entities as (seq_idx, start, end, type) tuples
                def extract_entities(sequence):
                    entities = set()
                    current_entity = None
                    
                    for pos, tag in enumerate(sequence):
                        if tag.startswith('B-'):
                            if current_entity:
                                entities.add(current_entity)
                            current_entity = (seq_idx, pos, pos, tag[2:])
                        elif tag.startswith('I-') and current_entity and tag[2:] == current_entity[3]:
                            current_entity = (current_entity[0], current_entity[1], pos, current_entity[3])
                        else:
                            if current_entity:
                                entities.add(current_entity)
                            current_entity = None
                    
                    if current_entity:
                        entities.add(current_entity)
                    return entities
                
                true_entities_set.update(extract_entities(true_seq))
                pred_entities_set.update(extract_entities(pred_seq))
            
            # Calculate manual F1 Î³Î¹Î± verification
            correct_entities = len(true_entities_set & pred_entities_set)
            manual_precision = correct_entities / max(len(pred_entities_set), 1)
            manual_recall = correct_entities / max(len(true_entities_set), 1)
            manual_f1 = 2 * manual_precision * manual_recall / max(manual_precision + manual_recall, 1e-8)
            
            print(f"  Manual validation (first 100 sequences):")
            print(f"    True entities: {len(true_entities_set)}")
            print(f"    Pred entities: {len(pred_entities_set)}")
            print(f"    Correct entities: {correct_entities}")
            print(f"    Manual F1: {manual_f1:.4f}")
            
        print("="*60)

        # Error analysis Î³Î¹Î± rare classes
        if not config.get("use_crf"):  # ÎœÏŒÎ½Î¿ Î³Î¹Î± standard ROBERTa models
            raw_preds = trainer.predict(tokenized_ds["test"]).predictions
            rare_classes = ["NATIONAL_LOCATION", "UNKNOWN_LOCATION", "FACILITY"]  
            export_rare_class_errors(tokenized_ds["test"], raw_preds, label_list, rare_classes)
        
        # Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Ï…Î¼Îµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚
        cleaned_results = {k.replace('test_', ''): v for k, v in test_results.items()}
        
        results[config['name']] = {
            "metrics": cleaned_results,
            "config": config,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "detailed_explanation": {
                "tag_level_metrics": {
                    "micro_f1": "Tag-weighted F1 score (Ï€Î¹Î¿ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Î¼Îµ Ï„Î¿ training)",
                    "macro_f1": "Unweighted average F1 across entity types",
                    "tag_level_accuracy": "Accuracy of individual tag predictions"
                },
                "entity_level_metrics": {
                    "f1": "Entity-level F1 (complete entity must match exactly)",
                    "precision": "Entity-level precision",
                    "recall": "Entity-level recall"
                },
                "entity_type_breakdown": {
                    "b_accuracy": "Accuracy Î³Î¹Î± B- tags Ï„Î¿Ï… entity type",
                    "i_accuracy": "Accuracy Î³Î¹Î± I- tags Ï„Î¿Ï… entity type", 
                    "combined_f1": "F1 score Î³Î¹Î± ÏŒÎ»Î± Ï„Î± tags (B- + I-) Ï„Î¿Ï… entity type"
                }
            }
        }
        
        # ğŸ’¾ INCREMENTAL SAVING - Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Ï…Î¼Îµ Î±Î¼Î­ÏƒÏ‰Ï‚ Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï‡Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î± Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î±
        incremental_save_file = "progressive_results_with_crf.json"
        try:
            # Î¦Î¿ÏÏ„ÏÎ½Î¿Ï…Î¼Îµ Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± results Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
            try:
                with open(incremental_save_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                existing_results = {}
            
            # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î¿ Î½Î­Î¿ result
            existing_results[config['name']] = results[config['name']]
            
            # Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Ï…Î¼Îµ Ï„Î¿ ÎµÎ½Î·Î¼ÎµÏÏ‰Î¼Î­Î½Î¿ Î±ÏÏ‡ÎµÎ¯Î¿
            with open(incremental_save_file, 'w', encoding='utf-8') as f:
                clean_existing_results = clean_for_json_serialization(existing_results)
                json.dump(clean_existing_results, f, indent=4, sort_keys=True, ensure_ascii=False)
            
            print(f"ğŸ’¾ Results for {config['name']} saved to {incremental_save_file}")
            
        except Exception as e:
            print(f"âš ï¸  Error saving results: {e}")
        
        print(f"âœ… Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± {config['name']}:")
        # Safe formatting Î³Î¹Î± micro_f1 Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ None Î® Î½Î± Î¼Î·Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹
        micro_f1_value = results[config['name']]['metrics'].get('micro_f1', None)
        if micro_f1_value is not None and isinstance(micro_f1_value, (int, float)):
            micro_f1_str = f"{micro_f1_value:.4f}"
        else:
            micro_f1_str = "N/A"
        
        print(f"   ğŸ“Š TAG-LEVEL: Micro-F1={micro_f1_str}")
        print(f"   ğŸ¯ ENTITY-LEVEL: F1={results[config['name']]['metrics']['f1']:.4f}, Precision={results[config['name']]['metrics']['precision']:.4f}, Recall={results[config['name']]['metrics']['recall']:.4f}")
        print(f"   ï¿½ Entity-level metrics Î¼ÎµÏ„ÏÎ¬Î½Îµ complete entities (exact match)")
        print(f"   ğŸ’¡ Tag-level metrics Î¼ÎµÏ„ÏÎ¬Î½Îµ individual tag accuracy")
    
    # ï¿½ SUMMARY OF ALL CONFIGURATIONS
    print(f"\nï¿½ EXPERIMENT SUMMARY:")
    print("="*50)
    print(f"âœ… Completed {len(results)} configurations")
    print(f"ğŸ’¾ Progressive results saved to: progressive_results_with_crf.json")
    print(f"ğŸ“‹ Metrics explanation:")
    print(f"   â€¢ Micro-F1: Tag-level weighted average (training-relevant)")
    print(f"   â€¢ Entity-F1: Complete entity matching (user-relevant)")
    print("="*50)
    
    return results

def export_rare_class_errors(tokenized_test_ds, predictions, label_list, rare_classes, output_file="rare_class_errors.txt"):
    """
    Î•Î¾Î¬Î³ÎµÎ¹ Ï„Î± Î»Î¬Î¸Î· Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏƒÏ€Î¬Î½Î¹Î± Î¿Î½Ï„ÏŒÏ„Î·Ï„Î± (rare class) ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ·.
    ENHANCED: Î‘Î½Î±Î»ÏÎµÎ¹ Ï„ÏŒÏƒÎ¿ tag-level ÏŒÏƒÎ¿ ÎºÎ±Î¹ entity-level errors.
    """
    print(f"\nğŸ” RARE CLASS ERROR ANALYSIS:")
    print("="*50)
    
    errors = []
    entity_errors = []  # Î“Î¹Î± entity-level errors
    predictions = np.argmax(predictions, axis=2)
    
    # Tag-level error analysis
    for idx, (preds) in enumerate(predictions):
        if idx >= len(tokenized_test_ds):
            break
        example = tokenized_test_ds[idx]
        true_labels = [label_list[l] for l in example['labels'] if l != -100]
        pred_labels = [label_list[p] for p, l in zip(preds, example['labels']) if l != -100]
        
        for i, (true_label, pred_label) in enumerate(zip(true_labels, pred_labels)):
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï„Î¿ true_label Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ ÎºÎ¬Ï€Î¿Î¹Î¿ Î±Ï€ÏŒ Ï„Î± rare_classes
            is_rare = False
            for rare_class in rare_classes:
                if true_label.endswith(rare_class):  # Ï€.Ï‡. "B-LOCATION-NAT" ends with "LOCATION-NAT"
                    is_rare = True
                    break
            
            if is_rare and true_label != pred_label:
                errors.append({
                    "example_idx": idx,
                    "token_pos": i,
                    "true": true_label,
                    "pred": pred_label,
                    "error_type": "tag_level"
                })
    
    # Entity-level error analysis Î³Î¹Î± rare classes
    print(f"Tag-level errors found: {len(errors)}")
    
    # Extract entities from sequences ÎºÎ±Î¹ find entity-level errors
    for idx in range(min(100, len(tokenized_test_ds), len(predictions))):  # Sample Î³Î¹Î± analysis
        example = tokenized_test_ds[idx]
        preds = predictions[idx]
        true_labels = [label_list[l] for l in example['labels'] if l != -100]
        pred_labels = [label_list[p] for p, l in zip(preds, example['labels']) if l != -100]
        
        # Extract entities
        def extract_entities(sequence):
            entities = []
            current_entity = None
            
            for pos, tag in enumerate(sequence):
                if tag.startswith('B-'):
                    if current_entity:
                        entities.append(current_entity)
                    current_entity = {'type': tag[2:], 'start': pos, 'end': pos, 'tags': [tag]}
                elif tag.startswith('I-') and current_entity and tag[2:] == current_entity['type']:
                    current_entity['end'] = pos
                    current_entity['tags'].append(tag)
                else:
                    if current_entity:
                        entities.append(current_entity)
                    current_entity = None
            
            if current_entity:
                entities.append(current_entity)
            return entities
        
        true_entities = extract_entities(true_labels)
        pred_entities = extract_entities(pred_labels)
        
        # Find missing rare entities
        for true_entity in true_entities:
            if any(true_entity['type'] == rare_class for rare_class in rare_classes):
                # Check if this entity was correctly predicted
                found_match = False
                for pred_entity in pred_entities:
                    if (pred_entity['type'] == true_entity['type'] and 
                        pred_entity['start'] == true_entity['start'] and 
                        pred_entity['end'] == true_entity['end']):
                        found_match = True
                        break
                
                if not found_match:
                    entity_errors.append({
                        "example_idx": idx,
                        "entity_type": true_entity['type'],
                        "entity_span": (true_entity['start'], true_entity['end']),
                        "entity_tags": true_entity['tags'],
                        "error_type": "entity_level_missing"
                    })
    
    print(f"Entity-level errors found (sample): {len(entity_errors)}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"RARE CLASS ERROR ANALYSIS\n")
        f.write(f"========================\n\n")
        f.write(f"Analyzed rare classes: {rare_classes}\n")
        f.write(f"Tag-level errors: {len(errors)}\n")
        f.write(f"Entity-level errors (sample): {len(entity_errors)}\n\n")
        
        f.write(f"IMPORTANT: F1 scores are calculated at ENTITY level, not tag level!\n")
        f.write(f"- Each entity (B- + I-*) counts as 1\n")
        f.write(f"- Partial matches don't count\n")
        f.write(f"- Entity boundaries must be exact\n\n")
        
        # Group tag-level errors by true label
        error_by_class = {}
        for err in errors:
            true_class = err['true']
            if true_class not in error_by_class:
                error_by_class[true_class] = []
            error_by_class[true_class].append(err)
        
        for true_class, class_errors in error_by_class.items():
            f.write(f"\n=== Errors for class: {true_class} ({len(class_errors)} errors) ===\n")
            pred_counts = {}
            for err in class_errors:
                pred = err['pred']
                pred_counts[pred] = pred_counts.get(pred, 0) + 1
            
            f.write("Confusion with other classes:\n")
            for pred_class, count in sorted(pred_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(class_errors)) * 100
                f.write(f"  {pred_class}: {count} times ({percentage:.1f}%)\n")
            
            f.write(f"\nFirst 10 examples:\n")
            for i, err in enumerate(class_errors[:10]):
                f.write(f"  {i+1}. Example {err['example_idx']}, Token {err['token_pos']}: {err['true']} -> {err['pred']}\n")
        
        # Entity-level errors
        if entity_errors:
            f.write(f"\n=== Entity-level errors (sample) ===\n")
            entity_error_by_type = {}
            for err in entity_errors:
                entity_type = err['entity_type']
                entity_error_by_type[entity_type] = entity_error_by_type.get(entity_type, 0) + 1
            
            f.write(f"Missing entities by type:\n")
            for entity_type, count in sorted(entity_error_by_type.items()):
                f.write(f"  {entity_type}: {count} missing entities\n")
    
    print(f"Exported {len(errors)} tag-level errors and {len(entity_errors)} entity-level errors to {output_file}")
    
    # Print summary - ENHANCED
    print(f"\nğŸ“Š COMPREHENSIVE RARE CLASS ERROR SUMMARY:")
    print(f"ğŸ·ï¸  TAG-LEVEL ERRORS:")
    error_by_class = {}
    for err in errors:
        true_class = err['true']
        error_by_class[true_class] = error_by_class.get(true_class, 0) + 1
    
    for rare_class in rare_classes:
        # Count both B- and I- tag errors Î³Î¹Î± ÎºÎ¬Î¸Îµ rare class
        b_errors = error_by_class.get(f'B-{rare_class}', 0)
        i_errors = error_by_class.get(f'I-{rare_class}', 0)
        total_errors = b_errors + i_errors
        print(f"  {rare_class}: {total_errors} total errors (B-: {b_errors}, I-: {i_errors})")
    
    if entity_errors:
        print(f"\nğŸ¯ ENTITY-LEVEL ERRORS (impacts F1 directly):")
        entity_error_by_type = {}
        for err in entity_errors:
            entity_type = err['entity_type']
            entity_error_by_type[entity_type] = entity_error_by_type.get(entity_type, 0) + 1
        
        for entity_type, count in sorted(entity_error_by_type.items()):
            print(f"  {entity_type}: {count} missing entities")
        
        print(f"\nğŸ’¡ Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·: Î‘Ï…Ï„Î¬ Ï„Î± entity-level errors ÎµÏ€Î·ÏÎµÎ¬Î¶Î¿Ï…Î½ Î¬Î¼ÎµÏƒÎ± Ï„Î¿ F1 score!")
    
    print("="*50)
    
    return errors

# --- 9. MAIN EXECUTION LOGIC ---
# Note: run_single_experiment removed - using test_all_loss_configurations instead


# --- 10. MAIN EXECUTION ---
if __name__ == "__main__":
    # ğŸ”§ Î•Î“ÎšÎ‘Î¤Î‘Î£Î¤Î‘Î£Î— DEPENDENCIES VALIDATION
    print("ğŸ” ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚ dependencies...")
    
    try:
        from torchcrf import CRF
        print("âœ… pytorch-crf library ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î·Î¼Î­Î½Î¿ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚")
    except ImportError:
        print("âŒ Î£Î¦Î‘Î›ÎœÎ‘: pytorch-crf library Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î·Î¼Î­Î½Î¿!")
        print("ğŸ”§ Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· pytorch-crf...")
        import subprocess
        import sys
        
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "pytorch-crf"])
            print("âœ… pytorch-crf ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î¬Î¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
            from torchcrf import CRF  # Re-import after installation
        except Exception as e:
            print(f"âŒ Î‘Ï€Î¿Ï„Ï…Ï‡Î¯Î± ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚ pytorch-crf: {str(e)}")
            print("ğŸ“– Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ manually: pip install pytorch-crf")
            print("ğŸ“– Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚: https://pytorch-crf.readthedocs.io/")
            exit(1)
    
    # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· comprehensive comparison ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ loss configurations
    print("ğŸ”„ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î´Î¿ÎºÎ¹Î¼Î®Ï‚ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ loss configurations (ROBERTa + ROBERTa+CRF)...")
    test_results = test_all_loss_configurations()
    
    # Save comparison results
    comparison_file = "loss_comparison_results_with_crf.json"
    with open(comparison_file, 'w', encoding='utf-8') as f:
        clean_test_results = clean_for_json_serialization(test_results)
        json.dump(clean_test_results, f, indent=4, sort_keys=True)
    print(f"\nÎ¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿ '{comparison_file}'")

# Î‘Î½ Ï„ÏÎ­Ï‡ÎµÏ„Î±Î¹ Ï‰Ï‚ main script
if __name__ == "__main__":
    import sys
    
    print("ğŸš€ STARTING GREEK LEGAL NER EXPERIMENTS")
    print("="*80)
    
    # Run experiments
    main_results = test_all_loss_configurations()
    
    print(f"\nğŸ‰ EXPERIMENTS COMPLETED!")
    print(f"ğŸ“Š Results saved to ner_results/ directory")