# ğŸ”— CRF Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ & Î¥Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÏƒÏ„Î¿ Greek Legal NER

## ğŸ“‹ Î¤Î¹ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ CRF (Conditional Random Field)

Î¤Î¿ **CRF** ÎµÎ¯Î½Î±Î¹ Î­Î½Î± **probabilistic model** Ï€Î¿Ï… ÎµÎ¾Î±ÏƒÏ†Î±Î»Î¯Î¶ÎµÎ¹ **valid sequences** ÏƒÏ„Î¿ Named Entity Recognition. Î•Î½Ï Ï„Î¿ ROBERTa Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÎºÎ¬Î¸Îµ token Î±Î½ÎµÎ¾Î¬ÏÏ„Î·Ï„Î±, Ï„Î¿ CRF Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï…Ï€ÏŒÏˆÎ· **ÏŒÎ»Î· Ï„Î·Î½ Î±Î»Î»Î·Î»Î¿Ï…Ï‡Î¯Î±** Î³Î¹Î± Î½Î± Î´ÏÏƒÎµÎ¹ structurally consistent predictions.

## âš™ï¸ ÎŸÎ¹ Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Ï€Î¿Ï… ÎˆÏ‡ÎµÏ„Îµ Î’Î¬Î»ÎµÎ¹

### ğŸ¯ **ÎšÏÏÎ¹ÎµÏ‚ CRF Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚**

```python
# Î£Ï„Î¿ crf_model.py:
self.crf = CRF(
    num_labels,           # 17 labels (8 entities Ã— 2 + O)
    batch_first=True      # ğŸ¯ Tensor format: [batch_size, seq_len]
)

# Î£Ï„Î¿ config.py:
crf_reduction: str = "mean"    # ğŸ¯ Loss aggregation method
use_crf: bool = True           # ğŸ¯ Enable CRF layer
```

### ğŸ”§ **CRF Loss Computation**

```python
# Î£Ï„Î¿ crf_model.py Î³ÏÎ±Î¼Î¼Î® ~237:
loss = -self.crf(
    logits,                    # ROBERTa predictions [batch, seq_len, num_labels]
    crf_labels,               # True labels [batch, seq_len]  
    mask=crf_mask,            # Mask Î³Î¹Î± padding tokens
    reduction='mean'          # ğŸ¯ Average loss across batch
)
```

### ğŸ­ **Mask Handling**

```python
# Proper padding handling:
crf_labels = labels.clone()
crf_labels[crf_labels == -100] = 0  # Map padding to O tag (index 0)
crf_mask = attention_mask.bool() & (labels != -100)  # Ignore padding
```

---

## ğŸ—ï¸ Î ÏÏ‚ Î”Î¿Ï…Î»ÎµÏÎµÎ¹ Ï„Î¿ CRF (Î’Î®Î¼Î±-Î²Î®Î¼Î±)

### Î’Î®Î¼Î± 1: ROBERTa Emissions
```python
# ROBERTa Î´Î¯Î½ÎµÎ¹ "emission scores" Î³Î¹Î± ÎºÎ¬Î¸Îµ token:
text = ["ÎŸ", "Î“Î¹Î¬Î½Î½Î·Ï‚", "ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹", "ÏƒÏ„Î·", "Microsoft"]

emission_scores = [
    [0.9, 0.05, 0.05, ...],    # "ÎŸ": Ï…ÏˆÎ·Î»ÏŒ score Î³Î¹Î± "O"
    [0.1, 0.8, 0.05, 0.05],    # "Î“Î¹Î¬Î½Î½Î·Ï‚": Ï…ÏˆÎ·Î»ÏŒ Î³Î¹Î± "B-PERSON"  
    [0.85, 0.1, 0.05, ...],    # "ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹": Ï…ÏˆÎ·Î»ÏŒ Î³Î¹Î± "O"
    [0.9, 0.05, 0.05, ...],    # "ÏƒÏ„Î·": Ï…ÏˆÎ·Î»ÏŒ Î³Î¹Î± "O"
    [0.1, 0.05, 0.05, 0.8]     # "Microsoft": Ï…ÏˆÎ·Î»ÏŒ Î³Î¹Î± "B-ORG"
]
```

### Î’Î®Î¼Î± 2: CRF Transition Matrix
```python
# Î¤Î¿ CRF Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ Ï€Î¯Î½Î±ÎºÎ± Î¼ÎµÏ„Î±Î²Î¬ÏƒÎµÏ‰Î½ [17Ã—17]:
transition_scores = {
    # Valid transitions (Ï…ÏˆÎ·Î»Î¬ scores):
    "O â†’ B-PERSON": 2.3,         # âœ… ÎœÏ€Î¿ÏÎµÎ¯ Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹ entity
    "B-PERSON â†’ I-PERSON": 3.1,  # âœ… Î£Ï…Î½Î­Ï‡ÎµÎ¹Î± entity
    "B-PERSON â†’ O": 1.8,         # âœ… Î¤Î­Î»Î¿Ï‚ entity
    
    # Invalid transitions (Ï‡Î±Î¼Î·Î»Î¬ scores):
    "O â†’ I-PERSON": -5.2,        # âŒ I- Ï‡Ï‰ÏÎ¯Ï‚ B-
    "B-PERSON â†’ I-ORG": -4.8,    # âŒ Î‘Î»Î»Î±Î³Î® entity type
    "I-PERSON â†’ B-PERSON": -2.1, # âŒ ÎÎ­Î¿ entity Ï‡Ï‰ÏÎ¯Ï‚ O
}
```

### Î’Î®Î¼Î± 3: Sequence Scoring
```python
# Î“Î¹Î± ÎºÎ¬Î¸Îµ Ï€Î¹Î¸Î±Î½Î® sequence, Ï„Î¿ CRF Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹:
# Score(sequence) = Î£(emission_scores) + Î£(transition_scores)

# Candidate 1: ["O", "B-PERSON", "O", "O", "B-ORG"]
score_1 = emission["O"][0] + transition["STARTâ†’O"] +
          emission["B-PERSON"][1] + transition["Oâ†’B-PERSON"] +
          emission["O"][2] + transition["B-PERSONâ†’O"] +
          emission["O"][3] + transition["Oâ†’O"] +
          emission["B-ORG"][4] + transition["Oâ†’B-ORG"] +
          transition["B-ORGâ†’END"]

# Candidate 2: ["O", "B-PERSON", "I-PERSON", "O", "B-ORG"] âŒ (wrong boundaries)
score_2 = emission["O"][0] + transition["STARTâ†’O"] +
          emission["B-PERSON"][1] + transition["Oâ†’B-PERSON"] +
          emission["I-PERSON"][2] + transition["B-PERSONâ†’I-PERSON"] +  # Lower emission!
          emission["O"][3] + transition["I-PERSONâ†’O"] +
          emission["B-ORG"][4] + transition["Oâ†’B-ORG"]

# Î¤Î¿ CRF ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Ï„Î·Î½ sequence Î¼Îµ Ï„Î¿ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ ÏƒÏ…Î½Î¿Î»Î¹ÎºÏŒ score
```

### Î’Î®Î¼Î± 4: Viterbi Decoding
```python
# Viterbi algorithm Î²ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· sequence efficiently:
best_sequence = crf.decode(logits, mask=crf_mask)
# Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±: ["O", "B-PERSON", "O", "O", "B-ORG"]
```

---

## ğŸ¯ Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÎ­Ï‚ Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ ÏƒÏ„Î¿Î½ ÎšÏÎ´Î¹ÎºÎ¬ ÏƒÎ±Ï‚

### 1. **`num_labels=17`**
```python
# Î¤Î± 17 labels ÏƒÎ±Ï‚:
labels = [
    "O",                      # 0: Outside
    "B-FACILITY",            # 1: Begin Facility
    "I-FACILITY",            # 2: Inside Facility  
    "B-GPE",                 # 3: Begin Geo-Political Entity
    "I-GPE",                 # 4: Inside GPE
    "B-LEGISLATION_REFERENCE", # 5: Begin Legislation Reference
    "I-LEGISLATION_REFERENCE", # 6: Inside Legislation Reference
    "B-NATIONAL_LOCATION",     # 7: Begin National Location
    "I-NATIONAL_LOCATION",     # 8: Inside National Location
    "B-UNKNOWN_LOCATION",      # 9: Begin Unknown Location
    "I-UNKNOWN_LOCATION",      # 10: Inside Unknown Location
    "B-ORGANIZATION",          # 11: Begin Organization
    "I-ORGANIZATION",          # 12: Inside Organization
    "B-PERSON",               # 13: Begin Person
    "I-PERSON",               # 14: Inside Person
    "B-PUBLIC_DOCUMENT",       # 15: Begin Public Document
    "I-PUBLIC_DOCUMENT"        # 16: Inside Public Document
]

# CRF transition matrix: 17Ã—17 = 289 Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Î½Î± Î¼Î¬Î¸ÎµÎ¹
```

### 2. **`batch_first=True`**
```python
# Tensor dimensions:
logits.shape = [batch_size, sequence_length, num_labels]
labels.shape = [batch_size, sequence_length]

# Î‘Î½Ï„Î¯ Î³Î¹Î± [sequence_length, batch_size, num_labels] (default)
# Î£Ï…Î¼Î²Î±Ï„ÏŒ Î¼Îµ Ï„Î± PyTorch/HuggingFace conventions
```

### 3. **`reduction='mean'`**
```python
# Loss aggregation options:

# 'mean' (Current choice):
final_loss = sum(sequence_losses) / num_valid_sequences
# ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ - ÎºÎ±Î»ÏŒ Î³Î¹Î± comparison across batches

# 'sum' (Alternative):  
final_loss = sum(sequence_losses)
# ÎœÎ·-ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ - Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚

# 'none' (Alternative):
final_loss = [loss_seq1, loss_seq2, ...]  # Tensor Î¼Îµ individual losses
# Î“Î¹Î± custom weighting
```

### 4. **Mask Handling**
```python
# Î— ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® ÏƒÎ±Ï‚ Î³Î¹Î± padding:

# Step 1: Map -100 to O tag (index 0)
crf_labels[crf_labels == -100] = 0

# Step 2: Create proper mask  
crf_mask = attention_mask.bool() & (labels != -100)

# Step 3: Apply mask in CRF
loss = -self.crf(logits, crf_labels, mask=crf_mask, reduction='mean')

# âœ… Î•Î¾Î±Î¹ÏÎµÎ¯ padding tokens Î±Ï€ÏŒ loss computation
# âœ… Î”ÎµÎ½ ÎµÏ€Î·ÏÎµÎ¬Î¶ÎµÎ¹ transition learning Î±Ï€ÏŒ invalid positions
```

---

## ğŸ”„ Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ & Options

### 1. **Reduction Strategies**

#### Option A: `reduction='sum'` (Cumulative Loss)
```python
# Î£Ï„Î¿ config.py:
crf_reduction: str = "sum"

# Î£Ï„Î¿ crf_model.py: 
loss = -self.crf(logits, crf_labels, mask=crf_mask, reduction='sum')

# âœ… ÎšÎ±Î»ÏŒ Î³Î¹Î±: Variable batch sizes, gradient accumulation
# âŒ Î ÏÏŒÎ²Î»Î·Î¼Î±: Non-normalized loss values
```

#### Option B: `reduction='none'` (Per-Sequence Loss)
```python
# Î£Ï„Î¿ config.py:
crf_reduction: str = "none"

# Î£Ï„Î¿ crf_model.py:
sequence_losses = -self.crf(logits, crf_labels, mask=crf_mask, reduction='none')
# sequence_losses.shape = [batch_size]

# Custom weighting:
weights = torch.tensor([1.0, 2.0, 1.5, ...])  # Per-sequence weights
weighted_loss = (sequence_losses * weights).mean()

# âœ… ÎšÎ±Î»ÏŒ Î³Î¹Î±: Custom weighting, sample importance
# âŒ Î ÏÏŒÎ²Î»Î·Î¼Î±: Î Î¹Î¿ Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ· implementation
```

### 2. **CRF Constraint Variants**

#### Option A: **Linear-Chain CRF** (Current)
```python
# Î¤Î¹ Î­Ï‡ÎµÏ„Îµ Ï„ÏÏÎ±:
self.crf = CRF(num_labels, batch_first=True)

# ÎœÎ¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¹ÎµÎ¯: P(y_i | y_{i-1}, x)
# ÎœÏŒÎ½Î¿ bigram transitions: label[i-1] â†’ label[i]
```

#### Option B: **Semi-CRF** (Advanced)
```python
# Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÏŒ (Î´ÎµÎ½ Ï„Î¿ Î­Ï‡ÎµÏ„Îµ):
from torchcrf import SemiCRF
self.crf = SemiCRF(num_labels, max_seg_len=10)

# ÎœÎ¿Î½Ï„ÎµÎ»Î¿Ï€Î¿Î¹ÎµÎ¯: P(segment | x)  
# ÎœÎ±Î¸Î±Î¯Î½ÎµÎ¹ entity lengths, ÏŒÏ‡Î¹ Î¼ÏŒÎ½Î¿ transitions
# âœ… ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î³Î¹Î± entities Î¼Îµ specific length patterns
# âŒ Î Î¹Î¿ Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ¿, Î±ÏÎ³ÏŒÏ„ÎµÏÎ¿ training
```

#### Option C: **Constrained CRF** (Custom)
```python
# Custom constraints (Î´ÎµÎ½ Ï„Î¿ Î­Ï‡ÎµÏ„Îµ):
def create_constraint_matrix(num_labels):
    """Manually define valid transitions"""
    transitions = torch.full((num_labels, num_labels), -1000.0)  # Very low score
    
    # Allow only valid BIO transitions:
    transitions[0, 0] = 0.0    # O â†’ O
    transitions[0, 1::2] = 0.0  # O â†’ B-* (all B- tags)
    
    for i in range(1, num_labels, 2):  # B- tags  
        transitions[i, 0] = 0.0      # B-* â†’ O
        transitions[i, i+1] = 0.0    # B-* â†’ I-* (same entity)
        transitions[i, 1::2] = 0.0   # B-* â†’ B-* (new entity)
    
    for i in range(2, num_labels, 2):  # I- tags
        transitions[i, 0] = 0.0      # I-* â†’ O  
        transitions[i, i] = 0.0      # I-* â†’ I-* (continue)
        transitions[i, 1::2] = 0.0   # I-* â†’ B-* (new entity)
    
    return transitions

# âœ… Î•Î¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Î±Ï…ÏƒÏ„Î·ÏÏŒ BIO enforcement
# âŒ Î›Î¹Î³ÏŒÏ„ÎµÏÎ· flexibility Î³Î¹Î± Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î½Î± Î¼Î¬Î¸ÎµÎ¹
```

### 3. **Alternative Sequence Models**

#### Option A: **BiLSTM-CRF** (Classic)
```python
# Î‘Î½Ï„Î¯ Î³Î¹Î± ROBERTa + CRF:
class BiLSTMCRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 
                           bidirectional=True, batch_first=True)
        self.hidden2tag = nn.Linear(hidden_dim * 2, num_labels)
        self.crf = CRF(num_labels, batch_first=True)

# âœ… Î¤Î±Ï‡ÏÏ„ÎµÏÎ¿ training, Î»Î¹Î³ÏŒÏ„ÎµÏÎ± parameters
# âŒ Î§ÎµÎ¹ÏÏŒÏ„ÎµÏÎµÏ‚ representations Î±Ï€ÏŒ ROBERTa
```

#### Option B: **Transformer without CRF** (Simple)
```python
# ÎœÏŒÎ½Î¿ ROBERTa Ï‡Ï‰ÏÎ¯Ï‚ CRF:
class SimpleTransformer(nn.Module):
    def __init__(self, model_name, num_labels):
        self.bert = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Linear(config.hidden_size, num_labels)
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)

# âœ… Î‘Ï€Î»Î¿ÏÏƒÏ„ÎµÏÎ¿, Ï„Î±Ï‡ÏÏ„ÎµÏÎ¿ inference  
# âŒ Î”ÎµÎ½ ÎµÎ¾Î±ÏƒÏ†Î±Î»Î¯Î¶ÎµÎ¹ valid BIO sequences
```

#### Option C: **Pointer Networks** (Advanced)
```python
# Sequence-to-sequence approach:
# Î‘Î½Ï„Î¯ Î³Î¹Î± token classification â†’ span detection
# ÎœÎ±Î¸Î±Î¯Î½ÎµÎ¹ start/end positions Î³Î¹Î± ÎºÎ¬Î¸Îµ entity

# âœ… ÎšÎ±Î»ÏÏ„ÎµÏÎ¿ Î³Î¹Î± overlapping entities
# âŒ Î Î¹Î¿ Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ¿, Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ data format
```

### 4. **Loss Function Variants**

#### Option A: **Focal CRF Loss** (Custom)
```python
# Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼ÏŒÏ‚ CRF + Focal Loss:
class FocalCRFLoss(nn.Module):
    def __init__(self, crf, gamma=2.0, alpha=None):
        self.crf = crf
        self.gamma = gamma
        self.alpha = alpha
    
    def forward(self, logits, labels, mask):
        # Standard CRF loss
        crf_loss = -self.crf(logits, labels, mask=mask, reduction='none')
        
        # Apply focal weighting
        pt = torch.exp(-crf_loss)  # "confidence"
        focal_weight = (1 - pt) ** self.gamma
        
        if self.alpha is not None:
            alpha_weight = self.alpha[labels]  # Per-class weighting
            focal_weight = alpha_weight * focal_weight
        
        return (focal_weight * crf_loss).mean()

# âœ… Combines structure (CRF) + class balance (Focal)
# âŒ Î Î¹Î¿ Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ¿, Ï€ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÎºÏŒ
```

#### Option B: **Weighted CRF Loss** (Custom)
```python
# Per-sequence weighting:
def weighted_crf_loss(crf, logits, labels, mask, sequence_weights):
    sequence_losses = -crf(logits, labels, mask=mask, reduction='none')
    weighted_losses = sequence_losses * sequence_weights
    return weighted_losses.mean()

# âœ… ÎšÎ±Î»ÏŒ Î³Î¹Î± domain adaptation, temporal weighting
# âŒ Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ manual weight definition
```

---

## ğŸ¯ Î£Ï…Î½Î¹ÏƒÏ„ÏÎ¼ÎµÎ½ÎµÏ‚ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î³Î¹Î± ÏƒÎ±Ï‚

### âœ… **ÎšÏÎ±Ï„Î®ÏƒÏ„Îµ Ï„Î¹Ï‚ Î¥Ï€Î¬ÏÏ‡Î¿Ï…ÏƒÎµÏ‚** (Î•Î¯Î½Î±Î¹ Î¬ÏÎ¹ÏƒÏ„ÎµÏ‚!)

```python
# Î¤ÏÎ­Ï‡Î¿Ï…ÏƒÎµÏ‚ ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ (PERFECT Î³Î¹Î± research):
self.crf = CRF(num_labels=17, batch_first=True)
loss = -self.crf(logits, crf_labels, mask=crf_mask, reduction='mean')
crf_reduction: str = "mean"
```

**Î“Î¹Î±Ï„Î¯**:
- **Standard approach**: Î‘Ï…Ï„ÏŒ Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½ Ï„Î± top NER papers
- **Proper masking**: Î£Ï‰ÏƒÏ„ÏŒ handling Ï„Ï‰Î½ padding tokens
- **Efficient**: Optimal balance performance vs complexity

### ğŸ”„ **Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ­Ï‚ Î³Î¹Î± Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ**

#### Experiment 1: Different Reduction
```python
# Î£Ï„Î¿ config.py, Ï€ÏÎ¿ÏƒÎ¸Î­ÏƒÏ„Îµ:
crf_reduction: str = "sum"  # Î‘Î½Ï„Î¯ Î³Î¹Î± "mean"

# Test impact on:
# - Training stability
# - Convergence speed  
# - Final performance
```

#### Experiment 2: Constrained Transitions
```python
# Î ÏÎ¿ÏƒÎ¸Î­ÏƒÏ„Îµ ÏƒÏ„Î¿ crf_model.py:
def initialize_transitions(self):
    """Initialize CRF transitions with BIO constraints"""
    with torch.no_grad():
        # Set invalid transitions to very low values
        self.crf.transitions.fill_(-1000.0)
        
        # Allow valid transitions
        for i in range(self.num_labels):
            for j in range(self.num_labels):
                if self.is_valid_transition(i, j):
                    self.crf.transitions[i, j] = 0.0

# âœ… Enforces strict BIO compliance Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏÏ‡Î®
# Test: Does it help with rare entities?
```

#### Experiment 3: Per-Entity CRF Weighting
```python
# Custom loss Î³Î¹Î± rare entities:
def entity_weighted_crf_loss(crf, logits, labels, mask, entity_weights):
    # Weight sequences based on rare entities they contain
    sequence_weights = compute_sequence_weights(labels, entity_weights)
    sequence_losses = -crf(logits, labels, mask=mask, reduction='none')
    return (sequence_losses * sequence_weights).mean()

# âœ… Focus Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ ÏƒÏ„Î± rare entity sequences
```

---

## ğŸ” Monitoring & Debugging CRF

### ğŸ“Š **CRF-Specific Metrics Î½Î± Î Î±ÏÎ±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯Ï„Îµ**

```python
# Î ÏÎ¿ÏƒÎ¸Î­ÏƒÏ„Îµ ÏƒÏ„Î¿ evaluation:

def analyze_crf_performance(self, model, dataset):
    """Analyze CRF-specific performance"""
    
    # 1. BIO Constraint Violations
    violations = count_bio_violations(predictions)
    print(f"BIO violations: {violations} / {total_predictions}")
    
    # 2. Transition Matrix Analysis  
    transitions = model.crf.transitions.detach()
    valid_transitions = extract_high_scoring_transitions(transitions)
    
    # 3. Entity Boundary Accuracy
    boundary_accuracy = compute_boundary_accuracy(true_entities, pred_entities)
    
    # 4. Sequence-Level Accuracy (complete match)
    sequence_accuracy = compute_sequence_accuracy(true_sequences, pred_sequences)
    
    return {
        'bio_violation_rate': violations / total_predictions,
        'boundary_accuracy': boundary_accuracy,
        'sequence_accuracy': sequence_accuracy,
        'learned_transitions': valid_transitions
    }
```

### ğŸ¯ **Success Indicators**

```python
# ÎšÎ±Î»Î¬ signs Î³Î¹Î± CRF:
âœ… BIO violation rate < 1%
âœ… Boundary accuracy > 85%  
âœ… Sequence accuracy improvements vs non-CRF
âœ… Learned transitions make linguistic sense

# Red flags:
âŒ High BIO violation rate (CRF not learning constraints)
âŒ No improvement over standard CrossEntropy
âŒ Erratic transition matrix (numerical instability)
```

**Bottom line**: ÎŸÎ¹ CRF ÏÏ…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚ ÎµÎ¯Î½Î±Î¹ **research-grade standard**. Î¤Î¿ Î¼ÏŒÎ½Î¿ Ï€Î¿Ï… Î¸Î± Î´Î¿ÎºÎ¹Î¼Î¬Î¶Î±Ï„Îµ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ `reduction` method, Î±Î»Î»Î¬ Ï„Î¿ `"mean"` ÎµÎ¯Î½Î±Î¹ Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î® Î³Î¹Î± ÏƒÏ„Î±Î¸ÎµÏÏŒ training! ğŸ¯
