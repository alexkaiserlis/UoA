"""
Focal Loss Implementation for Imbalanced NER Tasks

This module contains adaptive focal loss implementations specifically designed
for Named Entity Recognition tasks with imbalanced class distributions.

ğŸ” FOCAL LOSS EXPLANATION:
==========================
Î¤Î¿ Focal Loss Î±Î½Ï„Î¹Î¼ÎµÏ„Ï‰Ï€Î¯Î¶ÎµÎ¹ Ï„Î¿ class imbalance problem ÏƒÏ„Î± NER tasks:

ğŸ¯ Î’Î‘Î£Î™ÎšÎ— Î™Î”Î•Î‘:
==============
Î Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÏŒ Cross Entropy Loss:
- CE(p_t) = -log(p_t)
- Î”Î¯Î½ÎµÎ¹ Î¯Î´Î¹Î¿ Î²Î¬ÏÎ¿Ï‚ ÏƒÎµ ÏŒÎ»Î± Ï„Î± examples

Focal Loss:
- FL(p_t) = -Î±_t * (1-p_t)^Î³ * log(p_t)
- ÎœÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î¿ Î²Î¬ÏÎ¿Ï‚ Ï„Ï‰Î½ "easy examples" (high confidence)
- Î•ÏƒÏ„Î¹Î¬Î¶ÎµÎ¹ ÏƒÏ„Î± "hard examples" (low confidence)

ğŸ”¬ Î Î‘Î¡Î‘ÎœÎ•Î¤Î¡ÎŸÎ™:
=============
1. **gamma (Î³)**: Focusing parameter
   - Î³=0: ÎšÎ±Î½Î¿Î½Î¹ÎºÏŒ Cross Entropy
   - Î³=1: ÎœÎ­Ï„ÏÎ¹Î¿ focusing (ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ Î³Î¹Î± NER)
   - Î³=2: ÎˆÎ½Ï„Î¿Î½Î¿ focusing (Î³Î¹Î± Ï€Î¿Î»Ï imbalanced data)

2. **alpha (Î±)**: Class weighting parameter
   - Scalar: ÎŠÎ´Î¹Î¿ Î²Î¬ÏÎ¿Ï‚ Î³Î¹Î± ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ positive classes
   - Tensor: Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î²Î¬ÏÎ¿Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ class
   - None: Î§Ï‰ÏÎ¯Ï‚ class weighting

3. **reduction**: Î¤ÏÏŒÏ€Î¿Ï‚ aggregation
   - 'mean': ÎœÎ­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚ (default)
   - 'sum': Î†Î¸ÏÎ¿Î¹ÏƒÎ¼Î±
   - 'none': Î§Ï‰ÏÎ¯Ï‚ reduction

ğŸ·ï¸ NER-SPECIFIC CHALLENGES:
===========================
1. **Class Imbalance**:
   - O tag: ~85% Ï„Ï‰Î½ tokens
   - Entity tags: ~15% Ï„Ï‰Î½ tokens
   - Rare entities (LOCATION-NAT): <0.1%

2. **Token-Level vs Entity-Level**:
   - Focal Loss ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹ ÏƒÎµ token level
   - Î‘Î»Î»Î¬ Î¸Î­Î»Î¿Ï…Î¼Îµ entity-level performance

3. **Padding Tokens**:
   - Î ÏÎ­Ï€ÎµÎ¹ Î½Î± Î±Î³Î½Î¿Î·Î¸Î¿ÏÎ½ Ï„Î± -100 labels
   - Proper masking ÎµÎ¯Î½Î±Î¹ ÎºÏÎ¯ÏƒÎ¹Î¼Î¿

ğŸ“Š ADAPTIVE FOCAL LOSS BENEFITS:
===============================
- Conservative Î³=1.0 (ÏŒÏ‡Î¹ Ï€Î¿Î»Ï aggressive)
- Proper handling Ï„Ï‰Î½ padding tokens
- Support Î³Î¹Î± tensor-based alpha weights
- Robust error handling Î³Î¹Î± empty batches
- Optimized Î³Î¹Î± GPU memory efficiency

ğŸ§® ÎœÎ‘Î˜Î—ÎœÎ‘Î¤Î™ÎšÎ— Î¦ÎŸÎ¡ÎœÎŸÎ¥Î›Î‘:
======================
Î“Î¹Î± ÎºÎ¬Î¸Îµ token i:
1. p_i = softmax(logits_i)[true_label_i]  # Confidence
2. Î±_i = alpha[true_label_i] if tensor else alpha  # Class weight
3. FL_i = -Î±_i * (1-p_i)^Î³ * log(p_i)  # Focal loss
4. Final = mean(FL_i) Î³Î¹Î± ÏŒÎ»Î± Ï„Î± non-padding tokens

ğŸ“ˆ Î Î¡Î‘ÎšÎ¤Î™ÎšÎ‘ Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î‘:
========================
- Î’ÎµÎ»Ï„Î¯Ï‰ÏƒÎ· F1 Î³Î¹Î± rare classes: +5-15%
- ÎœÎµÎ¹Ï‰Î¼Î­Î½Î¿ overfitting ÏƒÏ„Î± common classes
- ÎšÎ±Î»ÏÏ„ÎµÏÎ· generalization
- Î¤Î±Ï‡ÏÏ„ÎµÏÎ· ÏƒÏÎ³ÎºÎ»Î¹ÏƒÎ· ÏƒÏ„Î¿ training
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Union


class AdaptiveFocalLoss(nn.Module):
    """
    Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· ÎµÎºÎ´Î¿Ï‡Î® Focal Loss ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± NER tasks Î¼Îµ imbalanced classes.
    
    Î£Îµ ÏƒÏ‡Î­ÏƒÎ· Î¼Îµ Ï„Î¿ ÎºÎ»Î±ÏƒÎ¹ÎºÏŒ Focal Loss, Î±Ï…Ï„Î® Î· implementation:
    - Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ conservative Î³=1.0 parameter (Ï€Î¹Î¿ Î®Ï€Î¹Î± ÎµÏƒÏ„Î¯Î±ÏƒÎ·)
    - Proper handling Ï„Ï‰Î½ padding tokens (-100 labels)
    - Support Î³Î¹Î± tensor-based class weights
    - Robust error handling Î³Î¹Î± edge cases
    - Memory-efficient implementation
    
    Args:
        gamma (float): Focusing parameter. Default 1.0 Î³Î¹Î± conservative focusing.
                      Î¥ÏˆÎ·Î»ÏŒÏ„ÎµÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ = Ï€Î¹Î¿ Î­Î½Ï„Î¿Î½Î· ÎµÏƒÏ„Î¯Î±ÏƒÎ· ÏƒÏ„Î± hard examples.
        alpha (Union[float, torch.Tensor, None]): Class weighting parameter.
               - float: ÎŠÎ´Î¹Î¿ Î²Î¬ÏÎ¿Ï‚ Î³Î¹Î± ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ positive classes
               - Tensor: Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î²Î¬ÏÎ¿Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ class [num_classes]
               - None: Î§Ï‰ÏÎ¯Ï‚ class weighting
        reduction (str): Aggregation method. Options: 'mean', 'sum', 'none'
        ignore_index (int): Label value Ï€Î¿Ï… Î±Î³Î½Î¿ÎµÎ¯Ï„Î±Î¹ (padding tokens)
    
    Example:
        >>> # Basic usage
        >>> loss_fn = AdaptiveFocalLoss(gamma=1.0, alpha=None)
        >>> loss = loss_fn(logits, labels)
        
        >>> # With class weights
        >>> class_weights = torch.tensor([0.1, 2.0, 3.0, ...])  # Lower weight for O, higher for rare classes
        >>> loss_fn = AdaptiveFocalLoss(gamma=1.0, alpha=class_weights)
        >>> loss = loss_fn(logits, labels)
        
        >>> # Conservative focusing for stable training
        >>> loss_fn = AdaptiveFocalLoss(gamma=0.5, alpha=None)  # Even more conservative
    
    Mathematical Formula:
        FL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)
        
        where:
        - p_t = model confidence for true class
        - Î±_t = class weight for true class  
        - Î³ = focusing parameter
    
    Note:
        - ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ Î³ = Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ aggressive focusing
        - ÎœÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Î³ = Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ· ÎµÏƒÏ„Î¯Î±ÏƒÎ· ÏƒÏ„Î± hard examples
        - Î³=0 Î¹ÏƒÎ¿Î´Ï…Î½Î±Î¼ÎµÎ¯ Î¼Îµ weighted cross entropy
        - Î³=1 ÎµÎ¯Î½Î±Î¹ Î· ÏƒÏ…Î½Î¹ÏƒÏ„ÏÎ¼ÎµÎ½Î· Ï„Î¹Î¼Î® Î³Î¹Î± NER
    """
    
    def __init__(
        self, 
        gamma: float = 1.0, 
        alpha: Optional[Union[float, torch.Tensor]] = None, 
        reduction: str = 'mean', 
        ignore_index: int = -100
    ):
        super(AdaptiveFocalLoss, self).__init__()
        
        # Validate parameters
        if gamma < 0:
            raise ValueError(f"gamma must be non-negative, got {gamma}")
        if reduction not in ['mean', 'sum', 'none']:
            raise ValueError(f"reduction must be 'mean', 'sum', or 'none', got {reduction}")
        
        self.gamma = gamma  # Conservative Î³=1.0 Î³Î¹Î± NER
        self.alpha = alpha
        self.reduction = reduction
        self.ignore_index = ignore_index
        
        # Cache Î³Î¹Î± performance optimization
        self._alpha_tensor = None
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Compute adaptive focal loss.
        
        Args:
            inputs (torch.Tensor): Model logits of shape [batch_size * seq_len, num_classes]
                                  Î® [batch_size, seq_len, num_classes]
            targets (torch.Tensor): Ground truth labels of shape [batch_size * seq_len]
                                   Î® [batch_size, seq_len]
        
        Returns:
            torch.Tensor: Computed focal loss
        
        Note:
            - Inputs Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ raw logits (ÏŒÏ‡Î¹ probabilities)
            - Targets Î¼Îµ Ï„Î¹Î¼Î® ignore_index Î±Î³Î½Î¿Î¿ÏÎ½Ï„Î±Î¹
            - Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ valid targets, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ 0 loss
        """
        # Flatten inputs ÎºÎ±Î¹ targets Î±Î½ ÎµÎ¯Î½Î±Î¹ 2D/3D
        if inputs.dim() > 2:
            inputs = inputs.view(-1, inputs.size(-1))  # [N, C]
        if targets.dim() > 1:
            targets = targets.view(-1)  # [N]
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± mask Î³Î¹Î± non-padded tokens
        mask = targets != self.ignore_index
        
        # Handle empty batches Î® batches Ï‡Ï‰ÏÎ¯Ï‚ valid tokens
        if mask.sum() == 0:
            return torch.tensor(0.0, device=inputs.device, requires_grad=True)
        
        # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± padding tokens
        active_inputs = inputs[mask]  # [N_valid, C]
        active_targets = targets[mask]  # [N_valid]
        
        # Compute cross entropy loss ÎºÎ±Î¹ probabilities
        ce_loss = F.cross_entropy(active_inputs, active_targets, reduction='none')
        pt = torch.exp(-ce_loss)  # Confidence Î³Î¹Î± Ï„Î·Î½ true class
        
        # Alpha weighting (class balancing)
        alpha_t = self._compute_alpha_weights(active_targets, active_inputs.device)
        
        # Focal loss computation: -Î±_t * (1-p_t)^Î³ * log(p_t)
        focal_weight = alpha_t * (1 - pt) ** self.gamma
        focal_loss = focal_weight * ce_loss
        
        # Apply reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:  # 'none'
            return focal_loss
    
    def _compute_alpha_weights(self, targets: torch.Tensor, device: torch.device) -> torch.Tensor:
        """
        Compute alpha weights Î³Î¹Î± ÎºÎ¬Î¸Îµ target.
        
        Args:
            targets (torch.Tensor): Active target labels
            device (torch.device): Device Î³Î¹Î± tensor operations
        
        Returns:
            torch.Tensor: Alpha weights Î³Î¹Î± ÎºÎ¬Î¸Îµ target
        """
        if self.alpha is None:
            return torch.ones_like(targets, dtype=torch.float, device=device)
        
        if isinstance(self.alpha, (float, int)):
            return torch.full_like(targets, self.alpha, dtype=torch.float, device=device)
        
        # Handle tensor alpha (class-specific weights)
        if isinstance(self.alpha, torch.Tensor):
            # Cache alpha tensor on correct device
            if self._alpha_tensor is None or self._alpha_tensor.device != device:
                self._alpha_tensor = self.alpha.to(device=device, dtype=torch.float)
            
            # Index alpha weights Î¼Îµ Ï„Î± targets
            return self._alpha_tensor[targets]
        
        raise TypeError(f"alpha must be None, float, or torch.Tensor, got {type(self.alpha)}")
    
    def get_config(self) -> dict:
        """
        Get configuration dictionary Î³Î¹Î± serialization.
        
        Returns:
            dict: Configuration parameters
        """
        return {
            'gamma': self.gamma,
            'alpha': self.alpha.tolist() if isinstance(self.alpha, torch.Tensor) else self.alpha,
            'reduction': self.reduction,
            'ignore_index': self.ignore_index
        }
    
    def extra_repr(self) -> str:
        """String representation Î³Î¹Î± debugging."""
        alpha_str = f"tensor({self.alpha.shape})" if isinstance(self.alpha, torch.Tensor) else str(self.alpha)
        return f"gamma={self.gamma}, alpha={alpha_str}, reduction={self.reduction}, ignore_index={self.ignore_index}"


class BalancedFocalLoss(AdaptiveFocalLoss):
    """
    Focal Loss Î¼Îµ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î¿ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ class weights Î²Î¬ÏƒÎµÎ¹ frequency.
    
    Î‘Ï…Ï„Î® Î· ÎºÎ»Î¬ÏƒÎ· ÎµÏ€ÎµÎºÏ„ÎµÎ¯Î½ÎµÎ¹ Ï„Î¿ AdaptiveFocalLoss Î¼Îµ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÏÏÎ¸Î¼Î¹ÏƒÎ·
    Ï„Ï‰Î½ alpha weights Î²Î¬ÏƒÎµÎ¹ Ï„Î·Ï‚ ÏƒÏ…Ï‡Î½ÏŒÏ„Î·Ï„Î±Ï‚ Ï„Ï‰Î½ classes ÏƒÏ„Î¿ dataset.
    
    Args:
        gamma (float): Focusing parameter
        weight_method (str): ÎœÎ­Î¸Î¿Î´Î¿Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Ï weights:
                           - 'inverse': 1/freq
                           - 'sqrt_inverse': 1/sqrt(freq)  
                           - 'log_inverse': 1/log(freq + 1)
        smooth_factor (float): Smoothing factor Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® division by zero
        reduction (str): Aggregation method
        ignore_index (int): Label value Ï€Î¿Ï… Î±Î³Î½Î¿ÎµÎ¯Ï„Î±Î¹
    
    Example:
        >>> # Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î¿Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ weights
        >>> loss_fn = BalancedFocalLoss(gamma=1.0, weight_method='sqrt_inverse')
        >>> 
        >>> # Î§ÏÎ®ÏƒÎ· ÏƒÎµ training loop
        >>> for batch in dataloader:
        >>>     logits = model(batch['input_ids'])
        >>>     loss = loss_fn(logits, batch['labels'])
        >>>     loss.backward()
    
    Note:
        Î¤Î± class weights Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Î½Ï„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î±Ï€ÏŒ Ï„Î¿ training data
        ÎºÎ±Ï„Î¬ Ï„Î·Î½ Ï€ÏÏÏ„Î· forward pass.
    """
    
    def __init__(
        self,
        gamma: float = 1.0,
        weight_method: str = 'sqrt_inverse',
        smooth_factor: float = 1.0,
        reduction: str = 'mean',
        ignore_index: int = -100
    ):
        # Initialize Î¼Îµ None alpha - Î¸Î± Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„ÎµÎ¯ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±
        super().__init__(gamma=gamma, alpha=None, reduction=reduction, ignore_index=ignore_index)
        
        if weight_method not in ['inverse', 'sqrt_inverse', 'log_inverse']:
            raise ValueError(f"weight_method must be 'inverse', 'sqrt_inverse', or 'log_inverse', got {weight_method}")
        
        self.weight_method = weight_method
        self.smooth_factor = smooth_factor
        self._weights_computed = False
    
    def compute_class_weights(self, targets: torch.Tensor, num_classes: int) -> torch.Tensor:
        """
        Compute class weights Î²Î¬ÏƒÎµÎ¹ frequency ÏƒÏ„Î¿ dataset.
        
        Args:
            targets (torch.Tensor): All target labels Î±Ï€ÏŒ Ï„Î¿ dataset
            num_classes (int): Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ classes
        
        Returns:
            torch.Tensor: Class weights [num_classes]
        """
        # Count frequencies (Î±Î³Î½Î¿ÏÎ½Ï„Î±Ï‚ ignore_index)
        valid_targets = targets[targets != self.ignore_index]
        class_counts = torch.bincount(valid_targets, minlength=num_classes).float()
        
        # Add smoothing
        class_counts = class_counts + self.smooth_factor
        
        # Compute weights based on method
        if self.weight_method == 'inverse':
            weights = 1.0 / class_counts
        elif self.weight_method == 'sqrt_inverse':
            weights = 1.0 / torch.sqrt(class_counts)
        elif self.weight_method == 'log_inverse':
            weights = 1.0 / torch.log(class_counts + 1)
        
        # Normalize weights
        weights = weights / weights.mean()
        
        return weights
    
    def set_class_weights(self, targets: torch.Tensor, num_classes: int):
        """
        Set class weights Î²Î¬ÏƒÎµÎ¹ Ï„Î¿Ï… training dataset.
        
        Args:
            targets (torch.Tensor): Training targets
            num_classes (int): Number of classes
        """
        self.alpha = self.compute_class_weights(targets, num_classes)
        self._weights_computed = True
        
        print(f"ğŸ“Š Computed class weights ({self.weight_method}):")
        for i, weight in enumerate(self.alpha):
            print(f"  Class {i}: {weight:.4f}")


def create_focal_loss(
    loss_type: str = "adaptive_focal",
    gamma: float = 1.0,
    alpha: Optional[Union[float, torch.Tensor]] = None,
    reduction: str = 'mean',
    **kwargs
) -> nn.Module:
    """
    Factory function Î³Î¹Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± focal loss implementations.
    
    Args:
        loss_type (str): Type of focal loss:
                        - 'adaptive_focal': AdaptiveFocalLoss
                        - 'balanced_focal': BalancedFocalLoss  
                        - 'standard_ce': Standard CrossEntropyLoss
                        - 'weighted_ce': Weighted CrossEntropyLoss
        gamma (float): Focusing parameter Î³Î¹Î± focal losses
        alpha: Class weights (depends on loss_type)
        reduction (str): Reduction method
        **kwargs: Additional arguments Î³Î¹Î± specific loss types
    
    Returns:
        nn.Module: Configured loss function
    
    Example:
        >>> # Adaptive focal loss Î¼Îµ custom weights
        >>> class_weights = torch.tensor([0.1, 2.0, 3.0, 2.5, ...])
        >>> loss_fn = create_focal_loss('adaptive_focal', gamma=1.0, alpha=class_weights)
        
        >>> # Balanced focal loss Î¼Îµ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± weights
        >>> loss_fn = create_focal_loss('balanced_focal', gamma=1.0, weight_method='sqrt_inverse')
        
        >>> # Standard cross entropy Î³Î¹Î± comparison
        >>> loss_fn = create_focal_loss('standard_ce')
    """
    if loss_type == "adaptive_focal":
        return AdaptiveFocalLoss(
            gamma=gamma,
            alpha=alpha,
            reduction=reduction,
            ignore_index=kwargs.get('ignore_index', -100)
        )
    
    elif loss_type == "balanced_focal":
        return BalancedFocalLoss(
            gamma=gamma,
            weight_method=kwargs.get('weight_method', 'sqrt_inverse'),
            smooth_factor=kwargs.get('smooth_factor', 1.0),
            reduction=reduction,
            ignore_index=kwargs.get('ignore_index', -100)
        )
    
    elif loss_type == "standard_ce":
        return nn.CrossEntropyLoss(
            weight=alpha if isinstance(alpha, torch.Tensor) else None,
            reduction=reduction,
            ignore_index=kwargs.get('ignore_index', -100)
        )
    
    elif loss_type == "weighted_ce":
        if alpha is None:
            raise ValueError("weighted_ce requires alpha parameter")
        return nn.CrossEntropyLoss(
            weight=alpha,
            reduction=reduction,
            ignore_index=kwargs.get('ignore_index', -100)
        )
    
    else:
        raise ValueError(f"Unknown loss_type: {loss_type}. "
                        f"Supported: 'adaptive_focal', 'balanced_focal', 'standard_ce', 'weighted_ce'")


# Configuration constants
DEFAULT_GAMMA = 1.0  # Conservative Î³Î¹Î± NER
DEFAULT_REDUCTION = 'mean'
DEFAULT_IGNORE_INDEX = -100

__all__ = [
    'AdaptiveFocalLoss',
    'BalancedFocalLoss', 
    'create_focal_loss',
    'DEFAULT_GAMMA',
    'DEFAULT_REDUCTION', 
    'DEFAULT_IGNORE_INDEX'
]
