#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Î³Î¹Î± Ï€ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î¹ÎµÏÎ±ÏÏ‡Î¹ÎºÏÎ½ Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½ Î±Ï€ÏŒ EURLEX57K train labels
ÏƒÏ„Î¿ enhanced concepts Î±ÏÏ‡ÎµÎ¯Î¿
"""

import json
import os
from collections import defaultdict
from typing import Dict, Any, List, Set

def load_enhanced_concepts(file_path: str) -> Dict[str, Any]:
    """Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… enhanced concepts Î±ÏÏ‡ÎµÎ¯Î¿Ï…"""
    print("ğŸ”„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· enhanced concepts...")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(data)} concepts")
        return data
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·: {e}")
        return {}

def load_train_labels(file_path: str) -> List[Dict[str, Any]]:
    """Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Ï‰Î½ train labels Î±Ï€ÏŒ JSONL"""
    print("ğŸ”„ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· train labels...")
    
    labels = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    label_data = json.loads(line.strip())
                    labels.append(label_data)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î· Î³ÏÎ±Î¼Î¼Î® {line_num}: {e}")
                    continue
                    
                # Progress indicator Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± Î±ÏÏ‡ÎµÎ¯Î±
                if line_num % 50000 == 0:
                    print(f"   ğŸ“Š Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î³ÏÎ±Î¼Î¼Î®Ï‚ {line_num:,}")
                    
        print(f"âœ… Î¦Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ {len(labels):,} label entries")
        return labels
        
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· train labels: {e}")
        return []

def create_label_to_concept_mapping(enhanced_concepts: Dict[str, Any]) -> Dict[str, str]:
    """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± mapping Î±Ï€ÏŒ label text ÏƒÎµ concept ID"""
    print("ğŸ”„ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± label-to-concept mapping...")
    
    label_mapping = {}
    
    for concept_id, concept_data in enhanced_concepts.items():
        # ÎšÏÏÎ¹Î¿ title
        title = concept_data.get('title', '').strip()
        if title:
            label_mapping[title] = concept_id
            
        # Alt labels
        alt_labels = concept_data.get('alt_labels', [])
        for alt_label in alt_labels:
            if alt_label and alt_label.strip():
                label_mapping[alt_label.strip()] = concept_id
                
        # EURLEX label Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹
        eurlex_label = concept_data.get('eurlex_label', '').strip()
        if eurlex_label:
            label_mapping[eurlex_label] = concept_id
    
    print(f"âœ… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ mapping Î³Î¹Î± {len(label_mapping):,} labels")
    return label_mapping

def build_hierarchical_paths(train_labels: List[Dict[str, Any]], 
                           label_mapping: Dict[str, str]) -> Dict[str, List[Dict[str, Any]]]:
    """ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Î¹ÎµÏÎ±ÏÏ‡Î¹ÎºÏÎ½ Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½ Î³Î¹Î± ÎºÎ¬Î¸Îµ concept"""
    print("ğŸ”„ ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Î¹ÎµÏÎ±ÏÏ‡Î¹ÎºÏÎ½ Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½...")
    
    concept_paths = defaultdict(list)
    unmatched_labels = set()
    
    for entry in train_labels:
        # Î•Î¾Î±Î³Ï‰Î³Î® path Î±Ï€ÏŒ levels
        path_levels = []
        for level in ['level_1_label', 'level_2_label', 'level_3_label', 'level_4_label', 'level_5_label']:
            label = entry.get(level, '').strip()
            if label:
                path_levels.append(label)
            else:
                break  # Î£Ï„Î±Î¼Î±Ï„Î¬Î¼Îµ ÏƒÏ„Î¿ Ï€ÏÏÏ„Î¿ ÎºÎµÎ½ÏŒ level
        
        if not path_levels:
            continue
            
        # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ Ï„Î¿ concept ID Î³Î¹Î± Ï„Î¿ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ (leaf) label
        leaf_label = path_levels[-1]
        concept_id = label_mapping.get(leaf_label)
        
        if concept_id:
            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± path object
            path_obj = {
                "path": path_levels,
                "depth": len(path_levels),
                "root_category": path_levels[0],
                "leaf_concept": leaf_label,
                "celex_id": entry.get('celex_id', ''),
                "full_path_string": " > ".join(path_levels)
            }
            
            concept_paths[concept_id].append(path_obj)
        else:
            unmatched_labels.add(leaf_label)
    
    print(f"âœ… Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎ±Î½ Î´Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚ Î³Î¹Î± {len(concept_paths):,} concepts")
    print(f"âš ï¸  {len(unmatched_labels):,} unmatched labels")
    
    return dict(concept_paths)

def calculate_path_statistics(paths: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ statistics Î³Î¹Î± Ï„Î¹Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚ ÎµÎ½ÏŒÏ‚ concept"""
    if not paths:
        return {}
    
    depths = [p['depth'] for p in paths]
    root_categories = [p['root_category'] for p in paths]
    celex_ids = [p['celex_id'] for p in paths if p['celex_id']]
    
    return {
        "total_paths": len(paths),
        "unique_paths": len(set(p['full_path_string'] for p in paths)),
        "depth_stats": {
            "min_depth": min(depths),
            "max_depth": max(depths),
            "avg_depth": round(sum(depths) / len(depths), 2)
        },
        "root_categories": {
            "unique_roots": list(set(root_categories)),
            "root_count": len(set(root_categories))
        },
        "document_coverage": {
            "unique_documents": len(set(celex_ids)),
            "total_document_mentions": len(celex_ids)
        }
    }

def enhance_concepts_with_paths(enhanced_concepts: Dict[str, Any], 
                              concept_paths: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
    """Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î¹ÎµÏÎ±ÏÏ‡Î¹ÎºÏÎ½ Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½ ÏƒÏ„Î± concepts"""
    print("ğŸ”„ Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î¹ÎµÏÎ±ÏÏ‡Î¹ÎºÏÎ½ Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½ ÏƒÏ„Î± concepts...")
    
    enhanced_count = 0
    
    for concept_id, concept_data in enhanced_concepts.items():
        if concept_id in concept_paths:
            paths = concept_paths[concept_id]
            
            # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· duplicates
            unique_paths = []
            seen_paths = set()
            for path in paths:
                path_key = path['full_path_string']
                if path_key not in seen_paths:
                    unique_paths.append(path)
                    seen_paths.add(path_key)
            
            # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· ÏƒÏ„Î¿ concept
            concept_data['hierarchical_paths'] = unique_paths
            concept_data['path_metadata'] = calculate_path_statistics(unique_paths)
            
            enhanced_count += 1
    
    print(f"âœ… Î ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎ±Î½ Î´Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚ ÏƒÎµ {enhanced_count:,} concepts")
    return enhanced_concepts

def save_enhanced_concepts_with_paths(enhanced_concepts: Dict[str, Any], output_path: str):
    """Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ enhanced concepts Î¼Îµ paths"""
    print(f"ğŸ’¾ Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· enhanced concepts Î¼Îµ paths ÏƒÏ„Î¿ {output_path}...")
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enhanced_concepts, f, ensure_ascii=False, indent=2)
        print("âœ… Î•Ï€Î¹Ï„Ï…Ï‡Î®Ï‚ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·!")
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·: {e}")

def generate_path_statistics_report(enhanced_concepts: Dict[str, Any]) -> Dict[str, Any]:
    """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¿Ï report statistics"""
    print("ğŸ“Š Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± statistics report...")
    
    concepts_with_paths = 0
    total_paths = 0
    all_depths = []
    all_root_categories = set()
    all_documents = set()
    
    for concept_id, concept_data in enhanced_concepts.items():
        if 'hierarchical_paths' in concept_data:
            concepts_with_paths += 1
            paths = concept_data['hierarchical_paths']
            total_paths += len(paths)
            
            for path in paths:
                all_depths.append(path['depth'])
                all_root_categories.add(path['root_category'])
                if path['celex_id']:
                    all_documents.add(path['celex_id'])
    
    report = {
        "concepts_overview": {
            "total_concepts": len(enhanced_concepts),
            "concepts_with_paths": concepts_with_paths,
            "concepts_without_paths": len(enhanced_concepts) - concepts_with_paths,
            "path_coverage_percentage": round((concepts_with_paths / len(enhanced_concepts)) * 100, 2)
        },
        "paths_overview": {
            "total_hierarchical_paths": total_paths,
            "average_paths_per_concept": round(total_paths / concepts_with_paths, 2) if concepts_with_paths > 0 else 0
        },
        "depth_analysis": {
            "min_depth": min(all_depths) if all_depths else 0,
            "max_depth": max(all_depths) if all_depths else 0,
            "avg_depth": round(sum(all_depths) / len(all_depths), 2) if all_depths else 0,
            "depth_distribution": {str(d): all_depths.count(d) for d in set(all_depths)}
        },
        "category_analysis": {
            "unique_root_categories": len(all_root_categories),
            "root_categories_list": sorted(list(all_root_categories))
        },
        "document_analysis": {
            "unique_source_documents": len(all_documents),
            "total_document_references": len([path['celex_id'] for concept_data in enhanced_concepts.values() 
                                           if 'hierarchical_paths' in concept_data 
                                           for path in concept_data['hierarchical_paths'] 
                                           if path['celex_id']])
        }
    }
    
    return report

def main():
    """ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·"""
    print("ğŸš€ Î Î¡ÎŸÎ£Î˜Î—ÎšÎ— Î™Î•Î¡Î‘Î¡Î§Î™ÎšÎ©Î Î”Î™Î‘Î”Î¡ÎŸÎœÎ©Î Î£Î¤Î‘ CONCEPTS")
    print("=" * 60)
    
    # Î”Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚ Î±ÏÏ‡ÎµÎ¯Ï‰Î½
    data_dir = os.path.dirname(os.path.abspath(__file__))
    enhanced_concepts_path = os.path.join(data_dir, "eurovoc_enhanced_concepts_with_eurlex.json")
    train_labels_path = os.path.join(data_dir, "eurlex57k_train_labels.jsonl")
    output_path = os.path.join(data_dir, "eurovoc_enhanced_concepts_with_paths.json")
    stats_path = os.path.join(data_dir, "hierarchical_paths_statistics.json")
    
    # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    enhanced_concepts = load_enhanced_concepts(enhanced_concepts_path)
    if not enhanced_concepts:
        print("âŒ Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï†Î¿ÏÏ„ÏÏƒÏ‰ Ï„Î± enhanced concepts")
        return
    
    train_labels = load_train_labels(train_labels_path)
    if not train_labels:
        print("âŒ Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï†Î¿ÏÏ„ÏÏƒÏ‰ Ï„Î± train labels")
        return
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± mappings ÎºÎ±Î¹ paths
    label_mapping = create_label_to_concept_mapping(enhanced_concepts)
    concept_paths = build_hierarchical_paths(train_labels, label_mapping)
    
    # Î•Ï€Î­ÎºÏ„Î±ÏƒÎ· concepts Î¼Îµ paths
    enhanced_concepts_with_paths = enhance_concepts_with_paths(enhanced_concepts, concept_paths)
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± statistics report
    stats_report = generate_path_statistics_report(enhanced_concepts_with_paths)
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
    save_enhanced_concepts_with_paths(enhanced_concepts_with_paths, output_path)
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· statistics
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats_report, f, ensure_ascii=False, indent=2)
    
    print("\nğŸ“Š Î¤Î•Î›Î™ÎšÎ‘ STATISTICS:")
    print(f"ğŸ“ Concepts Î¼Îµ paths: {stats_report['concepts_overview']['concepts_with_paths']:,}")
    print(f"ğŸ“ˆ Coverage: {stats_report['concepts_overview']['path_coverage_percentage']}%")
    print(f"ğŸŒ³ Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚: {stats_report['paths_overview']['total_hierarchical_paths']:,}")
    print(f"ğŸ“š Root categories: {stats_report['category_analysis']['unique_root_categories']}")
    print(f"ğŸ“„ Source documents: {stats_report['document_analysis']['unique_source_documents']:,}")
    
    print(f"\nğŸ‰ ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î©Î£Î—!")
    print(f"ğŸ“ Enhanced concepts: {output_path}")
    print(f"ğŸ“Š Statistics report: {stats_path}")

if __name__ == "__main__":
    main()
