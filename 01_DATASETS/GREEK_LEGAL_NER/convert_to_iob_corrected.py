#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Î³Î¹Î± Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Ï‰Î½ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÏÎ½ JSON datasets ÏƒÎµ IOB format
Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ mapping Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎµ Î±Ï€ÏŒ Ï„Î·Î½ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ HuggingFace datasets
"""

import json
import os

def get_correct_label_mapping():
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ mapping Î±Ï€ÏŒ Î±ÏÎ¹Î¸Î¼Î¿ÏÏ‚ ÏƒÎµ IOB labels
    Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÏ„Î·Î½ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ HuggingFace datasets
    """
    return {
        0: 'O',
        1: 'B-ORG',
        2: 'I-ORG', 
        3: 'B-GPE',
        4: 'I-GPE',
        5: 'B-LEG-REFS',
        6: 'I-LEG-REFS',
        7: 'B-PUBLIC-DOCS',
        8: 'I-PUBLIC-DOCS',
        9: 'B-PERSON',
        10: 'I-PERSON',
        11: 'B-FACILITY',
        12: 'I-FACILITY',
        13: 'B-LOCATION-UNK',
        14: 'I-LOCATION-UNK',
        15: 'B-LOCATION-NAT',
        16: 'I-LOCATION-NAT'
    }

def convert_numeric_labels_to_correct_iob(labels_list):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬ labels ÏƒÎµ ÏƒÏ‰ÏƒÏ„Î¬ IOB strings
    
    Args:
        labels_list (list): Lista Î¼Îµ Ï„Î± Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬ labels Î¼Î¹Î±Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚
        
    Returns:
        list: Lista Î¼Îµ ÏƒÏ‰ÏƒÏ„Î¬ IOB tags
    """
    label_mapping = get_correct_label_mapping()
    iob_tags = []
    
    for label in labels_list:
        iob_tags.append(label_mapping.get(int(label), f'UNKNOWN_{int(label)}'))
    
    return iob_tags

def convert_dataset_to_correct_iob(input_file, output_file):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î­Î½Î± dataset Î±Ï€ÏŒ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬ labels ÏƒÎµ ÏƒÏ‰ÏƒÏ„ÏŒ IOB format
    """
    print(f"ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Ï‰ {input_file} -> {output_file}")
    
    # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… Î±ÏÏ‡Î¹ÎºÎ¿Ï dataset
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Î•Î¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
    inputs = data.get('input', [])
    numeric_labels = data.get('label', [])
    languages = data.get('language', ['el'] * len(inputs))  # Default ÏƒÎµ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹
    
    print(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(inputs)} Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚")
    print(f"  - Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(numeric_labels)} label sequences")
    
    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® labels ÏƒÎµ IOB format
    iob_labels = []
    total_tokens = 0
    
    for sentence_labels in numeric_labels:
        iob_tags = convert_numeric_labels_to_correct_iob(sentence_labels)
        iob_labels.append(iob_tags)
        total_tokens += len(sentence_labels)
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î¿Ï… dataset
    iob_dataset = {
        "input": inputs,
        "label": iob_labels,
        "language": languages
    }
    
    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏƒÏ…Î½Î­Ï€ÎµÎ¹Î±Ï‚
    assert len(inputs) == len(iob_labels), "Mismatch ÏƒÎµ Î±ÏÎ¹Î¸Î¼ÏŒ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½!"
    for i, (tokens, labels) in enumerate(zip(inputs, iob_labels)):
        assert len(tokens) == len(labels), f"Mismatch ÏƒÎµ Ï€ÏÏŒÏ„Î±ÏƒÎ· {i}: {len(tokens)} tokens vs {len(labels)} labels"
    
    print(f"  âœ… ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏƒÏ…Î½Î­Ï€ÎµÎ¹Î±Ï‚: OK")
    
    # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ IOB labels
    all_iob_labels = [label for sentence in iob_labels for label in sentence]
    unique_labels = set(all_iob_labels)
    print(f"  ğŸ“Š Unique IOB labels: {len(unique_labels)}")
    print(f"     {sorted(unique_labels)}")
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(iob_dataset, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ Î¼Îµ {total_tokens} tokens")
    
    return len(unique_labels), total_tokens

def validate_iob_format(iob_labels):
    """
    Î•Î»Î­Î³Ï‡ÎµÎ¹ Î±Î½ Ï„Î± IOB labels ÎµÎ¯Î½Î±Î¹ ÏƒÏ‰ÏƒÏ„Î¬ ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÎ¼Î­Î½Î±
    """
    errors = []
    
    for sentence_idx, sentence_labels in enumerate(iob_labels):
        for token_idx, label in enumerate(sentence_labels):
            if label.startswith('I-'):
                # Î•Î»Î­Î³Ï‡ÎµÎ¹ Î±Î½ Ï„Î¿ I- Î±ÎºÎ¿Î»Î¿Ï…Î¸ÎµÎ¯ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ B- Î® I-
                if token_idx == 0:
                    errors.append(f"Sentence {sentence_idx}, Token {token_idx}: I- tag at start of sentence")
                else:
                    prev_label = sentence_labels[token_idx - 1]
                    entity_type = label[2:]  # Î‘Ï†Î±Î¹ÏÎµÎ¯ Ï„Î¿ 'I-'
                    
                    if prev_label == 'O':
                        errors.append(f"Sentence {sentence_idx}, Token {token_idx}: I-{entity_type} after O")
                    elif prev_label.startswith('B-') or prev_label.startswith('I-'):
                        prev_entity_type = prev_label[2:]
                        if prev_entity_type != entity_type:
                            errors.append(f"Sentence {sentence_idx}, Token {token_idx}: I-{entity_type} after {prev_label}")
    
    return errors

def main():
    """
    ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ - Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î· Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î®
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Datasets Î³Î¹Î± Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î®
    datasets = {
        "train": {
            "input": os.path.join(script_dir, "train.json"),
            "output": os.path.join(script_dir, "train_iob_corrected.json")
        },
        "test": {
            "input": os.path.join(script_dir, "test.json"),
            "output": os.path.join(script_dir, "test_iob_corrected.json")
        },
        "validation": {
            "input": os.path.join(script_dir, "validation.json"),
            "output": os.path.join(script_dir, "validation_iob_corrected.json")
        }
    }

    print("="*80)
    print("Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— ÎœÎ•Î¤Î‘Î¤Î¡ÎŸÎ Î— DATASETS Î£Î• IOB FORMAT")
    print("="*80)

    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… ÏƒÏ‰ÏƒÏ„Î¿Ï mapping
    mapping = get_correct_label_mapping()
    print(f"âœ… Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Ï Ï„Î¿ ÏƒÏ‰ÏƒÏ„ÏŒ mapping:")
    for num, label in sorted(mapping.items()):
        print(f"   {num}: {label}")
    print()

    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ datasets
    total_datasets = 0
    total_tokens = 0
    all_unique_labels = set()

    for dataset_name, paths in datasets.items():
        if not os.path.exists(paths["input"]):
            print(f"âš ï¸  Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ {paths['input']} Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹! Î Î±ÏÎ±Î»ÎµÎ¯Ï€ÎµÏ„Î±Î¹...")
            continue
        
        try:
            unique_labels_count, token_count = convert_dataset_to_correct_iob(
                paths["input"], 
                paths["output"]
            )
            total_datasets += 1
            total_tokens += token_count
            
            # Î£Ï…Î»Î»Î¿Î³Î® unique labels Î³Î¹Î± validation
            with open(paths["output"], 'r', encoding='utf-8') as f:
                iob_data = json.load(f)
            
            dataset_labels = [label for sentence in iob_data["label"] for label in sentence]
            all_unique_labels.update(dataset_labels)
            
        except Exception as e:
            print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î· Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î¿Ï… {dataset_name}: {e}")

    # Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Î­Î»ÎµÎ³Ï‡Î¿Ï‚
    print("\n" + "="*80)
    print("Î£Î¥ÎÎŸÎ¨Î— Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ—Î£ ÎœÎ•Î¤Î‘Î¤Î¡ÎŸÎ Î—Î£")
    print("="*80)
    print(f"âœ… ÎœÎµÏ„Î±Ï„ÏÎ¬Ï€Î·ÎºÎ±Î½ {total_datasets} datasets")
    print(f"ğŸ“Š Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ tokens: {total_tokens:,}")
    print(f"ğŸ·ï¸  Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ unique IOB labels: {len(all_unique_labels)}")

    print("\nğŸ“‹ ÎŒÎ»Î± Ï„Î± IOB labels:")
    for label in sorted(all_unique_labels):
        print(f"   {label}")

    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ IOB format correctness
    print("\nğŸ” ÎˆÎ›Î•Î“Î§ÎŸÎ£ IOB FORMAT CORRECTNESS:")
    all_errors = []

    for dataset_name, paths in datasets.items():
        if os.path.exists(paths["output"]):
            with open(paths["output"], 'r', encoding='utf-8') as f:
                iob_data = json.load(f)
            
            errors = validate_iob_format(iob_data["label"])
            if errors:
                print(f"âŒ {dataset_name}: {len(errors)} errors found")
                all_errors.extend(errors)
            else:
                print(f"âœ… {dataset_name}: No IOB format errors")

    if all_errors:
        print(f"\nâš ï¸  Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ {len(all_errors)} IOB format errors:")
        for error in all_errors[:10]:  # Show first 10 errors
            print(f"   {error}")
        if len(all_errors) > 10:
            print(f"   ... ÎºÎ±Î¹ {len(all_errors) - 10} Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ±")
    else:
        print(f"\nâœ… ÎŒÎ»Î± Ï„Î± datasets Î­Ï‡Î¿Ï…Î½ ÏƒÏ‰ÏƒÏ„ÏŒ IOB format!")

    print("="*80)
    print("Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— ÎœÎ•Î¤Î‘Î¤Î¡ÎŸÎ Î— ÎŸÎ›ÎŸÎšÎ›Î—Î¡Î©Î˜Î—ÎšÎ•!")
    print("="*80)

if __name__ == "__main__":
    main()
