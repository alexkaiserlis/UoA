# NER for Legal Documents

A comprehensive Named Entity Recognition (NER) system for Greek legal documents, developed as part of a research project at the University of the Aegean.

## Project Overview

This project focuses on developing and evaluating NER models specifically trained on Greek legal text data. It includes data preprocessing, model training, evaluation, and comparison scripts.

## Project Structure

```
â”œâ”€â”€ 01_DATASETS/                    # Dataset management and preprocessing
â”‚   â”œâ”€â”€ GREEK_LEGAL_NER/           # Main NER dataset
â”‚   â”‚   â”œâ”€â”€ *.py                   # Data processing scripts
â”‚   â”‚   â”œâ”€â”€ *.json                 # Training/validation/test data
â”‚   â”‚   â””â”€â”€ NER MODEL/             # Trained models
â”‚   â”œâ”€â”€ ETHNIKO_TYPOGRAFEIO/       # Legal documents from Greek National Printing Office
â”‚   â”œâ”€â”€ HELLASVOC/                 # Greek legal vocabulary dataset
â”‚   â””â”€â”€ RAW_DATA/                  # Raw unprocessed data
â”‚
â”œâ”€â”€ 02_ANALYSIS_SCRIPTS/           # Data analysis and comparison tools
â”‚   â”œâ”€â”€ COMPARISON/                # Dataset comparison utilities
â”‚   â”œâ”€â”€ DATASET_ANALYSIS/          # Statistical analysis of datasets
â”‚   â””â”€â”€ ENTITY_COUNTING/           # Entity statistics and counting
â”‚
â”œâ”€â”€ 03_ML_MODELS/                  # Machine Learning models and training
â”‚   â”œâ”€â”€ NER_MODELS/               # Main NER model implementations
â”‚   â”‚   â”œâ”€â”€ *.py                  # Training and evaluation scripts
â”‚   â”‚   â”œâ”€â”€ config.py             # Model configurations
â”‚   â”‚   â”œâ”€â”€ experiments/          # Training experiments and results
â”‚   â”‚   â””â”€â”€ logs/                 # Training logs
â”‚   â”œâ”€â”€ BASELINE_MODELS/          # Baseline model implementations
â”‚   â””â”€â”€ EXPERIMENTAL/             # Experimental model architectures
â”‚
â”œâ”€â”€ 04_RESULTS/                   # Training results and analysis
â”‚   â”œâ”€â”€ COMPARISON_RESULTS/       # Model comparison results
â”‚   â”œâ”€â”€ CRF_RESULTS/             # CRF model specific results
â”‚   â””â”€â”€ FOCAL_LOSS_RESULTS/      # Focal loss experiments
â”‚
â”œâ”€â”€ 05_SCRAPERS/                 # Web scraping utilities
â”‚   â”œâ”€â”€ ET_SCRAPER/              # Ethniko Typografeio scraper
â”‚   â”œâ”€â”€ FEK_SCRAPER/             # FEK (Government Gazette) scraper
â”‚   â””â”€â”€ HELLASVOC_SCRAPER/       # Hellasvoc dataset scraper
â”‚
â”œâ”€â”€ 06_CONFIGS/                  # Configuration files
â”‚   â”œâ”€â”€ MODEL_CONFIGS/           # Model-specific configurations
â”‚   â””â”€â”€ VSCODE/                  # VS Code settings
â”‚
â”œâ”€â”€ 07_DOCUMENTATION/            # Project documentation
â”‚   â”œâ”€â”€ ANALYSIS_REPORTS/        # Analysis and evaluation reports
â”‚   â””â”€â”€ README_FILES/            # Various README files
â”‚
â””â”€â”€ FINAL_VOCABULARY_PACKAGE/    # Legal vocabulary processing tools
    â”œâ”€â”€ scripts/                 # Vocabulary processing scripts
    â””â”€â”€ IR/                      # Information Retrieval tools
```

## Key Features

### ðŸ¤– Machine Learning Models
- **Transformer-based NER**: RoBERTa models fine-tuned for Greek legal text
- **CRF Enhancement**: Conditional Random Field layer for sequence labeling
- **Focal Loss**: Advanced loss function for handling class imbalance
- **Multi-run Experiments**: Statistical validation with multiple training runs

### ðŸ“Š Analysis Tools
- Dataset comparison and statistics
- Entity distribution analysis
- Model performance evaluation
- Cross-dataset validation

### ðŸ”„ Data Processing
- IOB format conversion
- Data cleaning and preprocessing
- Dataset combination utilities
- Entity validation tools

## Getting Started

### Prerequisites
- Python 3.8+
- PyTorch
- Transformers library
- scikit-learn
- pandas
- numpy

### Installation
1. Clone the repository:
```bash
git clone https://github.com/alexkaiserlis/UoA.git
cd UoA
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Usage

#### Training a Model
```bash
cd 03_ML_MODELS/NER_MODELS
python main_script.py --preset CRF_ENHANCED
```

#### Multi-run Experiments
```bash
# Run 6 experiments with different seeds for statistical validation
python main_script.py --preset CRF_ENHANCED --num-runs 6 --base-seed 42
```

#### Data Analysis
```bash
cd 02_ANALYSIS_SCRIPTS/DATASET_ANALYSIS
python greek_legal_ner_analysis.py
```

## Model Configurations

The project includes several pre-configured model setups:

- **BASELINE**: Standard RoBERTa model for NER
- **CRF_ENHANCED**: RoBERTa + CRF layer for improved sequence labeling
- **FOCAL_LOSS**: Using focal loss to handle class imbalance
- **AUGMENTED**: Data augmentation techniques

## Dataset Information

### Greek Legal NER Dataset
- **Entities**: PERSON, ORGANIZATION, LOCATION, LAW, etc.
- **Format**: IOB tagging scheme
- **Language**: Greek
- **Domain**: Legal documents

### Data Sources
- Greek National Printing Office (Ethniko Typografeio)
- Government Gazette (FEK)
- Legal vocabulary databases

## Results and Evaluation

The project includes comprehensive evaluation metrics:
- F1-score per entity type
- Overall F1-score
- Precision and Recall
- Confusion matrices
- Statistical significance testing

## Multi-run Feature

For statistical reliability, the system supports multi-run experiments:
- Multiple training runs with different seeds
- Aggregated statistics (mean, std, min, max)
- Individual and combined results
- Robust evaluation methodology

See [MULTI_RUN_GUIDE.md](03_ML_MODELS/NER_MODELS/MULTI_RUN_GUIDE.md) for detailed usage instructions.
