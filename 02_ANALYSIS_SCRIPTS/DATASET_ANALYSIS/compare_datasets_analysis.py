"""
Comparative Greek Legal NER Dataset Analysis Script

Î‘Î½Î±Î»ÏÎµÎ¹ ÎºÎ±Î¹ ÏƒÏ…Î³ÎºÏÎ¯Î½ÎµÎ¹ Î´ÏÎ¿ datasets:
1. joelniklaus/greek_legal_ner 
2. joelito/lextreme (greek_legal_ner config)

ÎœÎµÏ„ÏÎ¬ÎµÎ¹:
- Î ÏŒÏƒÎ± documents Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÎµ ÎºÎ¬Î¸Îµ split
- Î ÏŒÏƒÎ± instances Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± IOB
- Î£Ï…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ® Î±Î½Î¬Î»Ï…ÏƒÎ· Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ Î´ÏÎ¿ datasets

Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±: Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ ÏƒÎµ txt Î±ÏÏ‡ÎµÎ¯Î±
"""

import os
from datasets import load_dataset
from collections import Counter, defaultdict
from datetime import datetime


def get_iob_label_mapping():
    """
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ mapping Î±Ï€ÏŒ Î±ÏÎ¹Î¸Î¼Î¿ÏÏ‚ ÏƒÎµ IOB labels Î³Î¹Î± Ï„Î¿ joelito/lextreme dataset
    Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÏ„Î·Î½ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ® Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¯Î± Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎµ Î±Ï€ÏŒ Ï„Î± datasets
    """
    return {
        0: 'O',
        1: 'B-ORG',
        2: 'I-ORG', 
        3: 'B-GPE',
        4: 'I-GPE',
        5: 'B-LEG-REFS',
        6: 'I-LEG-REFS',
        7: 'B-PUBLIC-DOCS',
        8: 'I-PUBLIC-DOCS',
        9: 'B-PERSON',
        10: 'I-PERSON',
        11: 'B-FACILITY',
        12: 'I-FACILITY',
        13: 'B-LOCATION-UNK',
        14: 'I-LOCATION-UNK',
        15: 'B-LOCATION-NAT',
        16: 'I-LOCATION-NAT'
    }


def convert_numeric_labels_to_iob(labels, label_mapping=None):
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬ labels ÏƒÎµ IOB strings
    
    Args:
        labels: Î›Î¯ÏƒÏ„Î± Î¼Îµ Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¬ labels
        label_mapping: Dictionary Î³Î¹Î± Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î±ÏÎ¹Î¸Î¼ÏÎ½ ÏƒÎµ strings
    
    Returns:
        Î›Î¯ÏƒÏ„Î± Î¼Îµ IOB string labels
    """
    if label_mapping is None:
        label_mapping = get_iob_label_mapping()
    
    converted_labels = []
    for label in labels:
        if isinstance(label, (int, float)):
            converted_labels.append(label_mapping.get(int(label), f'UNKNOWN_{int(label)}'))
        else:
            converted_labels.append(str(label))
    
    return converted_labels


def analyze_dataset(dataset_name, dataset_config=None, dataset_alias=None):
    """
    Î‘Î½Î±Î»ÏÎµÎ¹ Î­Î½Î± ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ dataset
    
    Args:
        dataset_name (str): ÎŒÎ½Î¿Î¼Î± Ï„Î¿Ï… dataset
        dataset_config (str): Configuration Ï„Î¿Ï… dataset (Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹)
        dataset_alias (str): Alias Î³Î¹Î± Ï„Î¿ dataset ÏƒÏ„Î·Î½ Î±Î½Î±Ï†Î¿ÏÎ¬
    
    Returns:
        dict: Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Î¿Ï… dataset
    """
    
    alias = dataset_alias or dataset_name
    print(f"ğŸ”„ Î¦Î¿ÏÏ„ÏÎ½Ï‰ Ï„Î¿ dataset: {alias}")
    
    try:
        # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· dataset
        if dataset_config:
            dataset = load_dataset(dataset_name, dataset_config)
        else:
            dataset = load_dataset(dataset_name)
        print(f"âœ… Dataset {alias} Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
        
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… dataset {alias}: {e}")
        return None
    
    # Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· dictionary Î³Î¹Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±
    results = {
        'dataset_info': {
            'name': dataset_name,
            'config': dataset_config,
            'alias': alias,
            'analysis_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        },
        'splits': {}
    }
    
    print(f"\nğŸ“Š Î‘Î½Î±Î»ÏÏ‰ Ï„Î± splits Ï„Î¿Ï… dataset {alias}...")
    
    # Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ¬Î¸Îµ split
    for split_name in dataset.keys():
        print(f"\nğŸ” Î‘Î½Î±Î»ÏÏ‰ Ï„Î¿ split: {split_name}")
        
        split_data = dataset[split_name]
        num_documents = len(split_data)
        
        # ÎœÎµÏ„ÏÎ·Ï„Î®Ï‚ Î³Î¹Î± IOB tags
        iob_counter = Counter()
        total_tokens = 0
        
        # Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎºÎ¬Î¸Îµ document
        for i, example in enumerate(split_data):
            if i % 100 == 0 and i > 0:
                print(f"   Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± document {i}/{num_documents}")
            
            # Î•Î¾Î±Î³Ï‰Î³Î® NER tags - Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î® Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬ field names
            ner_tags = None
            if 'ner' in example:
                ner_tags = example['ner']
            elif 'ner_tags' in example:
                ner_tags = example['ner_tags']
            elif 'labels' in example:
                ner_tags = example['labels']
            elif 'label' in example:  # Î“Î¹Î± joelito/lextreme
                ner_tags = example['label']
            
            if ner_tags is None:
                if i < 10:  # ÎœÏŒÎ½Î¿ Ï„Î± Ï€ÏÏÏ„Î± 10 warnings
                    print(f"âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ NER tags ÏƒÏ„Î¿ example {i}")
                continue
                
            total_tokens += len(ner_tags)
            
            # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÏÎ½ labels ÏƒÎµ IOB strings Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
            if len(ner_tags) > 0 and isinstance(ner_tags[0], (int, float)):
                ner_tags = convert_numeric_labels_to_iob(ner_tags)
            
            # ÎœÎ­Ï„ÏÎ·ÏƒÎ· ÎºÎ¬Î¸Îµ tag
            for tag in ner_tags:
                iob_counter[tag] += 1
        
        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î³Î¹Î± Ï„Î¿ split
        results['splits'][split_name] = {
            'num_documents': num_documents,
            'total_tokens': total_tokens,
            'iob_distribution': dict(iob_counter),
            'unique_tags': len(iob_counter)
        }
        
        print(f"   âœ… {split_name}: {num_documents:,} documents, {total_tokens:,} tokens")
    
    return results


def generate_single_dataset_report(results):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ Î³Î¹Î± Î­Î½Î± dataset
    
    Args:
        results (dict): Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚
        
    Returns:
        str: Î‘Î½Î±Ï†Î¿ÏÎ¬ ÏƒÎµ Î¼Î¿ÏÏ†Î® ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
    """
    
    if not results:
        return "âŒ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ·"
    
    report_lines = []
    
    # Header
    report_lines.append("="*80)
    report_lines.append(f"Î‘ÎÎ‘Î›Î¥Î£Î— DATASET: {results['dataset_info']['alias'].upper()}")
    report_lines.append("="*80)
    report_lines.append(f"Dataset: {results['dataset_info']['name']}")
    if results['dataset_info']['config']:
        report_lines.append(f"Config: {results['dataset_info']['config']}")
    report_lines.append(f"Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚: {results['dataset_info']['analysis_date']}")
    report_lines.append("")
    
    # Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
    report_lines.append("ğŸ“Š Î£Î¥ÎÎŸÎ›Î™ÎšÎ‘ Î£Î¤Î‘Î¤Î™Î£Î¤Î™ÎšÎ‘")
    report_lines.append("-" * 40)
    
    total_docs = sum(split_info['num_documents'] for split_info in results['splits'].values())
    total_tokens = sum(split_info['total_tokens'] for split_info in results['splits'].values())
    
    report_lines.append(f"Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ documents: {total_docs:,}")
    report_lines.append(f"Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ tokens: {total_tokens:,}")
    report_lines.append(f"Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± splits: {', '.join(results['splits'].keys())}")
    report_lines.append("")
    
    # Î‘Î½Î¬Î»Ï…ÏƒÎ· Î±Î½Î¬ split
    for split_name, split_info in results['splits'].items():
        report_lines.append(f"ğŸ“ SPLIT: {split_name.upper()}")
        report_lines.append("-" * 40)
        report_lines.append(f"Documents: {split_info['num_documents']:,}")
        report_lines.append(f"Tokens: {split_info['total_tokens']:,}")
        report_lines.append(f"ÎœÎ¿Î½Î±Î´Î¹ÎºÎ¬ IOB tags: {split_info['unique_tags']}")
        
        # Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ Ï„Î¿Ï… ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¿Ï dataset
        if total_docs > 0:
            doc_percentage = (split_info['num_documents'] / total_docs) * 100
        else:
            doc_percentage = 0
            
        if total_tokens > 0:
            token_percentage = (split_info['total_tokens'] / total_tokens) * 100
        else:
            token_percentage = 0
            
        report_lines.append(f"Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ documents: {doc_percentage:.2f}%")
        report_lines.append(f"Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ tokens: {token_percentage:.2f}%")
        report_lines.append("")
        
        # IOB Distribution
        report_lines.append("ğŸ·ï¸  ÎšÎ‘Î¤Î‘ÎÎŸÎœÎ— IOB TAGS:")
        
        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· tags
        iob_dist = split_info['iob_distribution']
        sorted_tags = sorted(iob_dist.keys(), key=lambda x: (
            0 if x == 'O' else 1 if str(x).startswith('B-') else 2 if str(x).startswith('I-') else 3,
            str(x)
        ))
        
        for tag in sorted_tags:
            count = iob_dist[tag]
            if split_info['total_tokens'] > 0:
                percentage = (count / split_info['total_tokens']) * 100
            else:
                percentage = 0
            report_lines.append(f"  {str(tag):<20}: {count:>8,} ({percentage:>6.2f}%)")
        
        report_lines.append("")
    
    report_lines.append("="*80)
    report_lines.append("Î¤Î•Î›ÎŸÎ£ Î‘ÎÎ‘Î¦ÎŸÎ¡Î‘Î£")
    report_lines.append("="*80)
    
    return "\n".join(report_lines)


def generate_comparative_report(results1, results2):
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ ÏƒÏ…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ Î¼ÎµÏ„Î±Î¾Ï Î´ÏÎ¿ datasets
    
    Args:
        results1 (dict): Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï€ÏÏÏ„Î¿Ï… dataset
        results2 (dict): Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î´ÎµÏÏ„ÎµÏÎ¿Ï… dataset
        
    Returns:
        str: Î£Ï…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬ ÏƒÎµ Î¼Î¿ÏÏ†Î® ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
    """
    
    if not results1 or not results2:
        return "âŒ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·"
    
    report_lines = []
    
    # Header
    report_lines.append("="*100)
    report_lines.append("Î£Î¥Î“ÎšÎ¡Î™Î¤Î™ÎšÎ— Î‘ÎÎ‘Î›Î¥Î£Î— GREEK LEGAL NER DATASETS")
    report_lines.append("="*100)
    report_lines.append(f"Dataset 1: {results1['dataset_info']['alias']} ({results1['dataset_info']['name']})")
    report_lines.append(f"Dataset 2: {results2['dataset_info']['alias']} ({results2['dataset_info']['name']})")
    report_lines.append(f"Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚
    report_lines.append("ğŸ“Š Î£Î¥ÎÎŸÎ›Î™ÎšÎ— Î£Î¥Î“ÎšÎ¡Î™Î£Î—")
    report_lines.append("-" * 60)
    
    # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ…Î½Î¿Î»Î¹ÎºÏÎ½
    total_docs1 = sum(split_info['num_documents'] for split_info in results1['splits'].values())
    total_tokens1 = sum(split_info['total_tokens'] for split_info in results1['splits'].values())
    total_docs2 = sum(split_info['num_documents'] for split_info in results2['splits'].values())
    total_tokens2 = sum(split_info['total_tokens'] for split_info in results2['splits'].values())
    
    report_lines.append(f"{'ÎœÎµÏ„ÏÎ¹ÎºÎ®':<25} {'Dataset 1':<20} {'Dataset 2':<20} {'Î”Î¹Î±Ï†Î¿ÏÎ¬':<15}")
    report_lines.append("-" * 80)
    report_lines.append(f"{'Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Docs':<25} {total_docs1:<20,} {total_docs2:<20,} {total_docs2-total_docs1:<+15,}")
    report_lines.append(f"{'Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Tokens':<25} {total_tokens1:<20,} {total_tokens2:<20,} {total_tokens2-total_tokens1:<+15,}")
    report_lines.append("")
    
    # Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î±Î½Î¬ split
    common_splits = set(results1['splits'].keys()) & set(results2['splits'].keys())
    
    if common_splits:
        report_lines.append("ğŸ“ Î£Î¥Î“ÎšÎ¡Î™Î£Î— Î‘ÎÎ‘ SPLIT")
        report_lines.append("-" * 60)
        
        for split_name in sorted(common_splits):
            split1 = results1['splits'][split_name]
            split2 = results2['splits'][split_name]
            
            report_lines.append(f"\nğŸ” SPLIT: {split_name.upper()}")
            report_lines.append("-" * 30)
            report_lines.append(f"{'ÎœÎµÏ„ÏÎ¹ÎºÎ®':<20} {'Dataset 1':<15} {'Dataset 2':<15} {'Î”Î¹Î±Ï†Î¿ÏÎ¬':<15}")
            report_lines.append("-" * 65)
            
            docs_diff = split2['num_documents'] - split1['num_documents']
            tokens_diff = split2['total_tokens'] - split1['total_tokens']
            tags_diff = split2['unique_tags'] - split1['unique_tags']
            
            report_lines.append(f"{'Documents':<20} {split1['num_documents']:<15,} {split2['num_documents']:<15,} {docs_diff:<+15,}")
            report_lines.append(f"{'Tokens':<20} {split1['total_tokens']:<15,} {split2['total_tokens']:<15,} {tokens_diff:<+15,}")
            report_lines.append(f"{'Unique Tags':<20} {split1['unique_tags']:<15} {split2['unique_tags']:<15} {tags_diff:<+15}")
    
    # Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· IOB tags
    report_lines.append("\n\nğŸ·ï¸  Î£Î¥Î“ÎšÎ¡Î™Î£Î— IOB DISTRIBUTIONS")
    report_lines.append("-" * 80)
    
    # Î£Ï…Î»Î»Î¿Î³Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ tags Î±Ï€ÏŒ Ï„Î± Î´ÏÎ¿ datasets
    all_tags1 = set()
    all_tags2 = set()
    
    for split_info in results1['splits'].values():
        all_tags1.update(split_info['iob_distribution'].keys())
    
    for split_info in results2['splits'].values():
        all_tags2.update(split_info['iob_distribution'].keys())
    
    all_tags = all_tags1 | all_tags2
    
    # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· tags
    sorted_all_tags = sorted(all_tags, key=lambda x: (
        0 if str(x) == 'O' else 1 if str(x).startswith('B-') else 2 if str(x).startswith('I-') else 3,
        str(x)
    ))
    
    report_lines.append(f"{'Tag':<20} {'Dataset 1':<15} {'Dataset 2':<15} {'ÎšÎ¿Î¹Î½ÏŒ':<10}")
    report_lines.append("-" * 60)
    
    for tag in sorted_all_tags:
        in_ds1 = "âœ“" if tag in all_tags1 else "âœ—"
        in_ds2 = "âœ“" if tag in all_tags2 else "âœ—"
        common = "âœ“" if tag in all_tags1 and tag in all_tags2 else "âœ—"
        
        report_lines.append(f"{str(tag):<20} {in_ds1:<15} {in_ds2:<15} {common:<10}")
    
    # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ tags
    common_tags = all_tags1 & all_tags2
    only_ds1 = all_tags1 - all_tags2
    only_ds2 = all_tags2 - all_tags1
    
    report_lines.append("\nğŸ“ˆ Î£Î¤Î‘Î¤Î™Î£Î¤Î™ÎšÎ‘ TAGS")
    report_lines.append("-" * 30)
    report_lines.append(f"ÎšÎ¿Î¹Î½Î¬ tags: {len(common_tags)}")
    report_lines.append(f"ÎœÏŒÎ½Î¿ ÏƒÏ„Î¿ Dataset 1: {len(only_ds1)}")
    report_lines.append(f"ÎœÏŒÎ½Î¿ ÏƒÏ„Î¿ Dataset 2: {len(only_ds2)}")
    
    if only_ds1:
        report_lines.append(f"Tags Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Dataset 1: {', '.join(map(str, sorted(only_ds1)))}")
    
    if only_ds2:
        report_lines.append(f"Tags Î¼ÏŒÎ½Î¿ ÏƒÏ„Î¿ Dataset 2: {', '.join(map(str, sorted(only_ds2)))}")
    
    report_lines.append("")
    report_lines.append("="*100)
    report_lines.append("Î¤Î•Î›ÎŸÎ£ Î£Î¥Î“ÎšÎ¡Î™Î¤Î™ÎšÎ—Î£ Î‘ÎÎ‘Î¦ÎŸÎ¡Î‘Î£")
    report_lines.append("="*100)
    
    return "\n".join(report_lines)


def save_report_to_file(report, output_file):
    """
    Î‘Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Î±Î½Î±Ï†Î¿ÏÎ¬ ÏƒÎµ txt Î±ÏÏ‡ÎµÎ¯Î¿
    
    Args:
        report (str): Î‘Î½Î±Ï†Î¿ÏÎ¬ ÏƒÎµ Î¼Î¿ÏÏ†Î® ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
        output_file (str): ÎŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï… ÎµÎ¾ÏŒÎ´Î¿Ï…
    """
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… Î‘Î½Î±Ï†Î¿ÏÎ¬ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ ÏƒÏ„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿: {output_file}")
        
    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·: {e}")


def main():
    """
    ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚
    """
    
    print("ğŸš€ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· ÏƒÏ…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ®Ï‚ Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Greek Legal NER Datasets")
    print("=" * 80)
    
    # Î‘Î½Î±Î»ÏÏƒÎµÎ¹Ï‚ datasets
    print("\n1ï¸âƒ£ Î‘Î½Î¬Î»Ï…ÏƒÎ· Ï€ÏÏÏ„Î¿Ï… dataset...")
    results1 = analyze_dataset(
        "joelniklaus/greek_legal_ner", 
        dataset_alias="joelniklaus/greek_legal_ner"
    )
    
    print("\n2ï¸âƒ£ Î‘Î½Î¬Î»Ï…ÏƒÎ· Î´ÎµÏÏ„ÎµÏÎ¿Ï… dataset...")
    results2 = analyze_dataset(
        "joelito/lextreme", 
        "greek_legal_ner",
        dataset_alias="joelito/lextreme"
    )
    
    if not results1 or not results2:
        print("âŒ Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Ï†Î¿ÏÏ„ÏÏƒÏ‰ ÏŒÎ»Î± Ï„Î± datasets")
        return
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± timestamp Î³Î¹Î± Î±ÏÏ‡ÎµÎ¯Î±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Î½Î±Ï†Î¿ÏÏÎ½
    print("\nğŸ“‹ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î±Î½Î±Ï†Î¿ÏÏÎ½...")
    
    # 1. Î‘Î½Î±Ï†Î¿ÏÎ¬ Î³Î¹Î± Ï„Î¿ Î´ÎµÏÏ„ÎµÏÎ¿ dataset
    report2 = generate_single_dataset_report(results2)
    output_file2 = f"lextreme_greek_legal_ner_analysis_{timestamp}.txt"
    save_report_to_file(report2, output_file2)
    
    # 2. Î£Ï…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ® Î±Î½Î±Ï†Î¿ÏÎ¬
    comparative_report = generate_comparative_report(results1, results2)
    comparative_file = f"datasets_comparison_{timestamp}.txt"
    save_report_to_file(comparative_report, comparative_file)
    
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÏÎ½Ï„Î¿Î¼Î·Ï‚ Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·Ï‚
    print("\nğŸ“Š Î£Î¥ÎÎ¤ÎŸÎœÎ— Î Î•Î¡Î™Î›Î—Î¨Î— Î£Î¥Î“ÎšÎ¡Î™Î£Î—Î£:")
    print("-" * 50)
    
    # Dataset 1
    total_docs1 = sum(split_info['num_documents'] for split_info in results1['splits'].values())
    total_tokens1 = sum(split_info['total_tokens'] for split_info in results1['splits'].values())
    
    # Dataset 2  
    total_docs2 = sum(split_info['num_documents'] for split_info in results2['splits'].values())
    total_tokens2 = sum(split_info['total_tokens'] for split_info in results2['splits'].values())
    
    print(f"Dataset 1 (joelniklaus): {total_docs1:,} docs, {total_tokens1:,} tokens")
    print(f"Dataset 2 (joelito):     {total_docs2:,} docs, {total_tokens2:,} tokens")
    print(f"Î”Î¹Î±Ï†Î¿ÏÎ¬:                {total_docs2-total_docs1:+,} docs, {total_tokens2-total_tokens1:+,} tokens")
    
    print(f"\nğŸ’¾ Î‘ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎ±Î½:")
    print(f"  - {output_file2}")
    print(f"  - {comparative_file}")


if __name__ == "__main__":
    main()
