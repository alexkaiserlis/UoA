#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Î³Î¹Î± ÎµÎ¾Î±Î³Ï‰Î³Î® ÎºÎ±Î¹ Î­Î»ÎµÎ³Ï‡Î¿ Ï„Ï‰Î½ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏÎ½ ground truth entities Î±Ï€ÏŒ Ï„Î¿ Greek Legal NER dataset
Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— Î•ÎšÎ”ÎŸÎ£Î— - Î›ÏÎ½ÎµÎ¹ Ï„Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Î¼Îµ Ï„Î¿Ï…Ï‚ Î»Î¬Î¸Î¿Ï‚ Î±ÏÎ¹Î¸Î¼Î¿ÏÏ‚ entities
"""

import json
from datasets import load_dataset
from collections import defaultdict, Counter
import pandas as pd

def extract_entities_from_bio_corrected(words, labels):
    """
    Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— ÎµÎ¾Î±Î³Ï‰Î³Î® entities Î±Ï€ÏŒ BIO format
    Î§ÎµÎ¹ÏÎ¯Î¶ÎµÏ„Î±Î¹ ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î± B- ÎºÎ±Î¹ I- tags
    """
    entities = []
    current_entity = None
    current_entity_words = []
    start_idx = 0
    
    for i, (word, label) in enumerate(zip(words, labels)):
        if label.startswith('B-'):
            # Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Î½Î¿Î»Î¿ÎºÎ»Î®ÏÏ‰Ï„Î¿ entity, Ï„Î¿ Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï€ÏÏÏ„Î±
            if current_entity is not None:
                entities.append({
                    'text': ' '.join(current_entity_words),
                    'label': current_entity,
                    'start_word': start_idx,
                    'end_word': i - 1,
                    'words': current_entity_words.copy(),
                    'length': len(current_entity_words)
                })
            
            # ÎÎµÎºÎ¹Î½Î¬Î¼Îµ Î½Î­Î¿ entity
            current_entity = label[2:]  # Î‘Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Ï„Î¿ 'B-'
            current_entity_words = [word]
            start_idx = i
            
        elif label.startswith('I-'):
            # Î£Ï…Î½ÎµÏ‡Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Ï„ÏÎ­Ï‡Î¿Î½ entity ÎœÎŸÎÎŸ Î±Î½ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ Î¯Î´Î¹Î¿ type
            entity_type = label[2:]  # Î‘Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Ï„Î¿ 'I-'
            if current_entity is not None and entity_type == current_entity:
                current_entity_words.append(word)
            else:
                # Î›Î¬Î¸Î¿Ï‚ I- tag Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿ B- Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ type
                # Î¤ÎµÎ»ÎµÎ¹ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿ entity (Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹)
                if current_entity is not None:
                    entities.append({
                        'text': ' '.join(current_entity_words),
                        'label': current_entity,
                        'start_word': start_idx,
                        'end_word': i - 1,
                        'words': current_entity_words.copy(),
                        'length': len(current_entity_words)
                    })
                
                # Î‘ÏÏ‡Î¯Î¶Î¿Ï…Î¼Îµ Î½Î­Î¿ entity Î±Ï€ÏŒ Î±Ï…Ï„ÏŒ Ï„Î¿ I- tag (Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ ÏŒÏ„Î¹ ÎµÎ¯Î½Î±Î¹ B-)
                current_entity = entity_type
                current_entity_words = [word]
                start_idx = i
                
        else:
            # 'O' label Î® Î¬Î»Î»Î· Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ· - Ï„ÎµÎ»ÎµÎ¹ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ Ï„ÏÎ­Ï‡Î¿Î½ entity
            if current_entity is not None:
                entities.append({
                    'text': ' '.join(current_entity_words),
                    'label': current_entity,
                    'start_word': start_idx,
                    'end_word': i - 1,
                    'words': current_entity_words.copy(),
                    'length': len(current_entity_words)
                })
                current_entity = None
                current_entity_words = []
    
    # Î‘Î½ Ï„Î¿ document Ï„ÎµÎ»ÎµÎ¹ÏÎ½ÎµÎ¹ Î¼Îµ entity
    if current_entity is not None:
        entities.append({
            'text': ' '.join(current_entity_words),
            'label': current_entity,
            'start_word': start_idx,
            'end_word': len(words) - 1,
            'words': current_entity_words.copy(),
            'length': len(current_entity_words)
        })
    
    return entities

def verify_dataset_counts():
    """
    Î•Ï€Î±Î»Î·Î¸ÎµÏÎµÎ¹ Ï„Î¿Ï…Ï‚ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¿ÏÏ‚ Î±ÏÎ¹Î¸Î¼Î¿ÏÏ‚ entities ÏƒÏ„Î¿ dataset
    Î£Ï…Î³ÎºÏÎ¯Î½ÎµÎ¹ Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¿Ï… Î­Î´Ï‰ÏƒÎµÏ‚
    """
    print("ğŸ” Î•Î Î‘Î›Î—Î˜Î•Î¥Î£Î— DATASET COUNTS...")
    ds = load_dataset("joelniklaus/greek_legal_ner")
    
    # ÎšÎ±Ï„Î±Î¼Î­Ï„ÏÎ·ÏƒÎ· Î¼Îµ Î´ÏÎ¿ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï…Ï‚ Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿
    method1_counts = defaultdict(lambda: defaultdict(int))  # ÎœÎ­Ï„ÏÎ·ÏƒÎ· tokens
    method2_counts = defaultdict(lambda: defaultdict(int))  # ÎœÎ­Ï„ÏÎ·ÏƒÎ· entities
    
    for split in ['train', 'validation', 'test']:
        print(f"\nğŸ“Š Î‘Î½Î¬Î»Ï…ÏƒÎ· split: {split}")
        split_data = ds[split]
        
        # Method 1: ÎœÎ­Ï„ÏÎ·ÏƒÎ· tokens (Ï„Î¿ Ï€Î±Î»Î¹ÏŒ Î¼Î±Ï‚)
        for example in split_data:
            labels = example['ner']
            for label in labels:
                if label != 'O':
                    method1_counts[label][split] += 1
        
        # Method 2: ÎœÎ­Ï„ÏÎ·ÏƒÎ· entities (Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î¿)
        for example in split_data:
            words = example['words']
            labels = example['ner']
            entities = extract_entities_from_bio_corrected(words, labels)
            for entity in entities:
                method2_counts[entity['label']][split] += 1
    
    # Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¿Ï… Î­Î´Ï‰ÏƒÎµÏ‚
    expected_counts = {
        ('FACILITY', 'test'): 142,
        ('FACILITY', 'train'): 1224,
        ('FACILITY', 'validation'): 60,
        ('GPE', 'test'): 1083,
        ('GPE', 'train'): 5400,
        ('GPE', 'validation'): 1214,
        ('LEG-REFS', 'test'): 1331,
        ('LEG-REFS', 'train'): 5159,
        ('LEG-REFS', 'validation'): 1382,
        ('LOCATION-NAT', 'test'): 26,
        ('LOCATION-NAT', 'train'): 145,
        ('LOCATION-NAT', 'validation'): 2,
        ('LOCATION-UNK', 'test'): 205,
        ('LOCATION-UNK', 'train'): 1316,
        ('LOCATION-UNK', 'validation'): 283,
        ('ORG', 'test'): 1354,
        ('ORG', 'train'): 5906,
        ('ORG', 'validation'): 1506,
        ('PERSON', 'test'): 491,
        ('PERSON', 'train'): 1921,
        ('PERSON', 'validation'): 475,
        ('PUBLIC-DOCS', 'test'): 452,
        ('PUBLIC-DOCS', 'train'): 2652,
        ('PUBLIC-DOCS', 'validation'): 556,
    }
    
    print(f"\n{'='*80}")
    print("ğŸ“Š Î£Î¥Î“ÎšÎ¡Î™Î£Î— ÎœÎ•Î˜ÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘ÎœÎ•Î¤Î¡Î—Î£Î—Î£")
    print(f"{'='*80}")
    print(f"{'Entity + Split':<25} {'Expected':<10} {'Method1(Tokens)':<15} {'Method2(Entities)':<15} {'Match?'}")
    print("-" * 80)
    
    all_entity_types = set()
    for split in ['train', 'validation', 'test']:
        all_entity_types.update(method1_counts.keys())
        all_entity_types.update(method2_counts.keys())
    
    total_mismatches = 0
    correct_method = None
    
    for entity_type in sorted(all_entity_types):
        for split in ['test', 'train', 'validation']:
            key = (entity_type, split)
            expected = expected_counts.get(key, 0)
            method1_count = method1_counts[entity_type][split]
            method2_count = method2_counts[entity_type][split]
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Ï€Î¿Î¹Î± Î¼Î­Î¸Î¿Î´Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ ÏƒÏ‰ÏƒÏ„Î®
            method1_match = "âœ…" if method1_count == expected else "âŒ"
            method2_match = "âœ…" if method2_count == expected else "âŒ"
            
            if method1_count == expected:
                correct_method = "Method1 (Tokens)"
            elif method2_count == expected:
                correct_method = "Method2 (Entities)"
            
            if method1_count != expected and method2_count != expected:
                total_mismatches += 1
            
            print(f"{key[0] + '+' + key[1]:<25} {expected:<10} {method1_count:<15} {method2_count:<15} {method1_match}/{method2_match}")
    
    print(f"\nğŸ¯ Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î‘:")
    print(f"   Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î±ÏƒÏ…Î¼Ï†Ï‰Î½Î¯ÎµÏ‚: {total_mismatches}")
    if correct_method:
        print(f"   Î£Ï‰ÏƒÏ„Î® Î¼Î­Î¸Î¿Î´Î¿Ï‚: {correct_method}")
    else:
        print(f"   âš ï¸  ÎšÎ±Î¼Î¯Î± Î¼Î­Î¸Î¿Î´Î¿Ï‚ Î´ÎµÎ½ Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ Ï€Î»Î®ÏÏ‰Ï‚ Î¼Îµ Ï„Î± expected counts!")
    
    return method1_counts, method2_counts, expected_counts

def main():
    print("ğŸ” Î”Î™ÎŸÎ¡Î˜Î©ÎœÎ•ÎÎ— Î•ÎšÎ”ÎŸÎ£Î— - Î•Ï€Î±Î»Î®Î¸ÎµÏ…ÏƒÎ· Ground Truth Entities")
    print("="*70)
    
    # Î•Ï€Î±Î»Î®Î¸ÎµÏ…ÏƒÎ· counts
    method1_counts, method2_counts, expected_counts = verify_dataset_counts()
    
    # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· dataset Î³Î¹Î± Ï€Î»Î®ÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ·
    print("\nğŸ“ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· dataset Î³Î¹Î± Ï€Î»Î®ÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ·...")
    ds = load_dataset("joelniklaus/greek_legal_ner")
    test_set = ds['test']
    
    print(f"Test set Î­Ï‡ÎµÎ¹ {len(test_set)} documents")
    
    # Î•Î¾Î±Î³Ï‰Î³Î® entities Î¼Îµ Ï„Î· Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î· Î¼Î­Î¸Î¿Î´Î¿
    all_entities = []
    entity_stats = defaultdict(lambda: defaultdict(int))
    document_stats = []
    
    print("Î•Î¾Î±Î³Ï‰Î³Î® entities Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ Î­Î³Î³ÏÎ±Ï†Î¿...")
    
    for doc_id, example in enumerate(test_set):
        words = example['words']
        labels = example['ner']
        
        # Î•Î¾Î±Î³Ï‰Î³Î® entities Î¼Îµ Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î· Î¼Î­Î¸Î¿Î´Î¿
        entities = extract_entities_from_bio_corrected(words, labels)
        
        # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± Ï„Î¿ document
        doc_stats = {
            'doc_id': doc_id,
            'total_words': len(words),
            'total_entities': len(entities),
            'entities_by_type': Counter([e['label'] for e in entities])
        }
        document_stats.append(doc_stats)
        
        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï„Î¿Ï… doc_id ÏƒÎµ ÎºÎ¬Î¸Îµ entity
        for entity in entities:
            entity['doc_id'] = doc_id
            entity['doc_total_words'] = len(words)
            all_entities.append(entity)
            
            # Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ· ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½
            entity_type = entity['label']
            entity_length = len(entity['words'])
            entity_stats[entity_type]['count'] += 1
            entity_stats[entity_type]['total_words'] += entity_length
            entity_stats[entity_type]['min_words'] = min(
                entity_stats[entity_type].get('min_words', float('inf')), 
                entity_length
            )
            entity_stats[entity_type]['max_words'] = max(
                entity_stats[entity_type].get('max_words', 0), 
                entity_length
            )
    
    # Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î¼Îµ Ï„Î± expected
    print(f"\nğŸ¯ Î¤Î•Î›Î™ÎšÎ— Î£Î¥Î“ÎšÎ¡Î™Î£Î— Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î©Î:")
    print("-" * 50)
    total_entities_found = len(all_entities)
    entity_counts_by_type = Counter([e['label'] for e in all_entities])
    
    for entity_type in sorted(entity_counts_by_type.keys()):
        found_count = entity_counts_by_type[entity_type]
        expected_count = expected_counts.get((entity_type, 'test'), 0)
        match_status = "âœ…" if found_count == expected_count else "âŒ"
        print(f"{entity_type:<15}: Found {found_count:>4}, Expected {expected_count:>4} {match_status}")
    
    print(f"\nÎ£Ï…Î½Î¿Î»Î¹ÎºÎ¬ entities: {total_entities_found}")
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    print("\nÎ‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½...")
    
    with open('corrected_groundtruth_entities.json', 'w', encoding='utf-8') as f:
        json.dump(all_entities, f, ensure_ascii=False, indent=2)
    
    with open('corrected_entity_counts_comparison.json', 'w', encoding='utf-8') as f:
        comparison_data = {
            'found_counts': dict(entity_counts_by_type),
            'expected_counts': {k[0]: v for k, v in expected_counts.items() if k[1] == 'test'},
            'method1_counts': {k: v['test'] for k, v in method1_counts.items()},
            'method2_counts': {k: v['test'] for k, v in method2_counts.items()},
            'total_entities_found': total_entities_found
        }
        json.dump(comparison_data, f, ensure_ascii=False, indent=2)
    
    print("âœ… Î”Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½!")

if __name__ == "__main__":
    main()
