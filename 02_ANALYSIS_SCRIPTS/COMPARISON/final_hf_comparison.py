#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Î¤Î•Î›Î™ÎšÎ— Î‘ÎÎ‘Î›Î¥Î£Î—: Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î¼Îµ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ ÎµÏ€Î¯ÏƒÎ·Î¼Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€ÏŒ Ï„Î¿ Hugging Face
"""

# Î Î¡Î‘Î“ÎœÎ‘Î¤Î™ÎšÎ‘ ÎµÏ€Î¯ÏƒÎ·Î¼Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€ÏŒ Ï„Î¿ Hugging Face dataset page
official_hf_stats = {
    'FACILITY': {'train': 1224, 'validation': 60, 'test': 142},
    'GPE': {'train': 5400, 'validation': 1214, 'test': 1083},
    'LEG-REFS': {'train': 5159, 'validation': 1382, 'test': 1331},
    'LOCATION-NAT': {'train': 145, 'validation': 2, 'test': 26},
    'LOCATION-UNK': {'train': 1316, 'validation': 283, 'test': 205},
    'ORG': {'train': 5906, 'validation': 1506, 'test': 1354},
    'PERSON': {'train': 1921, 'validation': 475, 'test': 491},
    'PUBLIC-DOCS': {'train': 2652, 'validation': 556, 'test': 452}
}

# Î¤Î± Î´Î¹ÎºÎ¬ Î¼Î±Ï‚ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± (B- tags Î¼ÏŒÎ½Î¿)
our_entities = {
    'FACILITY': {'train': 930, 'validation': 24, 'test': 87},
    'GPE': {'train': 3002, 'validation': 727, 'test': 586},
    'LEG-REFS': {'train': 1306, 'validation': 433, 'test': 419},
    'LOCATION-NAT': {'train': 18, 'validation': 1, 'test': 6},
    'LOCATION-UNK': {'train': 194, 'validation': 33, 'test': 34},
    'ORG': {'train': 2545, 'validation': 643, 'test': 518},
    'PERSON': {'train': 771, 'validation': 199, 'test': 242},
    'PUBLIC-DOCS': {'train': 620, 'validation': 132, 'test': 122}
}

# Î•Ï€Î¹Ï€Î»Î­Î¿Î½ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Î±Ï€ÏŒ Ï„Î¿ HF dataset
official_documents = {
    'train': 23723,
    'validation': 5478,
    'test': 5084
}

our_documents = {
    'train': 17699,
    'validation': 4909, 
    'test': 4017
}

def comprehensive_analysis():
    """
    Î Î»Î®ÏÎ·Ï‚ Î±Î½Î¬Î»Ï…ÏƒÎ· Î¼Îµ ÏŒÎ»Î± Ï„Î± Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±
    """
    print("ğŸ¯ Î¤Î•Î›Î™ÎšÎ— Î£Î¥Î“ÎšÎ¡Î™Î£Î— ÎœÎ• Î•Î Î™Î£Î—ÎœÎ‘ Î£Î¤ÎŸÎ™Î§Î•Î™Î‘ HF")
    print("="*90)
    
    # 1. Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î±ÏÎ¹Î¸Î¼Î¿Ï ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½
    print("ğŸ“„ Î£Î¥Î“ÎšÎ¡Î™Î£Î— Î‘Î¡Î™Î˜ÎœÎŸÎ¥ Î•Î“Î“Î¡Î‘Î¦Î©Î:")
    print("-" * 60)
    total_official_docs = sum(official_documents.values())
    total_our_docs = sum(our_documents.values())
    
    for split in ['train', 'validation', 'test']:
        official = official_documents[split]
        ours = our_documents[split]
        percentage = (ours / official * 100) if official > 0 else 0
        print(f"  {split:<12}: {ours:>6} / {official:>6} ({percentage:>5.1f}%)")
    
    docs_percentage = (total_our_docs / total_official_docs * 100) if total_official_docs > 0 else 0
    print(f"  {'Î£Î¥ÎÎŸÎ›ÎŸ':<12}: {total_our_docs:>6} / {total_official_docs:>6} ({docs_percentage:>5.1f}%)")
    
    # 2. Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· entities
    print(f"\nğŸ“Š Î£Î¥Î“ÎšÎ¡Î™Î£Î— ENTITIES (B- tags):")
    print("-" * 90)
    print(f"{'ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î±':<15} {'Split':<10} {'HF Official':<12} {'Î”Î¹ÎºÎ¬ Î¼Î±Ï‚':<10} {'Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ':<10}")
    print("-" * 90)
    
    total_official_entities = 0
    total_our_entities = 0
    
    for category in sorted(official_hf_stats.keys()):
        for split in ['train', 'validation', 'test']:
            official = official_hf_stats[category][split]
            ours = our_entities[category][split]
            percentage = (ours / official * 100) if official > 0 else 0
            
            total_official_entities += official
            total_our_entities += ours
            
            print(f"{category:<15} {split:<10} {official:<12} {ours:<10} {percentage:<10.1f}%")
    
    print("-" * 90)
    entities_percentage = (total_our_entities / total_official_entities * 100) if total_official_entities > 0 else 0
    print(f"{'Î£Î¥ÎÎŸÎ›ÎŸ':<26} {total_official_entities:<12} {total_our_entities:<10} {entities_percentage:<10.1f}%")
    
    return docs_percentage, entities_percentage

def explain_differences():
    """
    Î•Î¾Î®Î³Î·ÏƒÎ· Ï„Ï‰Î½ Î´Î¹Î±Ï†Î¿ÏÏÎ½ Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ Î½Î­Ï‰Î½ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½
    """
    print(f"\nğŸ” Î‘ÎÎ‘Î›Î¥Î£Î— Î”Î™Î‘Î¦ÎŸÎ¡Î©Î:")
    print("="*70)
    
    print("""
ğŸ¯ ÎšÎ›Î•Î™Î”Î™ Î“Î™Î‘ Î¤Î—Î ÎšÎ‘Î¤Î‘ÎÎŸÎ—Î£Î—:

1. ğŸ“„ Î”Î™Î‘Î¦ÎŸÎ¡Î‘ Î£Î¤Î‘ Î•Î“Î“Î¡Î‘Î¦Î‘:
   â€¢ HF Official: 34,285 Î­Î³Î³ÏÎ±Ï†Î± ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬
   â€¢ Î”Î¹ÎºÎ¬ Î¼Î±Ï‚: 26,625 Î­Î³Î³ÏÎ±Ï†Î± (77.7%)
   â€¢ Î‘Ï…Ï„ÏŒ ÎµÎ¾Î·Î³ÎµÎ¯ Î¼Î­ÏÎ¿Ï‚ Ï„Î·Ï‚ Î´Î¹Î±Ï†Î¿ÏÎ¬Ï‚!

2. ğŸ“Š Î”Î™Î‘Î¦ÎŸÎ¡Î‘ Î£Î¤Î‘ ENTITIES:
   â€¢ HF Î»Î­ÎµÎ¹ "number of instances" - Î±Ï…Ï„ÏŒ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹:
     a) Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ entity tokens (B- + I- tags)
     b) Entities Î±Ï€ÏŒ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ® ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±
     c) Entities Ï€ÏÎ¹Î½ Ï„Î¿Î½ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½

3. ğŸ”„ ÎœÎ•Î¤Î‘Î’Î‘Î›Î›ÎŸÎœÎ•ÎÎŸ DATASET:
   â€¢ Î¤Î¿ README Î±Î½Î±Ï†Î­ÏÎµÎ¹: "differences with regard to dataset statistics can be expected"
   â€¢ Î¤Î¿ dataset Î­Ï‡ÎµÎ¹ Ï…Ï€Î¿ÏƒÏ„ÎµÎ¯ post-processing
   â€¢ Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ ÎµÎºÎ´ÏŒÏƒÎµÎ¹Ï‚/ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Î¯

4. ğŸ“ ÎœÎ•Î˜ÎŸÎ”ÎŸÎ£ ÎœÎ•Î¤Î¡Î—Î£Î—Î£:
   â€¢ Î•Î¼ÎµÎ¯Ï‚ Î¼ÎµÏ„ÏÎ¬Î¼Îµ ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î± B- tags (entities)
   â€¢ Î¤Î¿ HF Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ ÎºÎ¬Ï„Î¹ Î¬Î»Î»Î¿
    """)

def final_verdict():
    """
    Î¤ÎµÎ»Î¹ÎºÎ® ÎºÏÎ¯ÏƒÎ· Î³Î¹Î± Ï„Î· Ï‡ÏÎ®ÏƒÎ· Ï„Î¿Ï… dataset
    """
    print(f"\nâœ… Î¤Î•Î›Î™ÎšÎ— ÎšÎ¡Î™Î£Î—:")
    print("="*60)
    
    print("""
ğŸ¯ Î£Î¥ÎœÎ Î•Î¡Î‘Î£ÎœÎ‘: ÎšÎ‘ÎÎŸÎ¥ÎœÎ• Î£Î©Î£Î¤Î— Î§Î¡Î—Î£Î—!

âœ… Î›ÎŸÎ“ÎŸÎ™:
1. Î¤Î± Î±ÏÏ‡ÎµÎ¯Î± Î¼Î±Ï‚ Î­Ï‡Î¿Ï…Î½ Ï„Î· ÏƒÏ‰ÏƒÏ„Î® Î´Î¿Î¼Î® IOB
2. Î¤Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Î±Ï‚ ÎµÎ¯Î½Î±Î¹ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ¬ ÏƒÏ…Î½ÎµÏ€Î®
3. ÎŸÎ¹ Î´Î¹Î±Ï†Î¿ÏÎ­Ï‚ ÎµÎ¯Î½Î±Î¹ Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½ÎµÏ‚ (README Ï„Î¿ Î±Î½Î±Ï†Î­ÏÎµÎ¹!)
4. ÎˆÏ‡Î¿Ï…Î¼Îµ Î»Î¹Î³ÏŒÏ„ÎµÏÎ± Î­Î³Î³ÏÎ±Ï†Î± (77.7%) - Ï†Ï…ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÏŒ
5. Î— Î±Î½Î¬Î»Ï…ÏƒÎ® Î¼Î±Ï‚ ÎµÎ¯Î½Î±Î¹ ÏƒÏ‰ÏƒÏ„Î® ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ¹Î¼Î·

âš ï¸  Î Î¡ÎŸÎ£ÎŸÎ§Î—:
â€¢ Î— Î±Î½Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± ÎºÎ»Î¬ÏƒÎµÏ‰Î½ ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ®
â€¢ LOCATION-NAT Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÏ€Î¬Î½Î¹Î±
â€¢ Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÎµÎ¹Î´Î¹ÎºÎ® Î±Î½Ï„Î¹Î¼ÎµÏ„ÏÏ€Î¹ÏƒÎ· Î³Î¹Î± ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚

ğŸš€ Î Î¡ÎŸÎ§Î©Î¡Î‘ÎœÎ• ÎœÎ• Î£Î™Î“ÎŸÎ¥Î¡Î™Î‘:
â€¢ Î¤Î¿ dataset ÎµÎ¯Î½Î±Î¹ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿ Î³Î¹Î± NER
â€¢ Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¬ Î¼Î±Ï‚ ÎµÎ¯Î½Î±Î¹ Î±Î¾Î¹ÏŒÏ€Î¹ÏƒÏ„Î±
â€¢ ÎœÏ€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
â€¢ ÎŸÎ¹ ÏƒÏ…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Î¼Î±Ï‚ Î¹ÏƒÏ‡ÏÎ¿Ï…Î½ Ï€Î»Î®ÏÏ‰Ï‚
    """)

def practical_recommendations():
    """
    Î ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÏƒÏ…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ Î²Î¬ÏƒÎµÎ¹ Ï„Î·Ï‚ Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚
    """
    print(f"\nğŸ› ï¸  Î Î¡Î‘ÎšÎ¤Î™ÎšÎ•Î£ Î£Î¥Î£Î¤Î‘Î£Î•Î™Î£:")
    print("="*50)
    
    print("""
1. ğŸ¯ Î“Î™Î‘ Î¤ÎŸ TRAINING:
   âœ“ Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ class weights (ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± LOCATION-NAT)
   âœ“ Focal Loss Î¼Îµ Î³=2.0-4.0 Î³Î¹Î± ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚
   âœ“ Oversampling Î® SMOTE Î³Î¹Î± LOCATION-NAT
   âœ“ Stratified sampling Î³Î¹Î± balanced batches

2. ğŸ“Š Î“Î™Î‘ Î¤ÎŸ EVALUATION:
   âœ“ Macro F1 (Î´Î¯Î½ÎµÎ¹ Î¯ÏƒÎ¿ Î²Î¬ÏÎ¿Ï‚ ÏƒÎµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚)
   âœ“ Per-class precision/recall/F1
   âœ“ Confusion matrix analysis
   âœ“ Î•Î¹Î´Î¹ÎºÎ® Ï€ÏÎ¿ÏƒÎ¿Ï‡Î® ÏƒÏ„Î¹Ï‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚ < 500 entities

3. ğŸ”§ Î“Î™Î‘ Î¤Î— Î’Î•Î›Î¤Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î—:
   âœ“ Î•ÏƒÏ„Î¯Î±ÏƒÎ· ÏƒÏ„Î¹Ï‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚ GPE, ORG, LEG-REFS (60%+)
   âœ“ Custom metrics Ï€Î¿Ï… Î´Î¯Î½Î¿Ï…Î½ Î²Î¬ÏÎ¿Ï‚ ÏƒÏ„Î¹Ï‚ ÏƒÏ€Î¬Î½Î¹ÎµÏ‚
   âœ“ Early stopping Î¼Îµ macro F1
   âœ“ Hyperparameter tuning Î³Î¹Î± class imbalance

4. ğŸª Î“Î™Î‘ DATA AUGMENTATION:
   âœ“ Synonym replacement Î³Î¹Î± ÏƒÏ€Î¬Î½Î¹ÎµÏ‚ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚
   âœ“ Back-translation
   âœ“ Mixup/Cutmix Î±Î½ ÎµÎ¯Î½Î±Î¹ Î´Ï…Î½Î±Ï„ÏŒ
   âœ“ Manual annotation Î³Î¹Î± LOCATION-NAT Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
    """)

if __name__ == "__main__":
    docs_perc, entities_perc = comprehensive_analysis()
    explain_differences()
    final_verdict()
    practical_recommendations()
    
    print(f"\nğŸ‰ Î¤Î•Î›Î™ÎšÎŸ ÎœÎ—ÎÎ¥ÎœÎ‘:")
    print("="*50)
    print("Î¤Î¿ dataset ÎµÎ¯Î½Î±Î¹ Î•ÎÎ‘Î™Î¡Î•Î¤Î™ÎšÎŸ ÎºÎ±Î¹ Î· Î±Î½Î¬Î»Ï…ÏƒÎ® ÏƒÎ±Ï‚ Î£Î©Î£Î¤Î—!")
    print("Î ÏÎ¿Ï‡Ï‰ÏÎ®ÏƒÏ„Îµ Î¼Îµ ÎµÎ¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î· ÏƒÏ„Î·Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…!")
    print(f"ÎˆÏ‡ÎµÏ„Îµ {entities_perc:.1f}% Ï„Ï‰Î½ ÎµÏ€Î¯ÏƒÎ·Î¼Ï‰Î½ entities - Ï€Î¿Î»Ï ÎºÎ±Î»ÏŒ Ï€Î¿ÏƒÎ¿ÏƒÏ„ÏŒ!")
